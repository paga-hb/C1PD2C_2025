{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e27f3f",
   "metadata": {},
   "source": [
    "---\n",
    "# OpenCL Programming in C\n",
    "---\n",
    "\n",
    "This notebook contains an introduction to OpenCL programming in C. For detailed coverage, the Khronos Group's documentation is a good source:\n",
    "\n",
    "- [Khronos Group OpenCL](https://www.khronos.org/opencl)\n",
    "- [OpenCL Specification](https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_API.html)\n",
    "- [OpenCL SDK](https://github.com/KhronosGroup/OpenCL-SDK)\n",
    "- [OpenCL Guide](https://github.com/KhronosGroup/OpenCL-Guide?tab=readme-ov-file)\n",
    "\n",
    "**Note! If you don't have an OpenCL-enabled device on your system**\n",
    "\n",
    "- You can run this notebook in Google CoLab.\n",
    "  - Skip ahead to [1.1 Running the Notebook Locally or on Google CoLab](#11-running-the-notebook-locally-or-on-google-coLab)\n",
    "\n",
    "**Note! If you are on Windows**\n",
    "\n",
    "- Make sure you have installed Visual Studio or the Build Tools for Visual Studio.\n",
    "- Make sure you have started VSCode (`code .`) from within a `Visual Studio Developer Command Prompt` to set necessary environment variables.\n",
    "  - This is required when using the MicroSoft Visual C/C++ (MSVC) compiler `cl.exe` in VSCode on Windows.\n",
    "- If you are using PowerShell as your default shell in VSCode, your default PowerShell profile file `profile.ps1` might not be digitally signed.\n",
    "  - This will lead to errors when you compile C code in VSCode.\n",
    "  - If so, you can fix this error by executing either of the two PowerShell commands below:\n",
    "    - `Rename-Item \"$env:USERPROFILE\\Documents\\WindowsPowerShell\\profile.ps1\" -NewName \"profile.ps1.bak\"`\n",
    "    - `Set-ExecutionPolicy RemoteSigned -Scope CurrentUser`\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- [1. Prerequisites](#1-prerequisites) \n",
    "  - [1.1 Running the Notebook Locally or on Google CoLab](#11-running-the-notebook-locally-or-on-google-coLab)\n",
    "  - [1.2 Operating System and VSCode Shell](#12-operating-system-and-vscode-shell)\n",
    "  - [1.3 C Compiler (`gcc`, `clang`, `cl`)](#13-c-compiler-gcc-clang-cl)\n",
    "  - [1.4 OpenCL Library and Header Files](#14-opencl-library-and-header-files)\n",
    "  - [1.5 Configuring `tasks.json`, `launch.json` and `c_cpp_properties.json`](#15-configuring-tasksjson-launchjson-and-c_cpp_propertiesjson)\n",
    "  - [1.6 Create the File `tasks.json`](#16-create-the-file-tasksjson)\n",
    "  - [1.7 Create the File `launch.json`](#17-create-the-file-launchjson)\n",
    "  - [1.8 Create the File `c_cpp_properties.json`](#18-create-the-file-c_cpp_propertiesjson)\n",
    "  - [1.9 VSCode Extensions](#19-vscode-extensions)\n",
    "  - [1.10 Using Built-in Cell Magic `%%writefile`](#110-using-built-in-cell-magic-writefile)\n",
    "  - [1.11 Compiling and Executing an OpenCL Program from a Notebook Code Cell](#111-compiling-and-executing-an-opencl-program-from-a-notebook-code-cell)\n",
    "  - [1.12 Compiling and Debugging a Single-file OpenCL Program](#112-compiling-and-debugging-a-single-file-opencl-program)\n",
    "  - [1.13 Compiling and Debugging a Multi-file OpenCL Program](#113-compiling-and-debugging-a-multi-file-opencl-program)\n",
    "- [2. OpenCL Basics](#2-opencl-basics)\n",
    "  - [2.1 Listing OpenCL-enabled Devices and Properties](#21-listing-opencl-enabled-devices-and-properties)\n",
    "  - [2.2 Hello World in Host Code (CPU)](#22-hello-world-in-host-code-cpu)\n",
    "  - [2.3 Hello World in Device Code (GPU)](#23-hello-world-in-device-code-gpu)\n",
    "  - [2.4 NDRange (Global Size), Work Groups, Work Items, Devices, CUs, and PEs](#24-ndrange-global-size-work-groups-work-items-devices-cus-and-pes)\n",
    "  - [2.5 Error Checking](#25-error-checking)\n",
    "  - [2.6 Measuring Execution Time on the Host (CPU) and on the Device (GPU)](#26-measuring-execution-time-on-the-host-cpu-and-on-the-device-gpu)\n",
    "  - [2.7 Shared Memory and Thread Synchronization on the Device (GPU)](#27-shared-memory-and-thread-synchronization-on-the-device-gpu)\n",
    "  - [2.8 Constant Memory on the Device (GPU)](#28-constant-memory-on-the-device-gpu)\n",
    "- [3. Sample Problems](#3-sample-problems)\n",
    "  - [3.1 1D Vector Addition on the Host (CPU)](#31-1d-vector-addition-on-the-host-cpu)\n",
    "  - [3.2 1D Vector Addition on the Device (GPU)](#32-1d-vector-addition-on-the-device-gpu)\n",
    "  - [3.3 1D Convolution on the Host (CPU)](#33-1d-convolution-on-the-host-cpu)\n",
    "  - [3.4 1D Convolution on the Device (GPU)](#34-1d-convolution-on-the-device-gpu)\n",
    "  - [3.5 2D Convolution on the Host (CPU)](#35-2d-convolution-on-the-host-cpu)\n",
    "  - [3.6 2D Convolution on the Device (GPU)](#36-2d-convolution-on-the-device-gpu)\n",
    "- [4. Cleanup](#4-cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c99674",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Prerequisites\n",
    "---\n",
    "\n",
    "## 1.1 Running the Notebook Locally or on Google CoLab\n",
    "\n",
    "Let's use the tool `clinfo` to list OpenCL-enabled devices on your system.\n",
    "- If you don't have `clinfo` installed, first install it on your system:\n",
    "  \n",
    "  **Linux**\n",
    "  \n",
    "  - Install `clinfo` with the [Advanced Package Tool](https://www.debian.org/doc/manuals/debian-faq/pkgtools.en.html#apt-get)\n",
    "    \n",
    "    ```bash\n",
    "    sudo apt install -y clinfo\n",
    "    ```\n",
    "\n",
    "  **Mac**\n",
    "  \n",
    "  - Install `clinfo` with [Homebrew](https://brew.sh)\n",
    "    \n",
    "    ```bash\n",
    "    brew install clinfo\n",
    "    ```\n",
    "\n",
    "  **Windows**\n",
    "  \n",
    "  - Download `clinfo.exe` from here: https://github.com/engineer1109/clinfo/releases/tag/v1.0\n",
    "  - Add it to your `PATH` environment variable (or just place it in your VSCode workspace folder).\n",
    "\n",
    "- Then run the cell below to check if you have an OpenCL-enabled device on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ffa325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of platforms                               1\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "  Platform Vendor                                 NVIDIA Corporation\n",
      "  Platform Version                                OpenCL 3.0 CUDA 12.8.90\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Platform Extensions with Version                cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "  Platform Numeric Version                        0xc00000 (3.0.0)\n",
      "  Platform Extensions function suffix             NV\n",
      "  Platform Host timer resolution                  0ns\n",
      "  Platform External memory handle types           Opaque FD\n",
      "  Platform Semaphore types                        <gatherPlatformInfo:11: get CL_PLATFORM_SEMAPHORE_TYPES_KHR size : error -30>\n",
      "  Platform External semaphore import types        Opaque FD\n",
      "  Platform External semaphore export types        <gatherPlatformInfo:13: get CL_PLATFORM_SEMAPHORE_EXPORT_HANDLE_TYPES_KHR : error -30>\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "Number of devices                                 1\n",
      "  Device Name                                     NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "  Device Vendor                                   NVIDIA Corporation\n",
      "  Device Vendor ID                                0x10de\n",
      "  Device Version                                  OpenCL 3.0 CUDA\n",
      "  Device UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Driver UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Valid Device LUID                               No\n",
      "  Device LUID                                     6d69-637300000000\n",
      "  Device Node Mask                                0\n",
      "  Device Numeric Version                          0xc00000 (3.0.0)\n",
      "  Driver Version                                  570.124.06\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 \n",
      "  Device OpenCL C all versions                    OpenCL C                                                         0x400000 (1.0.0)\n",
      "                                                  OpenCL C                                                         0x401000 (1.1.0)\n",
      "                                                  OpenCL C                                                         0x402000 (1.2.0)\n",
      "                                                  OpenCL C                                                         0xc00000 (3.0.0)\n",
      "  Device OpenCL C features                        __opencl_c_fp64                                                  0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_images                                                0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_int64                                                 0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_3d_image_writes                                       0xc00000 (3.0.0)\n",
      "  Latest conformance test passed                  v2023-10-10-00\n",
      "  Device Type                                     GPU\n",
      "  Device Topology (NV)                            PCI-E, 0000:01:00.0\n",
      "  Device PCI bus info (KHR)                       PCI-E, 0000:01:00.0\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               24\n",
      "  Max clock frequency                             1455MHz\n",
      "  Compute Capability (NV)                         8.9\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     1\n",
      "    Supported partition types                     None\n",
      "    Supported affinity domains                    (n/a)\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             1024x1024x64\n",
      "  Max work group size                             1024\n",
      "  Preferred work group size multiple (device)     32\n",
      "  Preferred work group size multiple (kernel)     32\n",
      "  Warp size (NV)                                  32\n",
      "  Max sub-groups per work group                   0\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                 1 / 1       \n",
      "    short                                                1 / 1       \n",
      "    int                                                  1 / 1       \n",
      "    long                                                 1 / 1       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                1 / 1       \n",
      "    double                                               1 / 1        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  External memory handle types                    Opaque FD\n",
      "  Semaphore types                                 <printDeviceInfo:105: get number of CL_DEVICE_SEMAPHORE_TYPES_KHR : error -30>\n",
      "  External semaphore import types                 Opaque FD\n",
      "  External semaphore export types                 (n/a)\n",
      "  Global memory size                              8198619136 (7.636GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           2049654784 (1.909GiB)\n",
      "  Unified memory for Host and Device              No\n",
      "  Integrated memory (NV)                          No\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       4096 bits (512 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           0 bytes\n",
      "    Global                                        0 bytes\n",
      "    Local                                         0 bytes\n",
      "  Atomic memory capabilities                      relaxed, work-group scope\n",
      "  Atomic fence capabilities                       relaxed, acquire/release, work-group scope\n",
      "  Max size for global variable                    0\n",
      "  Preferred total size of global vars             0\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        688128 (672KiB)\n",
      "  Global Memory cache line size                   128 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             32\n",
      "    Max size for 1D images from buffer            268435456 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   0 bytes\n",
      "    Pitch alignment for 2D image buffers          0 pixels\n",
      "    Max 2D image size                             32768x32768 pixels\n",
      "    Max 3D image size                             16384x16384x16384 pixels\n",
      "    Max number of read image args                 256\n",
      "    Max number of write image args                32\n",
      "    Max number of read/write image args           0\n",
      "  Pipe support                                    No\n",
      "  Max number of pipe args                         0\n",
      "  Max active pipe reservations                    0\n",
      "  Max pipe packet size                            0\n",
      "  Local memory type                               Local\n",
      "  Local memory size                               49152 (48KiB)\n",
      "  Registers per block (NV)                        65536\n",
      "  Max number of constant args                     9\n",
      "  Max constant buffer size                        65536 (64KiB)\n",
      "  Generic address space support                   No\n",
      "  Max size of kernel argument                     32764 (32KiB)\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "  Device enqueue capabilities                     (n/a)\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        No\n",
      "    Profiling                                     No\n",
      "    Preferred size                                0\n",
      "    Max size                                      0\n",
      "  Max queues on device                            0\n",
      "  Max events on device                            0\n",
      "  Prefer user sync for interop                    No\n",
      "  Profiling timer resolution                      1000ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            No\n",
      "    Non-uniform work-groups                       No\n",
      "    Work-group collective functions               No\n",
      "    Sub-group independent forward progress        No\n",
      "    Kernel execution timeout (NV)                 No\n",
      "    Concurrent copy and kernel execution (NV)     Yes\n",
      "      Number of async copy engines                2\n",
      "    IL version                                    (n/a)\n",
      "    ILs with version                              (n/a)\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                (n/a)\n",
      "  Built-in kernels with version                   (n/a)\n",
      "  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Device Extensions with Version                  cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "\n",
      "NULL platform behavior\n",
      "  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform\n",
      "  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform\n",
      "  clCreateContext(NULL, ...) [default]            No platform\n",
      "  clCreateContext(NULL, ...) [other]              Success [NV]\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  Invalid device type for platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No platform\n"
     ]
    }
   ],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ba36b",
   "metadata": {},
   "source": [
    "### Inspect the output from the cell above:\n",
    "- If you see a `Number of platforms` listed above with a value of at least `1`.\n",
    "  - Skip to [1.2 Operating System and VSCode Shell](#12-operating-system-and-vscode-shell)\n",
    "- If you don't see a `Number of platforms` listed above with a value of at least `1`, follow the instructions below.\n",
    "\n",
    "  1. Click the icon below to open the notebook in Google CoLab.\n",
    "     \n",
    "     [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paga-hb/C1PD2C_2025/blob/main/notebooks/opencl.ipynb)\n",
    "\n",
    "  2. When the notebook opens in CoLab, choose `File -> Save a copy in Drive` from the main menu.\n",
    "  3. Choose `Runtime -> Change runtime type` from the main menu, select `TP4 GPU` as the hardware accelerator, and click the `Save` button.\n",
    "  4. In a notebook cell, run the following code:\n",
    "\n",
    "      ```c\n",
    "      !sudo apt install -y gdb\n",
    "      ```\n",
    "\n",
    "  5. When the cell stops executing, continue with [1.2 Operating System and VSCode Shell](#12-operating-system-and-vscode-shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f98c6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Operating System and VSCode Shell\n",
    "\n",
    "We are going to compose JSON configuration files for VSCode, so let's collect some information about your environment.\n",
    "\n",
    "- Let's start by finding out what OS you are on and what default shell you are using in VSCode.\n",
    "\n",
    "**Linux/Mac/CoLab**\n",
    "\n",
    "- Run the cell below.\n",
    "\n",
    "**Windows**\n",
    "\n",
    "- Find out (or change) which shell you are using in VSCode.\n",
    "  - Open the Command Palette: `Ctrl + Shift + P`\n",
    "  - Enter the text (and press `<Enter>`): `Preferences: Open Settings (UI)`\n",
    "  - In the search field, enter the text: `terminal.integrated.defaultProfile.windows`\n",
    "  - Choose the tab `User` or `Workspace` (`User` are global settings, `Workspace` only applies to the current workspace)\n",
    "  - Click the link: `Edit in settings.json`\n",
    "  - Set your desired shell:\n",
    "    - `\"terminal.integrated.defaultProfile.windows\": \"PowerShell\"`\n",
    "    - `\"terminal.integrated.defaultProfile.windows\": \"Command Prompt\"`\n",
    "- Choose your VSCode shell in the cell below.\n",
    "  - If you are using Powershell:\n",
    "    - Comment the row `windows_shell = \"cmd\"`\n",
    "    - Uncomment the row `windows_shell = \"powershell\"`\n",
    "- Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3f3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System (OS) : linux\n"
     ]
    }
   ],
   "source": [
    "windows_shell_name = \"cmd\"\n",
    "#windows_shell_name = \"powershell\"\n",
    "\n",
    "import platform, os\n",
    "os_name = platform.system()\n",
    "if os_name == \"Darwin\":\n",
    "    os_name = \"osx\"\n",
    "os_name = os_name.lower()\n",
    "\n",
    "print(f\"{'Operating System (OS)':<21} : {os_name}\")\n",
    "if os_name == 'windows':\n",
    "    windows_shell_path = !where {windows_shell_name}\n",
    "    windows_shell_path = windows_shell_path[0]\n",
    "    windows_shell_name = os.path.basename(windows_shell_path)\n",
    "    print(f\"{'Windows Shell Name':<21} : {windows_shell_name}\")\n",
    "    print(f\"{'Windows Shell Path':<21} : {windows_shell_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0549db",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3 C Compiler (`gcc`, `clang`, `cl`)\n",
    "\n",
    "To avoid full paths to the C compiler and debugger in the JSON configuration files, make sure the path to the C compiler is in your `PATH` environment variable.\n",
    "\n",
    "**CoLab**\n",
    "- Run the cell below.\n",
    "\n",
    "**Linux/Mac/Windows**\n",
    "- In the cell below, choose the installed C compiler you want to use  \n",
    "  - If you are using `cl` (the C/C++ compiler, part of Microsoft Visual Studio build tools).\n",
    "    - Make sure you have launched VSCode from within a `Developer Command Prompt for VS`.\n",
    "      - Search in your Start Menu for `Developer Command Prompt for VS` (the version depends on your installed Visual Studio version).\n",
    "      - Open it => it launches a command prompt with all environment variables (paths, includes, libs) configured to run `cl.exe` and other build tools.\n",
    "      - Open VSCode from the command prompt: `code .`\n",
    "    - Comment the row `c_compiler = \"gcc\"`\n",
    "    - Uncomment the row `c_compiler = \"cl\"`\n",
    "  - If you are using `clang` (the C compiler, part of the LLVM project).\n",
    "    - Comment the row `c_compiler = \"gcc\"`\n",
    "    - Uncomment the row `c_compiler = \"clang\"`\n",
    "  - If you are using `gcc` (GNU Compiler Collection), you're all set.\n",
    "- Run the cell below to get the path to the C compiler.\n",
    "- If nothing shows up, you need to install a C compiler (and/or make sure the C compiler is in your `PATH` environment variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e700521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Compiler Name : gcc\n",
      "C Compiler Path : /usr/bin/gcc\n",
      "C Debugger Name : gdb\n",
      "C Debugger Path : /usr/bin/gdb\n"
     ]
    }
   ],
   "source": [
    "c_compiler = \"gcc\"\n",
    "#c_compiler = \"clang\"\n",
    "#c_compiler = \"cl\"\n",
    "\n",
    "import os\n",
    "if os_name == 'windows':\n",
    "    c_compiler_path = !where {c_compiler}\n",
    "else:\n",
    "    c_compiler_path = !which {c_compiler}\n",
    "c_compiler_path = c_compiler_path[0]\n",
    "c_compiler_name = os.path.basename(c_compiler_path)\n",
    "\n",
    "if c_compiler == 'cl':\n",
    "    c_debugger_name = \"cdb.exe\"\n",
    "    c_debugger_path = \"<integrated>\"\n",
    "if c_compiler == \"gcc\":\n",
    "    c_debugger_name = \"gdb\"\n",
    "if c_compiler == \"clang\":\n",
    "    c_debugger_name = \"lldb\"\n",
    "\n",
    "if os_name == 'windows':\n",
    "    if c_compiler != 'cl':\n",
    "        c_debugger_path = !where {c_debugger_name}\n",
    "else:\n",
    "    c_debugger_path = !which {c_debugger_name}\n",
    "\n",
    "if c_compiler != 'cl':\n",
    "    c_debugger_path = c_debugger_path[0]\n",
    "    c_debugger_name = os.path.basename(c_debugger_path)\n",
    "\n",
    "print(f\"{'C Compiler Name':<15} : {c_compiler_name}\")\n",
    "print(f\"{'C Compiler Path':<15} : {c_compiler_path}\")\n",
    "print(f\"{'C Debugger Name':<15} : {c_debugger_name}\")\n",
    "print(f\"{'C Debugger Path':<15} : {c_debugger_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c2c68",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.4 OpenCL Library and Header Files\n",
    "\n",
    "Let's find out where your OpenCL library and header files are located on your system.\n",
    "\n",
    "- Run the cell below to get the path to OpenCL's library and header files.\n",
    "- If nothing shows up, you need to install the OpenCL SDK for at least one device on your computer (and/or make sure environment variables are set up correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d0048ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/include/CL/cl.h\n",
      "/usr/local/cuda-12.8/targets/x86_64-linux/include/CL/cl.h\n",
      "/usr/local/cuda-12.8/targets/x86_64-linux/lib/libOpenCL.so\n",
      "/usr/lib/x86_64-linux-gnu/libOpenCL.so\n"
     ]
    }
   ],
   "source": [
    "if os_name == \"linux\":\n",
    "    !find /usr -name cl.h 2>/dev/null\n",
    "    !find /usr -name libOpenCL.so 2>/dev/null\n",
    "\n",
    "if os_name == \"osx\":\n",
    "    print(\"/System/Library/Frameworks/OpenCL.framework/Headers/\")\n",
    "    print(\"/System/Library/Frameworks/OpenCL.framework/OpenCL/\")\n",
    "\n",
    "if os_name == \"windows\":\n",
    "    shell_path = !echo %COMSPEC%\n",
    "    if os.path.basename(shell_path[0]) == \"powershell.exe\":\n",
    "        !Get-ChildItem -Recurse -Force -Filter \"cl.h\" -Path \"$env:USERPROFILE\" -ErrorAction SilentlyContinue\n",
    "        !Get-ChildItem -Recurse -Force -Filter \"cl.h\" -Path \"$env:PROGRAMFILES\" -ErrorAction SilentlyContinue\n",
    "        !Get-ChildItem -Recurse -Force -Filter \"OpenCl.lib\" -Path \"$env:USERPROFILE\" -ErrorAction SilentlyContinue\n",
    "        !Get-ChildItem -Recurse -Force -Filter \"OpenCl.lib\" -Path \"$env:PROGRAMFILES\" -ErrorAction SilentlyContinue\n",
    "    else:\n",
    "        !dir /s /b \"%USERPROFILE%\\cl.h\"\n",
    "        !dir /s /b \"%ProgramFiles%\\cl.h\"\n",
    "        !dir /s /b \"%USERPROFILE%\\OpenCl.lib\"\n",
    "        !dir /s /b \"%ProgramFiles%\\OpenCl.lib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caafe2",
   "metadata": {},
   "source": [
    "### From the paths listed above:\n",
    "- Choose one path to the header files (`.h`) listed above (enter the full path as listed above).\n",
    "- Choose one path to the library file (`.so` or `.lib`) listed above (enter the full path as listed above).\n",
    "- Enter the paths in the cell below.\n",
    "- Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e53001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCL Include Path : \"/usr/local/cuda-12.8/targets/x86_64-linux/include\"\n",
      "OpenCL Lib Path     : \"/usr/local/cuda-12.8/targets/x86_64-linux/lib\"\n"
     ]
    }
   ],
   "source": [
    "opencl_include_path = r\"/usr/local/cuda-12.8/targets/x86_64-linux/include/CL/cl.h\"\n",
    "opencl_lib_path = r\"/usr/local/cuda-12.8/targets/x86_64-linux/lib/libOpenCL.so\"\n",
    "\n",
    "import os\n",
    "opencl_include_dir = os.path.dirname(opencl_include_path)\n",
    "if os.path.basename(opencl_include_dir) == \"CL\":\n",
    "    opencl_include_dir = f'\"{os.path.dirname(opencl_include_dir)}\"'\n",
    "\n",
    "opencl_lib_dir = f'\"{os.path.dirname(opencl_lib_path)}\"'\n",
    "print(f\"{'OpenCL Include Path':<19} : {opencl_include_dir}\")\n",
    "print(f\"{'OpenCL Lib Path':<19} : {opencl_lib_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4407b8e",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.5 Configuring `tasks.json`, `launch.json`, and `c_cpp_properties.json`\n",
    "\n",
    "- To develop OpenCL programs in C with VSCode, we need to configure three VSCode workspace configuration files.\n",
    "  - In the file `tasks.json` we can configure various tasks, such as build tasks for compiling OpenCL programs in C with our chosen C compiler.\n",
    "  - In the file `launch.json` we can configure various debug options, such as debugging C programs with our chosen C debugger.\n",
    "  - In the file `c_cpp_properties.json` we can configure the compiler to use for linting purposes (intellisense).\n",
    "    - It isn't strictly necessary to create this configuration file to be able to run and debug C programs in VSCode.\n",
    "- VSCode workspace configuration files (`.json`) are stored in the subfolder `.vscode`.\n",
    "- Run the cell below to create the folder `.vscode`.\n",
    "\n",
    "**Note**\n",
    "\n",
    "- This notebook doesn't dscribe the contents of these three files in detail. To learn more, visit: \n",
    "  - [task.json](https://code.visualstudio.com/docs/debugtest/tasks)\n",
    "  - [launch.json](https://code.visualstudio.com/docs/debugtest/debugging)\n",
    "  - [c_cpp_properties.json](https://code.visualstudio.com/docs/cpp/configure-intellisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034371f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\".vscode\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205a57b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.6 Create the File `tasks.json`\n",
    "\n",
    "- Run the cell below to create the file `tasks.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374b2ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"HelloWorld\\\\INSERT\"\n"
     ]
    }
   ],
   "source": [
    "original = f'\"HelloWorld\"'\n",
    "to_insert = \"INSERT\"\n",
    "\n",
    "# Insert 'to_insert' before the second last character of 'original'\n",
    "result = original[:-1] + r\"\\\\\" + to_insert + original[-1:]\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12375bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "src_path = \"${workspaceFolder}/src/*.c\"\n",
    "include_path = \"${workspaceFolder}/include\"\n",
    "bin_path = \"${workspaceFolder}/bin/main.exe\"\n",
    "if os_name == \"windows\":\n",
    "    src_path = \"${workspaceFolder}\\\\src\\\\*.c\"\n",
    "    include_path = \"${workspaceFolder}\\\\include\"\n",
    "    bin_path = \"${workspaceFolder}\\\\bin\\\\main.exe\"\n",
    "\n",
    "makedir_command = \"mkdir\"\n",
    "makedir_args = [\"-p\", \"src\", \"include\", \"bin\"]\n",
    "if os_name == \"windows\":\n",
    "    makedir_command = windows_shell_path\n",
    "    if windows_shell_name == \"powershell.exe\":\n",
    "        makedir_args = [\"-NoProfile\", \"-ExecutionPolicy\", \"Bypass\", \"-Command\", \"New-Item -ItemType Directory -Path 'src','include','bin' -Force -ErrorAction SilentlyContinue\"]\n",
    "    else:\n",
    "        makedir_args = [\"/c\", \"if not exist src mkdir src & if not exist include mkdir include & if not exist bin mkdir bin\"]\n",
    "\n",
    "clean_command = \"find\"\n",
    "clean_args = [\"./bin\", \"-type\", \"f\", \"-name\", \"*.exe\", \"-delete\"]\n",
    "if os_name == \"windows\":\n",
    "    clean_command = windows_shell_path\n",
    "    if windows_shell_name == \"powershell.exe\":\n",
    "        clean_args = [\"-NoProfile\", \"-ExecutionPolicy\", \"Bypass\", \"-Command\", \"Get-ChildItem -Path .\\\\bin -Include *.exe, *.ilk, *.pdb, *.obj -Recurse | Remove-Item -Force\"]\n",
    "    else:\n",
    "        clean_args = [\"/c\", \"del /s /q /f .\\\\bin\\\\*.exe 2>nul .\\\\bin\\\\*.ilk 2>nul .\\\\bin\\\\*.pdb 2>nul .\\\\bin\\\\*.obj pdb 2>nul\"]\n",
    "\n",
    "c_build_command = c_compiler_path\n",
    "c_build_multi_args = [\"-std=c17\", \"-Wall\", \"-g\", src_path, \"-I\", include_path, \"-o\", bin_path] \n",
    "c_build_active_args = [\"-std=c17\", \"-Wall\", \"-g\", \"${file}\", \"-o\", bin_path]\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    c_build_multi_args = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"src\\\\*.c\", \"/I\", \"include\"]\n",
    "    c_build_active_args = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"${file}\"]\n",
    "\n",
    "c_build_multi_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", src_path, \"-I\", include_path, \"-I\", opencl_include_dir, \"-L\", opencl_lib_dir, \"-l\", \"OpenCL\", \"-o\", bin_path] \n",
    "c_build_active_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", \"${file}\", \"-I\", opencl_include_dir, \"-L\", opencl_lib_dir, \"-l\", \"OpenCL\", \"-o\", bin_path]\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    opencl_lib_file = opencl_lib_dir[:-1] + r\"\\OpenCL.lib\" + opencl_lib_dir[-1:]\n",
    "    c_build_multi_args_opencl = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"src\\\\*.c\", \"/I\", \"include\", \"/I\", opencl_include_dir, \"/link\", opencl_lib_file]\n",
    "    c_build_active_args_opencl = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"${file}\", \"/I\", opencl_include_dir, \"/link\",  opencl_lib_file]\n",
    "if os_name == \"osx\" and c_compiler_name == \"clang\":\n",
    "    c_build_multi_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", src_path, \"-I\", include_path, \"-framework\", \"OpenCL\", \"-o\", bin_path] \n",
    "    c_build_active_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", \"${file}\", \"-framework\", \"OpenCL\", \"-o\", bin_path]\n",
    "\n",
    "tasks_json = {\n",
    "    \"version\": \"2.0.0\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"Make directories\",\n",
    "            \"command\": makedir_command,\n",
    "            \"args\": makedir_args,\n",
    "            \"problemMatcher\": []\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"Clean .exe files\",\n",
    "            \"dependsOn\": [\"Make directories\"],\n",
    "            \"command\": clean_command,\n",
    "            \"args\": clean_args,\n",
    "            \"problemMatcher\": []\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"opencl: build multi file\",\n",
    "            \"dependsOn\": [\"Clean .exe files\"],\n",
    "            \"command\": c_build_command,\n",
    "            \"args\": c_build_multi_args_opencl,\n",
    "            \"options\": {\n",
    "                \"cwd\": \"${workspaceFolder}\"\n",
    "            },\n",
    "            \"problemMatcher\": [\n",
    "                \"$gcc\"\n",
    "            ],\n",
    "            \"group\": {\n",
    "                \"kind\": \"build\",\n",
    "                \"isDefault\": False\n",
    "            },\n",
    "            \"detail\": f\"compiler: {c_compiler_path}\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"opencl: build active file\",\n",
    "            \"dependsOn\": [\"Clean .exe files\"],\n",
    "            \"command\": c_build_command,\n",
    "            \"args\": c_build_active_args_opencl,\n",
    "            \"options\": {\n",
    "                \"cwd\": \"${fileDirname}\"\n",
    "            },\n",
    "            \"problemMatcher\": [\n",
    "                \"$gcc\"\n",
    "            ],\n",
    "            \"group\": {\n",
    "                \"kind\": \"build\",\n",
    "                \"isDefault\": True\n",
    "            },\n",
    "            \"detail\": f\"compiler: {c_compiler_path}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "json_string = json.dumps(tasks_json, indent=4)\n",
    "with open(\".vscode/tasks.json\", \"w\") as f:\n",
    "    json.dump(tasks_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88dea1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.7 Create the File `launch.json`\n",
    "\n",
    "- Run the cell below to create the file `launch.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70ff14b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "if \"gdb\" in c_debugger_name:\n",
    "    c_debugger_type = \"cppdbg\"\n",
    "    c_debugger_mi_mode = \"gdb\"\n",
    "    stop_at_entry_name = \"stopAtEntry\"\n",
    "    environment = True\n",
    "    console = False\n",
    "    setupcommands = [{\"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": True}]\n",
    "if \"lldb\" in c_debugger_name:\n",
    "    c_debugger_type = \"lldb\"\n",
    "    c_debugger_mi_mode = \"lldb\"\n",
    "    stop_at_entry_name = \"stopOnEntry\"\n",
    "    environment = False\n",
    "    console = False\n",
    "    setupcommands = None\n",
    "if c_debugger_name == \"cdb.exe\":\n",
    "    c_debugger_name = \"msvc\"\n",
    "    c_debugger_path = None\n",
    "    c_debugger_type = \"cppvsdbg\"\n",
    "    c_debugger_mi_mode = None\n",
    "    stop_at_entry_name = \"stopAtEntry\"\n",
    "    environment = True\n",
    "    console = True\n",
    "    setupcommands = None\n",
    "\n",
    "launch_json = {\n",
    "    \"version\": \"0.2.0\",\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": \"opencl: launch multi file\",\n",
    "            \"preLaunchTask\": \"opencl: build multi file\",\n",
    "            \"type\": c_debugger_type,\n",
    "            \"request\": \"launch\",\n",
    "            \"program\": bin_path,\n",
    "            \"args\": [],\n",
    "            f\"{stop_at_entry_name}\": False,\n",
    "            \"cwd\": \"${workspaceFolder}\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"opencl: launch active file\",\n",
    "            \"preLaunchTask\": \"opencl: build active file\",\n",
    "            \"type\": c_debugger_type,\n",
    "            \"request\": \"launch\",\n",
    "            \"program\": bin_path,\n",
    "            \"args\": [],\n",
    "            f\"{stop_at_entry_name}\": False,\n",
    "            \"cwd\": \"${workspaceFolder}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "if environment:\n",
    "    for i in range(2):\n",
    "        launch_json[\"configurations\"][i][\"environment\"] = []\n",
    "if c_debugger_name != \"lldb\":\n",
    "    if console:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"console\"] = \"integratedTerminal\" # \"externalTerminal\"\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"externalConsole\"] = False # True\n",
    "    if c_debugger_mi_mode:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"MIMode\"] = c_debugger_mi_mode\n",
    "    if c_debugger_path:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"miDebuggerPath\"] = c_debugger_path\n",
    "    if setupcommands:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"setupCommands\"] = setupcommands\n",
    "\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "with open(\".vscode/launch.json\", \"w\") as f:\n",
    "    json.dump(launch_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feec20f",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.8 Create the File `c_cpp_properties.json`\n",
    "\n",
    "- Run the cell below to create the file `c_cpp_properties.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f2d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "if os_name == \"linux\" and c_compiler_name == \"gcc\":\n",
    "    intelliSenseMode = \"linux-gcc-x64\"\n",
    "    # intelliSenseMode = \"linux-gcc-arm64\"\n",
    "if os_name == \"linux\" and c_compiler_name == \"clang\":\n",
    "    intelliSenseMode = \"linux-clang-x64\"\n",
    "    # intelliSenseMode = \"linux-clang-arm64\"\n",
    "if os_name == \"osx\" and c_compiler_name == \"gcc\":\n",
    "    intelliSenseMode = \"macos-gcc-x64\"\n",
    "    # intelliSenseMode = \"macos-gcc-arm64\"\n",
    "if os_name == \"osx\" and c_compiler_name == \"clang\":\n",
    "    intelliSenseMode = \"macos-clang-x64\"\n",
    "    # intelliSenseMode = \"macos-clang-arm64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"gcc.exe\":\n",
    "    intelliSenseMode = \"windows-gcc-x64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"clang.exe\":\n",
    "    intelliSenseMode = \"windows-clang-x64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    intelliSenseMode = \"windows-msvc-x64\"\n",
    "\n",
    "launch_json = {\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": \"Linter\",\n",
    "            \"includePath\": [\n",
    "                \"${workspaceFolder}/**\"\n",
    "            ],\n",
    "            \"defines\": [],\n",
    "            \"compilerPath\": c_compiler_path,\n",
    "            \"cStandard\": \"c17\",\n",
    "            \"cppStandard\": \"c++17\",\n",
    "            \"intelliSenseMode\": intelliSenseMode\n",
    "        }\n",
    "    ],\n",
    "    \"version\": 4\n",
    "}\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "with open(\".vscode/c_cpp_properties.json\", \"w\") as f:\n",
    "    json.dump(launch_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6828fd9",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.9 VSCode Extensions\n",
    "\n",
    "**CoLab**\n",
    "- SKip to [1.10 Using Built-in Cell Magic `%%writefile`](#110-using-built-in-cell-magic-writefile).\n",
    "\n",
    "**Linux/Mac/Windows**\n",
    "\n",
    "- To develop OpenCL programs in C with VSCode, we need a few VSCode extensions (the last two are only needed for Jupyter Notebooks).\n",
    "  - C/C++ Extension Pack: https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools-extension-pack\n",
    "  - CodeLLDB: https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb\n",
    "  - Makefile Tools: https://marketplace.visualstudio.com/items?itemName=ms-vscode.makefile-tools\n",
    "  - Jupyter: https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter\n",
    "  - Python: https://marketplace.visualstudio.com/items?itemName=ms-python.python\n",
    "\n",
    "- Run the cell below to install any missing VSCode extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d66c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing extensions...\n",
      "Extension 'ms-vscode.cpptools-extension-pack' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'vadimcn.vscode-lldb' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-vscode.makefile-tools' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-toolsai.jupyter' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-python.python' is already installed.\n"
     ]
    }
   ],
   "source": [
    "!code --install-extension ms-vscode.cpptools-extension-pack --force\n",
    "!code --install-extension vadimcn.vscode-lldb --force\n",
    "!code --install-extension ms-vscode.makefile-tools --force\n",
    "!code --install-extension ms-toolsai.jupyter --force\n",
    "!code --install-extension ms-python.python --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca922521",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.10 Using Built-in Cell Magic `%%writefile`\n",
    "\n",
    "- The cell magic `%%writefile filename`, writes the contents of a notebook cell to the specified `filename` (or file path).\n",
    "  - This functionality is built-in to Jupyter Notebooks (it's not an extension).\n",
    "  - We can use it to write any code cell contents to a file in the file system.\n",
    "  - Let's write some OpenCL code to the file `main.c` with a kernel in `kernel.c`.\n",
    "- Run the two cells below.\n",
    "- Then inspect the resulting files `kernel.cl` and `main.c`, which contain each code cell's contents (except for the cell magic command `%%writefile filename`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e14b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "const char *source =\n",
    "\"__kernel void mykernel()\\n\"\n",
    "\"{\\n\"\n",
    "\"    printf(\\\"Hello World!\\\\n\\\");\\n\"\n",
    "\"}\\n\";\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &err);\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "\n",
    "    clFinish(queue);\n",
    "\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae19c0",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.11 Compiling and Executing an OpenCL Program From a Notebook Code Cell\n",
    "\n",
    "- We can compile and execute an OpenCL program from a notebook code cell using the syntax `!<shell command>`, where:\n",
    "  - `!` indicates that the succeeding text on the same row should be sent to the shell (terminal).\n",
    "  - `<shell command>` is the shell (terminal) command we want to execute.\n",
    "  - Standard output is redirected to the cell output.\n",
    "\n",
    "- Run the cell below to see what the build command and execute command is in your shell:\n",
    "  - The *build single file* command compiles and links the file `main.c` in your workspace folder and places the executable file `main.exe` in the `bin` folder.\n",
    "  - The *build multi file* command compiles and links all `.c` files in the `src` and `.h` files in the `include` folder and places the executable file `main.exe` in the `bin` folder.\n",
    "  - The *execute* command executes the file `main.exe` in the `bin` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95fadcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build single file command : \"/usr/bin/gcc\" -std=c17 -Wall -g main.c -I \"/usr/local/cuda-12.8/targets/x86_64-linux/include\" -L \"/usr/local/cuda-12.8/targets/x86_64-linux/lib\" -l OpenCL -o ./bin/main.exe\n",
      "Build multi file command  : \"/usr/bin/gcc\" -std=c17 -Wall -g ./src/*.c -I ./include -I \"/usr/local/cuda-12.8/targets/x86_64-linux/include\" -L \"/usr/local/cuda-12.8/targets/x86_64-linux/lib\" -l OpenCL -o ./bin/main.exe\n",
      "Execute command           : ./bin/main.exe\n"
     ]
    }
   ],
   "source": [
    "build_single_file_command = [f'\"{c_build_command}\"'] + c_build_active_args_opencl\n",
    "build_single_file_command = \" \".join(build_single_file_command).replace('${file}', 'main.c').replace('${workspaceFolder}', '.')\n",
    "\n",
    "build_multi_file_command = [f'\"{c_build_command}\"'] + c_build_multi_args_opencl\n",
    "build_multi_file_command = \" \".join(build_multi_file_command).replace('${file}', 'main.c').replace('${workspaceFolder}', '.')\n",
    "\n",
    "execute_command = bin_path.replace('${workspaceFolder}', '.')\n",
    "\n",
    "print(f'Build single file command : {build_single_file_command}')\n",
    "print(f'Build multi file command  : {build_multi_file_command}')\n",
    "print(f'Execute command           : {execute_command}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7298415",
   "metadata": {},
   "source": [
    "- Run the cell below to:\n",
    "  - Create the folders `bin`, `src`, and `include` in your workspace folder if they don't already exist.\n",
    "  - Build the single source code file `main.c` in your workspace folder into the executable file `main.exe` in the `bin` folder.\n",
    "  - Run the executable file `main.exe` in the `bin` folder.\n",
    "- Notice the file `main.exe` has been created in the file system (in the `bin` folder), and the program's output is shown as the cell's output in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6645bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "os.makedirs(\"include\", exist_ok=True)\n",
    "os.makedirs(\"bin\", exist_ok=True)\n",
    "\n",
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68970095",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.12 Compiling and Debugging a Single-file OpenCL Program\n",
    "\n",
    "Let's see `tasks.json` and `launch.json` in action for a single-file (`.c`) OpenCL program.\n",
    "\n",
    "**CoLab**\n",
    "- Skip to [2. OpenCL Basics](#2-opencl-basics).\n",
    "\n",
    "**Linux/Windows/Mac**\n",
    "- First, let's create the file `main.c` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7574216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "const char *source =\n",
    "\"__kernel void mykernel()\\n\"\n",
    "\"{\\n\"\n",
    "\"    printf(\\\"Hello World!\\\\n\\\");\\n\"\n",
    "\"}\\n\";\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        fprintf(stderr, \"%s failed: %d\\n\", msg, err);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "\n",
    "    // 3. Context\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "\n",
    "    // 4. Command queue\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "\n",
    "    // 5. Program\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateProgramWithSource\");\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "\n",
    "    // (Optional) Check build log if needed\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    // 6. Kernel\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "    checkOpenCL(err, \"clCreateKernel\");\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "\n",
    "    // 8. Wait and finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d672e0",
   "metadata": {},
   "source": [
    "- Now, let's debug the file `main.c`.\n",
    "  - Open the file `main.c` in VSCode's editor.\n",
    "  - Set a breakpoint on the two `return` statements (`F9`).\n",
    "  - Switch to the `Run and Debug` view (Linux/Windows: `Ctrl + Shift + D`, Mac: `Cmd + Shift + D`).\n",
    "  - In the drop-down combobox, select the launch configuration `<COMPILER>: launch active file`, where `<COMPILER>`is the name of your C compiler.\n",
    "  - Click the green `Play` icon.\n",
    "  - Use the debug toolbar in the top-middle of VSCode to debug the code.\n",
    "    - Notice the debugger stops at the breakpoints.\n",
    "      - This is because we are using a C debugger compatible with your chosen C compiler.\n",
    "    - Notice you can view variables (local, registers), watch variables, view the call stack, and toggle breakpoints in the `Run and Debug` view.\n",
    "  - Stop debugging (red `Square` icon in the debug toolbar).\n",
    "- Next, look at the status bar (at the bottom of VSCode) where you will see the name of the launch configuration `<COMPILER>: launch active file`.\n",
    "  - Click on it, and select `<COMPILER>: launch active file` again (make sure `main.c` is the active file in the editor, not the notebook).\n",
    "    - This is an alternative method to start a debug session.\n",
    "  - Stop debugging.\n",
    "- Press `F5` (make sure `main.c` is active in VSCode's editor), which is a third alternative to launch the debugger.\n",
    "  - This launches the debug configuration with `preLaunchTask` set to the default task (in `tasks.json`).\n",
    "  - Stop debugging.\n",
    "- Press (Linux/Windows: `Ctrl + Shift + B`, Mac: `Ctrl + Shift + B`) to execute the default build task (in `tasks.json`).\n",
    "  - Make sure `main.c` is active in VSCode's editor (since the default build task is set to the active file task).\n",
    "  - Notice the compiled executable `main.exe` is placed in the subfolder `bin` (configured in the default build task).\n",
    "    - This is also where the debugger finds the executable `main.exe` (configured in `launch.json`).\n",
    "\n",
    "- Remeber, you can always compile a single `main.c` file in your workspace folder and run it using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d05101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3a935",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.13 Compiling and Debugging a Multi-file OpenCL Program\n",
    "\n",
    "**CoLab**\n",
    "- Skip to [2. OpenCL Basics](#2-opencl-basics).\n",
    "\n",
    "**Linux/Windows/Mac**\n",
    "- Let's see `tasks.json` and `launch.json` in action for a multi-file (`*.c`) OpenCL program.\n",
    "  - First We will create two source code files `.c` in the `src` folder, and one header file `.h` in the `include` folder.\n",
    "    - We will use the same code as before, but will place the OpenCL kernel (function) code in its own `.cl` file, together with a source code file `.c` for loading it and its prototype in a header file `.h`.\n",
    "  - Then we will use:\n",
    "    - The other (non-default) build task in `tasks.json` to build the executable.\n",
    "    - The other launch configuration (linked to the non-default build task) in `launch.json` to debug it.\n",
    "\n",
    "- Run the two cells below to create:\n",
    "  - The kernel source code file `kernel.cl` in the folder `src`.\n",
    "  - The main source code file `main.c` in the folder `src`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5300a14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel()\n",
    "{\n",
    "    printf(\"Hello World!\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "881eae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "#define KERNEL_FILE \"src/kernel.cl\"\n",
    "\n",
    "char* read_kernel_source(const char* filename)\n",
    "{\n",
    "    FILE* fp = fopen(filename, \"r\");\n",
    "    if (!fp)\n",
    "    {\n",
    "        fprintf(stderr, \"Failed to open kernel file: %s\\n\", filename);\n",
    "        exit(1);\n",
    "    }\n",
    "    fseek(fp, 0, SEEK_END);\n",
    "    long size = ftell(fp);\n",
    "    rewind(fp);\n",
    "    char* src = (char*)malloc(size + 1);\n",
    "    fread(src, 1, size, fp);\n",
    "    src[size] = '\\0';\n",
    "    fclose(fp);\n",
    "    return src;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "\n",
    "    // 3. Context\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "\n",
    "    // 4. Command queue\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "\n",
    "    // 5. Program\n",
    "    char* source = read_kernel_source(KERNEL_FILE); // Read kernel source\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, (const char**)&source, NULL, &err);\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "\n",
    "    // (Optional) Check build log if needed\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    // 6. Kernel\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "\n",
    "    // 8. Wait and finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    free(source);\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f5850",
   "metadata": {},
   "source": [
    "- Build the multi-file C program.\n",
    "  - Notice the multi-file build task in `tasks.json` isn't the default build task (`isDefault` is not set to `true` under `group`).\n",
    "  - Therefore, we can't use (Linux/Windows: `Ctrl + Shift + B`, mac: `Cmd + Shift + B`).\n",
    "  - Instead we can:\n",
    "    - Bring up the Command Palette (Linux/Windows: `Ctrl + Shift + P`, mac: `Cmd + Shift + P`).\n",
    "    - Choose `Tasks: Run Task` and select the task `<COMPILER>: build multi file`, where `<COMPILER>` is the name of your chosen C compiler.\n",
    "  - The executable `main.exe` is placed in the `bin` folder (as configured in `tasks.json`).\n",
    "- Debug the multi-file C program.\n",
    "  - Open `main.c` and `math_utils.c` in the `src` folder and set breakpoints on the two `return` statements.\n",
    "  - Notice the multi-file launch task in `launch.json` isn't linked to the default build task (in `tasks.json`).\n",
    "    - Therefore, we can't use `F5`.\n",
    "    - Instead we can:\n",
    "      - Switch the the `Run and Debug` view, select `<COMPILER>: launch multi file` from the drop-down list, and click the green `Play` icon.\n",
    "      - Or select `<COMPILER>: launch multi file` from the status bar (at the bottom of VSCode).\n",
    "    - The C program is built and the debugger lauched, attaching to the executable `main.exe` in the `bin` folder.\n",
    "  - Stop debugging.\n",
    "\n",
    "- Remeber, you can always compile a multi-file C program (`src/*.c`, `include/*.h`) and run it using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3aae85f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa3231",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. OpenCL Basics\n",
    "---\n",
    "\n",
    "- Now we know how to create OpenCL programs in C (single-file, multi-file, and in a Jupyter Notebook cell).\n",
    "- Going forward, we will explore fundamental OpenCL programming concepts as single-file programs in notebook cells using the `%%writefile` cell magic command.\n",
    "- The Khronos Group's documentation is a good source to learn more about OpenCL:\n",
    "    - [Khronos Group OpenCL](https://www.khronos.org/opencl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a988e3b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 Listing OpenCL-enabled Devices and Properties\n",
    "\n",
    "First, let's find out what OpenCL-enabled devices are available on your computer.\n",
    "\n",
    "### Using `clinfo`\n",
    "\n",
    "- The simplest way to list OpenCL-enabled devices is using the tool `clinfo`.\n",
    "  - The top row shows you the number of platforms on your system:\n",
    "    - Example platforms: Nvidia, AMD, Intel, etc.\n",
    "  - For each platform, it shows you which devices are available:\n",
    "    - Example devices: an Nvidia GPU, an AMD CPU, an AMD GPU, an Intel CPU, etc.\n",
    "  - For each device, it shows you information about that device.\n",
    "- Run the cell below to see the OpenCL-enabled devices on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af0c17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of platforms                               1\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "  Platform Vendor                                 NVIDIA Corporation\n",
      "  Platform Version                                OpenCL 3.0 CUDA 12.8.90\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Platform Extensions with Version                cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "  Platform Numeric Version                        0xc00000 (3.0.0)\n",
      "  Platform Extensions function suffix             NV\n",
      "  Platform Host timer resolution                  0ns\n",
      "  Platform External memory handle types           Opaque FD\n",
      "  Platform Semaphore types                        <gatherPlatformInfo:11: get CL_PLATFORM_SEMAPHORE_TYPES_KHR size : error -30>\n",
      "  Platform External semaphore import types        Opaque FD\n",
      "  Platform External semaphore export types        <gatherPlatformInfo:13: get CL_PLATFORM_SEMAPHORE_EXPORT_HANDLE_TYPES_KHR : error -30>\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "Number of devices                                 1\n",
      "  Device Name                                     NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "  Device Vendor                                   NVIDIA Corporation\n",
      "  Device Vendor ID                                0x10de\n",
      "  Device Version                                  OpenCL 3.0 CUDA\n",
      "  Device UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Driver UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Valid Device LUID                               No\n",
      "  Device LUID                                     6d69-637300000000\n",
      "  Device Node Mask                                0\n",
      "  Device Numeric Version                          0xc00000 (3.0.0)\n",
      "  Driver Version                                  570.124.06\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 \n",
      "  Device OpenCL C all versions                    OpenCL C                                                         0x400000 (1.0.0)\n",
      "                                                  OpenCL C                                                         0x401000 (1.1.0)\n",
      "                                                  OpenCL C                                                         0x402000 (1.2.0)\n",
      "                                                  OpenCL C                                                         0xc00000 (3.0.0)\n",
      "  Device OpenCL C features                        __opencl_c_fp64                                                  0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_images                                                0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_int64                                                 0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_3d_image_writes                                       0xc00000 (3.0.0)\n",
      "  Latest conformance test passed                  v2023-10-10-00\n",
      "  Device Type                                     GPU\n",
      "  Device Topology (NV)                            PCI-E, 0000:01:00.0\n",
      "  Device PCI bus info (KHR)                       PCI-E, 0000:01:00.0\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               24\n",
      "  Max clock frequency                             1455MHz\n",
      "  Compute Capability (NV)                         8.9\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     1\n",
      "    Supported partition types                     None\n",
      "    Supported affinity domains                    (n/a)\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             1024x1024x64\n",
      "  Max work group size                             1024\n",
      "  Preferred work group size multiple (device)     32\n",
      "  Preferred work group size multiple (kernel)     32\n",
      "  Warp size (NV)                                  32\n",
      "  Max sub-groups per work group                   0\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                 1 / 1       \n",
      "    short                                                1 / 1       \n",
      "    int                                                  1 / 1       \n",
      "    long                                                 1 / 1       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                1 / 1       \n",
      "    double                                               1 / 1        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  External memory handle types                    Opaque FD\n",
      "  Semaphore types                                 <printDeviceInfo:105: get number of CL_DEVICE_SEMAPHORE_TYPES_KHR : error -30>\n",
      "  External semaphore import types                 Opaque FD\n",
      "  External semaphore export types                 (n/a)\n",
      "  Global memory size                              8198619136 (7.636GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           2049654784 (1.909GiB)\n",
      "  Unified memory for Host and Device              No\n",
      "  Integrated memory (NV)                          No\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       4096 bits (512 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           0 bytes\n",
      "    Global                                        0 bytes\n",
      "    Local                                         0 bytes\n",
      "  Atomic memory capabilities                      relaxed, work-group scope\n",
      "  Atomic fence capabilities                       relaxed, acquire/release, work-group scope\n",
      "  Max size for global variable                    0\n",
      "  Preferred total size of global vars             0\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        688128 (672KiB)\n",
      "  Global Memory cache line size                   128 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             32\n",
      "    Max size for 1D images from buffer            268435456 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   0 bytes\n",
      "    Pitch alignment for 2D image buffers          0 pixels\n",
      "    Max 2D image size                             32768x32768 pixels\n",
      "    Max 3D image size                             16384x16384x16384 pixels\n",
      "    Max number of read image args                 256\n",
      "    Max number of write image args                32\n",
      "    Max number of read/write image args           0\n",
      "  Pipe support                                    No\n",
      "  Max number of pipe args                         0\n",
      "  Max active pipe reservations                    0\n",
      "  Max pipe packet size                            0\n",
      "  Local memory type                               Local\n",
      "  Local memory size                               49152 (48KiB)\n",
      "  Registers per block (NV)                        65536\n",
      "  Max number of constant args                     9\n",
      "  Max constant buffer size                        65536 (64KiB)\n",
      "  Generic address space support                   No\n",
      "  Max size of kernel argument                     32764 (32KiB)\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "  Device enqueue capabilities                     (n/a)\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        No\n",
      "    Profiling                                     No\n",
      "    Preferred size                                0\n",
      "    Max size                                      0\n",
      "  Max queues on device                            0\n",
      "  Max events on device                            0\n",
      "  Prefer user sync for interop                    No\n",
      "  Profiling timer resolution                      1000ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            No\n",
      "    Non-uniform work-groups                       No\n",
      "    Work-group collective functions               No\n",
      "    Sub-group independent forward progress        No\n",
      "    Kernel execution timeout (NV)                 No\n",
      "    Concurrent copy and kernel execution (NV)     Yes\n",
      "      Number of async copy engines                2\n",
      "    IL version                                    (n/a)\n",
      "    ILs with version                              (n/a)\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                (n/a)\n",
      "  Built-in kernels with version                   (n/a)\n",
      "  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Device Extensions with Version                  cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "\n",
      "NULL platform behavior\n",
      "  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform\n",
      "  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform\n",
      "  clCreateContext(NULL, ...) [default]            No platform\n",
      "  clCreateContext(NULL, ...) [other]              Success [NV]\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  Invalid device type for platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No platform\n"
     ]
    }
   ],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cc78b",
   "metadata": {},
   "source": [
    "### Using C Code\n",
    "\n",
    "- We can also find out what Platforms and OpenCL-enabled devices are available using C code.\n",
    "  - The code below lists important properties that will become familiar the more you learn about OpenCL (for optimization purposes).\n",
    "- Run the cell below to list platforms, OpenCL-enabled devices, and (some of) their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "394e2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        fprintf(stderr, \"%s failed: %d\\n\", msg, err);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_uint numPlatforms;\n",
    "    cl_int err;\n",
    "\n",
    "    err = clGetPlatformIDs(0, NULL, &numPlatforms);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs (count)\");\n",
    "\n",
    "    if (numPlatforms == 0)\n",
    "    {\n",
    "        puts(\"No OpenCL platforms found.\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    cl_platform_id *platforms = (cl_platform_id *) malloc(sizeof(cl_platform_id) * numPlatforms);\n",
    "    err = clGetPlatformIDs(numPlatforms, platforms, NULL);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs\");\n",
    "\n",
    "    for (cl_uint p = 0; p < numPlatforms; ++p)\n",
    "    {\n",
    "        char platformName[128];\n",
    "        clGetPlatformInfo(platforms[p], CL_PLATFORM_NAME, sizeof(platformName), platformName, NULL);\n",
    "        printf(\"Platform %u: %s\\n\", p, platformName);\n",
    "\n",
    "        cl_uint numDevices = 0;\n",
    "        err = clGetDeviceIDs(platforms[p], CL_DEVICE_TYPE_ALL, 0, NULL, &numDevices);\n",
    "        if (err != CL_SUCCESS || numDevices == 0)\n",
    "        {\n",
    "            puts(\"  No devices found.\");\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        cl_device_id *devices = (cl_device_id *) malloc(sizeof(cl_device_id) * numDevices);\n",
    "        err = clGetDeviceIDs(platforms[p], CL_DEVICE_TYPE_ALL, numDevices, devices, NULL);\n",
    "        checkOpenCL(err, \"clGetDeviceIDs\");\n",
    "\n",
    "        for (cl_uint i = 0; i < numDevices; ++i)\n",
    "        {\n",
    "            char name[128];\n",
    "            cl_uint compute_units, max_work_group_size, clock_frequency;\n",
    "            cl_ulong global_mem, local_mem, constant_mem;\n",
    "            cl_device_type type;\n",
    "\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(name), name, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(type), &type, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_COMPUTE_UNITS, sizeof(compute_units), &compute_units, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(max_work_group_size), &max_work_group_size, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_CLOCK_FREQUENCY, sizeof(clock_frequency), &clock_frequency, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_GLOBAL_MEM_SIZE, sizeof(global_mem), &global_mem, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_LOCAL_MEM_SIZE, sizeof(local_mem), &local_mem, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE, sizeof(constant_mem), &constant_mem, NULL);\n",
    "\n",
    "            printf(\"  - Device %u: %s\\n\", i, name);\n",
    "            printf(\"    - %-31s : %s\\n\", \"Type\",\n",
    "                (type == CL_DEVICE_TYPE_GPU) ? \"GPU\" :\n",
    "                (type == CL_DEVICE_TYPE_CPU) ? \"CPU\" :\n",
    "                (type == CL_DEVICE_TYPE_ACCELERATOR) ? \"Accelerator\" : \"Other\");\n",
    "            printf(\"    - %-31s : %u\\n\", \"Compute Units (CU)\", compute_units);\n",
    "            printf(\"    - %-31s : %u\\n\", \"Max Work Group Size\", max_work_group_size);\n",
    "            printf(\"    - %-31s : %.2f MHz\\n\", \"Clock Frequency\", clock_frequency * 1.0);\n",
    "            printf(\"    - %-31s : %llu bytes\\n\", \"Global Memory Size\", (long long unsigned int)global_mem);\n",
    "            printf(\"    - %-31s : %llu bytes\\n\", \"Local Memory Size (shared)\", (long long unsigned int)local_mem);\n",
    "            printf(\"    - %-31s : %llu bytes\\n\", \"Constant Memory Size\", (long long unsigned int)constant_mem);\n",
    "            printf(\"\\n\");\n",
    "        }\n",
    "\n",
    "        free(devices);\n",
    "    }\n",
    "\n",
    "    free(platforms);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89a47e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform 0: NVIDIA CUDA\n",
      "  - Device 0: NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "    - Type                            : GPU\n",
      "    - Compute Units (CU)              : 24\n",
      "    - Max Work Group Size             : 32766\n",
      "    - Clock Frequency                 : 1455.00 MHz\n",
      "    - Global Memory Size              : 8198619136 bytes\n",
      "    - Local Memory Size (shared)      : 49152 bytes\n",
      "    - Constant Memory Size            : 65536 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd8e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Hello World in Host Code (CPU)\n",
    "\n",
    "- The program in the cell below is a simple C program (no OpenCL code) that runs on the host (CPU).\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`.\n",
    "\n",
    "      ```c\n",
    "      #include <stdio.h>\n",
    "      ```\n",
    "  - Then we define the `main()` function:\n",
    "    - We print out the text `Hello World!`.\n",
    "    - Then we return the exit code `0` to the operating system.\n",
    "    \n",
    "      ```c\n",
    "      int main(void)\n",
    "      {\n",
    "        printf(\"Hello World!\\n\");\n",
    "        \n",
    "        return 0;\n",
    "      }\n",
    "      ```\n",
    "- Run the cell below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d0042a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "\n",
    "// Host entry point (a normal C main function)\n",
    "int main(void)\n",
    "{\n",
    "   printf(\"Hello World!\\n\");\n",
    "   \n",
    "   return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63150b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763d82e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Hello World in Device Code (GPU)\n",
    "\n",
    "- The program in the cell below is a simple OpenCL program that runs code on the host (CPU) and the device (GPU).\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf` and `fprintf`.\n",
    "    - `stdlib.h` for the function `exit` and the symbolic constant `EXIT_FAILURE`.\n",
    "    - `CL/cl.h` for all the OpenCL-related functions, type, symbolic constants, etc. (most of them have a name that starts with) `cl`.\n",
    "      - We also preceed this `#include`  with a `#define` which chooses the OpenCL version we want to use\n",
    "\n",
    "      ```c\n",
    "      #include <stdio.h>\n",
    "      #include <stdlib.h>\n",
    "      #define CL_TARGET_OPENCL_VERSION 300 // the version of OpenCL we want to use (must be placed above the include below)\n",
    "      #include <CL/cl.h>                   // contains OpenCL function prototypes, types, symbolic constants, etc.\n",
    "      ```\n",
    "  - Then we define an OpenCL kernel function:\n",
    "    - `mykernel()` is the name of the **kernel function** (it can be any name we like, but we will stick with `mykernel` in this notebook).\n",
    "    - A **kernel function** runs on the device (GPU) and is **launched** (called) from the host (CPU).\n",
    "    - `__kernel` is an OpenCL **qualifier** (qualifying a function with `__kernel` makes the function a **kernel function**.\n",
    "    - The kernel function takes no arguments (`void`), prints the text `Hello World!`, and does NOT return a value (`void`).\n",
    "    - All **kernel functions** MUST have a `void` return type.\n",
    "    - Notice this is defined as a C litteral string, where each \"line\" within the string needs a newline character `\\n`.\n",
    "\n",
    "      ```c\n",
    "      const char *source =\n",
    "      \"__kernel void mykernel()\\n\"\n",
    "      \"{\\n\"\n",
    "      \"    printf(\\\"Hello World!\\\\n\\\");\\n\"\n",
    "      \"}\\n\";\n",
    "      ```\n",
    "  - Following this, we define two helper functions `checkOpenCL` and `checkOpenCLBuildLog`:\n",
    "    - `checkOpenCL` takes an OpenCL integer `cl_int` (containing an OpenCL error code `err`) and a C string (a message `msg`) as arguments.\n",
    "      - It then checks if the error code represents an error (`CL_SUCCESS` means no error), and if so:\n",
    "        - prints out the error code together with the message\n",
    "        - exits the program\n",
    "      \n",
    "      ```c\n",
    "      void checkOpenCL(cl_int err, const char *msg)\n",
    "      {\n",
    "          if (err != CL_SUCCESS)\n",
    "          {\n",
    "              fprintf(stderr, \"%s failed: %d\\n\", msg, err);\n",
    "              exit(EXIT_FAILURE);\n",
    "          }\n",
    "      }\n",
    "      ```\n",
    "    - `checkOpenCLBuildLog` takes an OpenCL error code of type `cl_int`, OpenCL program of type `cl_program`, and an OpenCL device of type `cl_device`.\n",
    "      - It then checks if the error code represents an error (`CL_SUCCESS` means no error), and if so:\n",
    "        - declares a buffer `log` and calls the OpenCL function `clGetProgramBuildInfo` to extract build information from the program into the buffer.\n",
    "          - OpenCL kernels (programs) are compiled as part of the C program, so to find out if a compilation error occured, we use this construct.\n",
    "        - prints out the build log in the buffer (this is how we can view errors from compiling our OpenCL kernels).\n",
    "        - exits the program\n",
    "      \n",
    "      ```c\n",
    "      void checkOpenCLBuildLog(cl_int err, cl_program program, cl_device_id device)\n",
    "      {\n",
    "          if (err != CL_SUCCESS)\n",
    "          {\n",
    "              char log[2048];\n",
    "              clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "              printf(\"Build log:\\n%s\\n\", log);\n",
    "              exit(EXIT_FAILURE);\n",
    "          }\n",
    "      }\n",
    "      ```\n",
    "  - Lastly we define the `main()` function:\n",
    "    - First, we declare a variable `err` of type `cl_int`, in which we can store error codes returned by OpenCL functions.\n",
    "      - Almost all OpenCL functions return an error, or populate a `cl_int` parameter with an error code.\n",
    "      - This is the error code we pass as an argument to the help functions above.\n",
    "    \n",
    "      ```c\n",
    "      cl_int err;\n",
    "      ```\n",
    "    - Then we find out what OpenCL platforms are available on our system.\n",
    "      - The variable `platform` of type `cl_platform_id` will contain a platform ID (e.g. for an Nvidia, AMD, or Intel platform).\n",
    "      - The variable `num_platforms` of type `cl_uint` will contain the number of platforms on our system.\n",
    "      - The call to the OpenCL function `clGetPlatformIDs` populates the two variables above with values.\n",
    "        - The first argument `1` to the function, means we only want `1` platform ID stored in the `platform` variable (the default platform on our system).\n",
    "        - Notice the function `clGetPlatformIDs` returns an error code which we store in the variable `err` we declared above.\n",
    "      - Finally, we call our helper function `checkOpenCL` passing in the error code `err`, and a message `\"clGetPlatformIDs\"` (i.e. the name of the OpenCL function).\n",
    "        - Our helper function will check so see if the error code constitutes an error, and if so, print it out with the message, then exit the C program.\n",
    "      \n",
    "      ```c\n",
    "      // 1. Platform\n",
    "      cl_platform_id platform;\n",
    "      cl_uint num_platforms;\n",
    "      err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "      checkOpenCL(err, \"clGetPlatformIDs\");\n",
    "      ```\n",
    "    - The next step is to find out what OpenCL-enabled devices are available for the platform we chose above.\n",
    "      - The variable `device` of type `cl_device_id` will contain a device ID (e.g. for an Nvidia GPU, AMD GPU/CPU, or Intel CPU, etc.).\n",
    "      - The variable `num_devices` of type `cl_uint` will contain the number of devices for the chosen platform.\n",
    "      - The call to the OpenCL function `clGetDeviceIDs` populates the two variables above with values.\n",
    "        - The first argument `platform` to the function contains the platform ID we chose above.\n",
    "        - The second argument `CL_DEVICE_TYPE_DEFAULT` means we only want the default device for this platform (e.g. an AMD plarfotm can contain both a GPU and a CPU).\n",
    "        - The third argument `1` means we only want `1` device ID (since we are using `CL_DEVICE_TYPE_DEFAULT`).\n",
    "        - The final two arguments are the variables that will be populated with values.\n",
    "      - Finally, we call our helper function `checkOpenCL` passing in the returned error code `err`.\n",
    "      \n",
    "      ```c\n",
    "      // 2. Device\n",
    "      cl_device_id device;\n",
    "      cl_uint num_devices;\n",
    "      err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "      checkOpenCL(err, \"clGetDeviceIDs\");\n",
    "      ```\n",
    "    - Great, so now we have chosen a platform, and the device within the platform we want to use, so the next step is to create a `context`.\n",
    "      - An OpenCL context is the abstraction used to manage a device, containing objects such as command queues, memory buffers, etc. described below.\n",
    "      - The variable `context` of type `cl_context` will contain the context for our chosen device.\n",
    "      - The call to the OpenCL function `clCreateContext` returns the `context`.\n",
    "        - The function taks multiple arguments, but we are only interested in the second, third, and final argument in this notebook.\n",
    "        - The second argument `1` is the number contexts we want.\n",
    "        - The third argument `device` is our chosen device from above.\n",
    "        - The final argument `err` is our variable for storing the error code produced by this function.\n",
    "          - Here we see a second way an OpenCL function populates a variable with an error code (passed as an argument, since the function's return value is a context).\n",
    "      - Finally, we call our helper function `checkOpenCL` passing in the returned error code `err`.\n",
    "      \n",
    "      ```c\n",
    "      // 3. Context\n",
    "      cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "      checkOpenCL(err, \"clCreateContext\");\n",
    "      ```\n",
    "    - Now that we have a context, we need to create a command queue in it.\n",
    "      - An OpenCL command queue is used to queue commands to the chosen device within a context, such as allocating memory on the device, launching a kernel, etc.\n",
    "      - The variable `queue` of type `cl_command_queue` will contain the command queue for our chosen device within our context.\n",
    "      - The call to the OpenCL function `clCreateCommandQueueWithProperties` returns the command `queue`.\n",
    "        - The first argument `context` is our context from above.\n",
    "        - The second argument `device` is our device from above.\n",
    "        - The third argument `0` (or `NULL`) means we are using default command queue properties (we can set additional properties, as we will see later).\n",
    "        - The final argument `err` is our variable for storing the error code produced by this function.\n",
    "      - Finally, we call our helper function `checkOpenCL` passing in the returned error code `err`.\n",
    "      \n",
    "      ```c\n",
    "      // 4. Command queue\n",
    "      cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "      checkOpenCL(err, \"lCreateCommandQueueWithProperties\");\n",
    "      ```\n",
    "    - Once we have a means of queueing commands to a device, we can create and build a program for our OpenCL kernel (from the source code for the kernel above).\n",
    "      - The variable `program` of type `cl_program` will contain the program for our kernel once it has been compiled.\n",
    "      - The call to the OpenCL function `clCreateProgramWithSource` returns the command `program`, where we're not interested in the fourth argument at the moment.\n",
    "        - The first argument `context` is our context from above.\n",
    "        - The second argument `1` means we only want to create one program (kernel).\n",
    "        - The third argument `source` is the source code for our device kernel (from the source code for the kernel above).\n",
    "        - The final argument `err` is our variable for storing the error code produced by this function.\n",
    "      - We then call our helper function `checkOpenCL` passing in the error code `err`.\n",
    "      - Now we need to compile our kernel'äs source code, which is done with the OpenLC function `clBuildProgram`.\n",
    "        - We're only interested in the first three arguments in this notebook.\n",
    "        - The first argument `program` is our program from above.\n",
    "        - The second argument `1` means we only want compile the sourced code for one kernel.\n",
    "        - The third argument `device` is our device from above (i.e. this kernel is for this device).\n",
    "      - Finally, we call our helper function `checkOpenCL` passing in the returned error code `err`.\n",
    "      \n",
    "      ```c\n",
    "      // 5. Program\n",
    "      cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &err);\n",
    "      checkOpenCL(err, \"clCreateProgramWithSource\");\n",
    "      err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "      checkOpenCLBuildLog(err, program, device);\n",
    "      ```\n",
    "    - The next step is to create the device kernel we want to use.\n",
    "      - The variable `kernel` of type `cl_kernel` will contain the compiled kernel.\n",
    "      - The call to the OpenCL function `clCreateKernel` returns the command `kernel`.\n",
    "        - The first argument `program` is our program from above.\n",
    "        - The second argument `\"mykernel\"` is the name of the kernel function in the source code from above.\n",
    "          - The source code can contain multiple kernel functions, so we need to specify the name of the kernel function we wan to use.\n",
    "        - The final argument `err` is our variable for storing the error code produced by this function.\n",
    "      - Finally, we call our helper function `checkOpenCL` passing in the returned error code `err`.\n",
    "      \n",
    "      ```c\n",
    "      // 6. Kernel\n",
    "      cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "      checkOpenCL(err, \"clCreateKernel\");\n",
    "      ```\n",
    "    - To execute a kernel on the device, we use the OpenCL function `clEnqueueNDRangeKernel`.\n",
    "      - We don't communicate directly with an OpenCL device, but instead enqueue commands in our command queue from above.\n",
    "      - First we create two launch configuration parameters `global_size` and `local_size`, both of type `size_t`.\n",
    "        - `global_size` is the total number of work items (threads) we want to launch when calling the kernel.\n",
    "        - `local_size` is number of work items (threads) in a work group.\n",
    "        - Both can be one-dimensional (1D), two-dimensional (2D), or three-dimensional (3D), depending on the problem we want to solve.\n",
    "          - E.g. for working with 1D arrays, 2D arrays, or 3D arrays.\n",
    "          - Each work item (thread) usually only processes 1 element in an array, but they all do so in parallel (at the same time).\n",
    "          - Each work item (thread) executes a copy of the same kernel function (with the same argument values to the kernel function).\n",
    "      - The call to the OpenCL function `clEnqueueNDRangeKernel` takes multiple arguments, but we'll focus on the most important ones in this notebook.\n",
    "        - The first argument `queue` is our queue from above.\n",
    "        - The second argument `kernel` is our kernel from above.\n",
    "        - The third argument `1` is the number of dimensions in our launch configuration.\n",
    "        - The fifth argument `global_size` is our variable with the total number of work items (threads) from above.\n",
    "        - The sixth argument `local_size` is our variable with the number of work items (threads) in a work group from above.\n",
    "        - The final argument can contain an OpenCL event (we will see how to use an event later).\n",
    "      - Finally, we call our helper function `checkOpenCL` passing in the returned error code `err`.\n",
    "      \n",
    "      ```c\n",
    "      // 7. Launch\n",
    "      size_t global_size = 1;\n",
    "      size_t local_size = 1;\n",
    "      err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "      checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "      ```\n",
    "    - Since the call to the OpenCL function `clEnqueueNDRangeKernel` is an asynchronous call, we need to wait until all enqueued items have been processed.\n",
    "      - We do this with the OpenCL function `clFinish`, passing in the `queue` we want to wait for as the single argument.\n",
    "      \n",
    "      ```c\n",
    "      // 8. Wait for the command queue to finish\n",
    "      clFinish(queue);\n",
    "      ```\n",
    "    - Finally, we release (deallocate) all resources we have created with a handful of OpenCL functions:\n",
    "      - `clReleaseKernel` takes our `kernel` as an argument and releases it.\n",
    "      - `clReleaseProgram` takes our `program` as an argument and releases it.\n",
    "      - `clReleaseCommandQueue` takes our `queue` as an argument and releases it.\n",
    "      - `clReleaseContext` takes our `context` as an argument and releases it.\n",
    "      \n",
    "      ```c\n",
    "      // 9. Cleanup\n",
    "      clReleaseKernel(kernel);\n",
    "      clReleaseProgram(program);\n",
    "      clReleaseCommandQueue(queue);\n",
    "      clReleaseContext(context);\n",
    "      ```\n",
    "- Wow! That was a lot of boiler plate code just to launch a single kernel on a device.\n",
    "  - This is bacause OpenCL is more general then e.g. CUDA, and can work with multiple types of hardware, GPU, CPU, FPGA, DSP, etc.\n",
    "  - Later, we will create helper functions to setup and teardown OpenCL, and use them in this notebook to make the examples clearer.\n",
    "- Additional steps, not shown above, are the steps for allocating memory on the device, copying memory between the host (CPU) and the device (GPU), etc.\n",
    "  - This is done before (copying memory from the host to the device) and after (copying memory from the device to the host) a kernel launch.\n",
    "- Run the cells below to see the output (the first cell writes the code to file, the second cell compiles and executes the C program with the OpenCL code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "const char *source =\n",
    "\"__kernel void mykernel()\\n\"\n",
    "\"{\\n\"\n",
    "\"    printf(\\\"Hello World!\\\\n\\\");\\n\"\n",
    "\"}\\n\";\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        fprintf(stderr, \"%s failed: %d\\n\", msg, err);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "void checkOpenCLBuildLog(cl_int err, cl_program program, cl_device_id device)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs\");\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "    checkOpenCL(err, \"clGetDeviceIDs\");\n",
    "\n",
    "    // 3. Context\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateContext\");\n",
    "\n",
    "    // 4. Command queue\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "    checkOpenCL(err, \"lCreateCommandQueueWithProperties\");\n",
    "\n",
    "    // 5. Program\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateProgramWithSource\");\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "    checkOpenCLBuildLog(err, program, device);\n",
    "\n",
    "    // 6. Kernel\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "    checkOpenCL(err, \"clCreateKernel\");\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "\n",
    "    // 8. Wait for the command queue to finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e45c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722cac4",
   "metadata": {},
   "source": [
    "### Creating a Utility Module\n",
    "- Let's create a C module `utils` containing helper functions we can use in this notebook, so the examples don't get bogged down in boiler plate code.\n",
    "  - `utils.c` (placed in folder `src`) contains helper function implementations.\n",
    "  - `utils.h` (placed in folder `include`) contains function prototypes for the helper functions.\n",
    "- Let's breifly look at what the helper functions do:\n",
    "  - First we use a couple of `#define`s for the path to a file containing the source code for our OpenCL kernel, and the name of the kernel function.\n",
    "    - `#define KERNEL_FILE \"src/kernel.cl\"` means the path to the file containing our kernel's source code will always be \"src/kernel.cl\"` in this notebook.\n",
    "    - `#define KERNEL_NAME \"mykernel\"` means the name of the kernel function within that source code will always be \"mykernel\"` in this notebook.\n",
    "    - This will make the examples clearer, so we don't have to specify this in every example with boiler plate code.\n",
    "  - `read_kernel_source` takes a filename (or path) to a file containing the source code for our kernel, and loads it into a variable.\n",
    "    - Note that this function allocates space on the heap, so the calling function must free the memory when done with it.\n",
    "  - `openclGetErrorString` takes an OpenCL error code, and returns a string representation of it.\n",
    "    - This will make it easier to interpret any errors produced in our code.\n",
    "  - `checkOpenCL` is our error checking function from before, but now prints out a string represenetation of an error instead of the error code.\n",
    "    - It calls the function `openclGetErrorString` above.\n",
    "  - `checkOpenCLBuildLog` basically does the same as `checkOpenCL`, but also takes a `program` and `device` as arguments, and prints out compile errors.\n",
    "    - This will enable us to see messages produced by the OpenCL compiler when it compiles our OpenCL kernels.\n",
    "  - `setupOpenCL` takes pointers `cl_context *context`, `cl_command_queue *queue`, `cl_program *program,` `cl_kernel *kernel` as arguments and performs steps 1-6 we sw before.\n",
    "    - 1. Platform - queries the system for an OpenCL platform and selects the default platform (i.e. Nvidia, AMD, or Intel).\n",
    "    - 2. Device - queries the platform for OpenCL-enabled devices and selects the default device (usually a GPU if it exists, otherwise a CPU).\n",
    "    - 3. Context - creates a `context`object for the device.\n",
    "    - 4. Command Queue - creates a command `queue` object for the device within the `context`.\n",
    "    - 5. Program - creates a `program` object and builds our OpenCL source code for the kernel (which will always be `src/kernel.cl` in this notebook).\n",
    "    - 6. Kernel - create a `kernel` object from the `program` given a kernel function name (which will always be `\"mykernel\"` in this notebook).\n",
    "    - We just need to remember to call this function at the top of our `main()` function to set things up using this boiler plate code.\n",
    "      - We also need to declare the variables `cl_context context`, `cl_command_queue queue`, `cl_program program,` `cl_kernel kernel` and pass them in as arguments.\n",
    "  - `teardownOpenCL` takes pointers `cl_context *context`, `cl_command_queue *queue`, `cl_program *program,` `cl_kernel *kernel` as arguments and performs the cleanup step.\n",
    "    - It releases (frees) the objects `kernel`, `program`, `queue`, and `context` created by the function `setupOpenCL` above.\n",
    "- Run the two cells below to create this utility module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0ebd150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define KERNEL_FILE \"src/kernel.cl\"\n",
    "#define KERNEL_NAME \"mykernel\"\n",
    "\n",
    "char* read_kernel_source(const char* filename)\n",
    "{\n",
    "    FILE* fp = fopen(filename, \"r\");\n",
    "    if (!fp)\n",
    "    {\n",
    "        fprintf(stderr, \"Failed to open kernel file: %s\\n\", filename);\n",
    "        exit(1);\n",
    "    }\n",
    "    fseek(fp, 0, SEEK_END);\n",
    "    long size = ftell(fp);\n",
    "    rewind(fp);\n",
    "    char* src = (char*)malloc(size + 1);\n",
    "    fread(src, 1, size, fp);\n",
    "    src[size] = '\\0';\n",
    "    fclose(fp);\n",
    "    return src;\n",
    "}\n",
    "\n",
    "const char* openclGetErrorString(cl_int err)\n",
    "{\n",
    "    switch (err)\n",
    "    {\n",
    "        case CL_SUCCESS:                                  return \"CL_SUCCESS\";\n",
    "        case CL_DEVICE_NOT_FOUND:                         return \"CL_DEVICE_NOT_FOUND\";\n",
    "        case CL_DEVICE_NOT_AVAILABLE:                     return \"CL_DEVICE_NOT_AVAILABLE\";\n",
    "        case CL_COMPILER_NOT_AVAILABLE:                   return \"CL_COMPILER_NOT_AVAILABLE\";\n",
    "        case CL_MEM_OBJECT_ALLOCATION_FAILURE:            return \"CL_MEM_OBJECT_ALLOCATION_FAILURE\";\n",
    "        case CL_OUT_OF_RESOURCES:                         return \"CL_OUT_OF_RESOURCES\";\n",
    "        case CL_OUT_OF_HOST_MEMORY:                       return \"CL_OUT_OF_HOST_MEMORY\";\n",
    "        case CL_PROFILING_INFO_NOT_AVAILABLE:             return \"CL_PROFILING_INFO_NOT_AVAILABLE\";\n",
    "        case CL_MEM_COPY_OVERLAP:                         return \"CL_MEM_COPY_OVERLAP\";\n",
    "        case CL_IMAGE_FORMAT_MISMATCH:                    return \"CL_IMAGE_FORMAT_MISMATCH\";\n",
    "        case CL_IMAGE_FORMAT_NOT_SUPPORTED:               return \"CL_IMAGE_FORMAT_NOT_SUPPORTED\";\n",
    "        case CL_BUILD_PROGRAM_FAILURE:                    return \"CL_BUILD_PROGRAM_FAILURE\";\n",
    "        case CL_MAP_FAILURE:                              return \"CL_MAP_FAILURE\";\n",
    "        case CL_MISALIGNED_SUB_BUFFER_OFFSET:             return \"CL_MISALIGNED_SUB_BUFFER_OFFSET\";\n",
    "        case CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST:return \"CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST\";\n",
    "        case CL_COMPILE_PROGRAM_FAILURE:                  return \"CL_COMPILE_PROGRAM_FAILURE\";\n",
    "        case CL_LINKER_NOT_AVAILABLE:                     return \"CL_LINKER_NOT_AVAILABLE\";\n",
    "        case CL_LINK_PROGRAM_FAILURE:                     return \"CL_LINK_PROGRAM_FAILURE\";\n",
    "        case CL_DEVICE_PARTITION_FAILED:                  return \"CL_DEVICE_PARTITION_FAILED\";\n",
    "        case CL_KERNEL_ARG_INFO_NOT_AVAILABLE:            return \"CL_KERNEL_ARG_INFO_NOT_AVAILABLE\";\n",
    "\n",
    "        // clCreateProgramWithSource, clBuildProgram, etc.\n",
    "        case CL_INVALID_VALUE:                            return \"CL_INVALID_VALUE\";\n",
    "        case CL_INVALID_DEVICE_TYPE:                      return \"CL_INVALID_DEVICE_TYPE\";\n",
    "        case CL_INVALID_PLATFORM:                         return \"CL_INVALID_PLATFORM\";\n",
    "        case CL_INVALID_DEVICE:                           return \"CL_INVALID_DEVICE\";\n",
    "        case CL_INVALID_CONTEXT:                          return \"CL_INVALID_CONTEXT\";\n",
    "        case CL_INVALID_QUEUE_PROPERTIES:                 return \"CL_INVALID_QUEUE_PROPERTIES\";\n",
    "        case CL_INVALID_COMMAND_QUEUE:                    return \"CL_INVALID_COMMAND_QUEUE\";\n",
    "        case CL_INVALID_HOST_PTR:                         return \"CL_INVALID_HOST_PTR\";\n",
    "        case CL_INVALID_MEM_OBJECT:                       return \"CL_INVALID_MEM_OBJECT\";\n",
    "        case CL_INVALID_IMAGE_FORMAT_DESCRIPTOR:          return \"CL_INVALID_IMAGE_FORMAT_DESCRIPTOR\";\n",
    "        case CL_INVALID_IMAGE_SIZE:                       return \"CL_INVALID_IMAGE_SIZE\";\n",
    "        case CL_INVALID_SAMPLER:                          return \"CL_INVALID_SAMPLER\";\n",
    "        case CL_INVALID_BINARY:                           return \"CL_INVALID_BINARY\";\n",
    "        case CL_INVALID_BUILD_OPTIONS:                    return \"CL_INVALID_BUILD_OPTIONS\";\n",
    "        case CL_INVALID_PROGRAM:                          return \"CL_INVALID_PROGRAM\";\n",
    "        case CL_INVALID_PROGRAM_EXECUTABLE:               return \"CL_INVALID_PROGRAM_EXECUTABLE\";\n",
    "        case CL_INVALID_KERNEL_NAME:                      return \"CL_INVALID_KERNEL_NAME\";\n",
    "        case CL_INVALID_KERNEL_DEFINITION:                return \"CL_INVALID_KERNEL_DEFINITION\";\n",
    "        case CL_INVALID_KERNEL:                           return \"CL_INVALID_KERNEL\";\n",
    "        case CL_INVALID_ARG_INDEX:                        return \"CL_INVALID_ARG_INDEX\";\n",
    "        case CL_INVALID_ARG_VALUE:                        return \"CL_INVALID_ARG_VALUE\";\n",
    "        case CL_INVALID_ARG_SIZE:                         return \"CL_INVALID_ARG_SIZE\";\n",
    "        case CL_INVALID_WORK_DIMENSION:                   return \"CL_INVALID_WORK_DIMENSION\";\n",
    "        case CL_INVALID_WORK_GROUP_SIZE:                  return \"CL_INVALID_WORK_GROUP_SIZE\";\n",
    "        case CL_INVALID_WORK_ITEM_SIZE:                   return \"CL_INVALID_WORK_ITEM_SIZE\";\n",
    "        case CL_INVALID_GLOBAL_OFFSET:                    return \"CL_INVALID_GLOBAL_OFFSET\";\n",
    "        case CL_INVALID_EVENT_WAIT_LIST:                  return \"CL_INVALID_EVENT_WAIT_LIST\";\n",
    "        case CL_INVALID_EVENT:                            return \"CL_INVALID_EVENT\";\n",
    "        case CL_INVALID_OPERATION:                        return \"CL_INVALID_OPERATION\";\n",
    "        case CL_INVALID_GL_OBJECT:                        return \"CL_INVALID_GL_OBJECT\";\n",
    "        case CL_INVALID_BUFFER_SIZE:                      return \"CL_INVALID_BUFFER_SIZE\";\n",
    "        case CL_INVALID_MIP_LEVEL:                        return \"CL_INVALID_MIP_LEVEL\";\n",
    "        case CL_INVALID_GLOBAL_WORK_SIZE:                 return \"CL_INVALID_GLOBAL_WORK_SIZE\";        \n",
    "        default:\n",
    "        {\n",
    "            static char unknown[64];\n",
    "            snprintf(unknown, sizeof(unknown), \"Unknown OpenCL error code %d\", err);\n",
    "            return unknown;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        printf(\"Error: %s (%s)\\n\", msg, openclGetErrorString(err));\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "void checkOpenCLBuildLog(cl_int err, cl_program program, cl_device_id device)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "void setupOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel)\n",
    "{\n",
    "    cl_int err;\n",
    "    \n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs\");\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "    checkOpenCL(err, \"clGetDeviceIDs\");\n",
    "\n",
    "    // 3. Context\n",
    "    /*cl_context*/ *context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateContext\");\n",
    "\n",
    "    // 4. Command queue\n",
    "    const cl_queue_properties props[] = { CL_QUEUE_PROPERTIES, CL_QUEUE_PROFILING_ENABLE, 0 };\n",
    "    /*cl_command_queue*/ *queue = clCreateCommandQueueWithProperties(*context, device, props, &err);\n",
    "    checkOpenCL(err, \"lCreateCommandQueueWithProperties\");\n",
    "\n",
    "    // 5. Program\n",
    "    char* source = read_kernel_source(KERNEL_FILE); // Read kernel source\n",
    "    /*cl_program*/ *program = clCreateProgramWithSource(*context, 1, (const char**)&source, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateProgramWithSource\");\n",
    "    err = clBuildProgram(*program, 1, &device, NULL, NULL, NULL);\n",
    "    checkOpenCLBuildLog(err, *program, device);\n",
    "\n",
    "    // 6. Kernel\n",
    "    /*cl_kernel*/ *kernel = clCreateKernel(*program, KERNEL_NAME, &err);\n",
    "    checkOpenCL(err, \"clCreateKernel\");\n",
    "}\n",
    "\n",
    "void teardownOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel)\n",
    "{\n",
    "    clReleaseKernel(*kernel);\n",
    "    clReleaseProgram(*program);\n",
    "    clReleaseCommandQueue(*queue);\n",
    "    clReleaseContext(*context);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd690e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing include/utils.h\n"
     ]
    }
   ],
   "source": [
    "%%writefile include/utils.h\n",
    "#pragma once\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "char* read_kernel_source(const char* filename);\n",
    "const char* openclGetErrorString(cl_int err);\n",
    "void checkOpenCL(cl_int err, const char *msg);\n",
    "void checkOpenCLBuildLog(cl_int err, cl_program program, cl_device_id device);\n",
    "void setupOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel);\n",
    "void teardownOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7f344",
   "metadata": {},
   "source": [
    "### Using the Utility Module\n",
    "- Now we can create:\n",
    "  - The source code for our OpenCL kernel in a file `src/kernel.cl` instead of embedding it as a C string inside our code.\n",
    "  - Our `main()` function without all the boiler plate code, and place it in a file `src/main.c`.\n",
    "  - All examples in this notebook will use the following skeleton code.\n",
    "    - We include any header files we need, including the utility module we just created `#include \"utils.h\"`.\n",
    "    - Then we create a variable `err` of type `cl_int` to store any error codes.\n",
    "    - Next, we create variables `cl_context context`, `cl_command_queue queue`, `cl_program program`, and `cl_kernel kernel`.\n",
    "     - Then, we pass the variables' addresses to our helper function `setupOpenCL`, which performs steps 1-6 in the boiler plate code.\n",
    "    - Then we have the code to launch our kernel (it has already been loaded and compiled by the boiler plate code).\n",
    "      - It will always be the kernel function called `mykernel` in the file `src/kernel.cl`.\n",
    "      - In the notebook examples, before launching a kernel, we will:\n",
    "        - Allocate and initialize memory on the host (CPU).\n",
    "        - Allocated memory on the device (GPU) and copy memory between the host (CPU) and device (GPU).\n",
    "        - Setup launch configuration parameters and the arguments we want to pass to the kernel function.\n",
    "      - We then wait for the kernel function to compelte (we can do that with the function `clFinish` as below, or use other relevant functions).\n",
    "        - This will block the `main()` function's main thread since this function call is synchronous.\n",
    "      - In the notebook examples, after waiting for the kernel to finish, we will:\n",
    "        - Copy memory between the device (GPU) and the host (CPU).\n",
    "        - Inspect the results from our kernel computation.\n",
    "      - Finally, we call our helper function `teardownOpenCL`, which releases the `context`, `queue`, `program`, and `kernel`. in the boiler plate code.\n",
    "        - When we create other OpenCL objects in our example code, we have to remeber to release these before calling this helper function.\n",
    "\n",
    "    ```c\n",
    "    #include <stdio.h>\n",
    "    #include <stdlib.h>\n",
    "    #include \"utils.h\"\n",
    "\n",
    "    int main()\n",
    "    {\n",
    "        cl_int err;\n",
    "\n",
    "        // 1. Platform, 2. Device, 3. Context, 4. Command queue, 5. Program, 6. Kernel\n",
    "        cl_context context;\n",
    "        cl_command_queue queue;\n",
    "        cl_program program;\n",
    "        cl_kernel kernel;\n",
    "        setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "        // 7. Launch\n",
    "        size_t global_size = 1;\n",
    "        size_t local_size = 1;\n",
    "        err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "        checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "\n",
    "        // 8. Wait and finish\n",
    "        clFinish(queue);\n",
    "\n",
    "        // 9. Cleanup\n",
    "        teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "        return 0;\n",
    "    }\n",
    "    ```\n",
    "- Run the three cells below to create the kernel `src/kernel.cl`, create and build the main program `src/main.c`, and run the resulting executable `bin/main.exe`.\n",
    "  - The example code is a lot clearer now that it isn't cluttered with all the boiler plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74d308f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel()\n",
    "{\n",
    "    printf(\"Hello World!\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e93c19f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform, 2. Device, 3. Context, 4. Command queue, 5. Program, 6. Kernel\n",
    "    cl_context context;\n",
    "    cl_command_queue queue;\n",
    "    cl_program program;\n",
    "    cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "\n",
    "    // 8. Wait and finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78e4fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d198326",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.4 NDRange (Global Size), Work Groups, Work Items, Devices, CUs, and PEs\n",
    "\n",
    "- In the previous OpenCL code, we saw:\n",
    "  - An OpenCL **kernel**, where:\n",
    "    - `__kernel` is an OpenCL-specific qualifier that makes a function a **kernel function**.\n",
    "      - A **kernel function** is executed on the device (GPU), and called from the host (CPU).\n",
    "    - `mykernel` is the name of the **kernel function**.\n",
    "    - `parameterlist` is the kernel function's comma-separated list of parameters.\n",
    "\n",
    "      ```c\n",
    "      __kernel kernel(parameterlist)\n",
    "      {\n",
    "\n",
    "      }\n",
    "      ```\n",
    "  - An OpenCL **kernel launch** (or really an enqueuement on the command queue), where:\n",
    "    - `queue` is the command queue object.\n",
    "    - `kernel` is the kernel object.\n",
    "    - `1` is the number of dimensions in our launch configuration (`1` for a 1D problem, `2` for a 2D problem, `3` for a 3D problem).\n",
    "    - `global_size` is a **launch parameter** with the **total number of `work items`** (threads).\n",
    "      - E.g. for a 2D problem we would set the parameter above to `2`, and define our variable as `size_t global_size[] = {<dim0>, <dim1>};`.\n",
    "    - `local_size` is a **launch parameter** with the **number of `work items` in a `work group`**.\n",
    "      - E.g. for a 2D problem we would set the parameter above to `2`, and define our variable as `size_t local_size[] = {<dim0>, <dim1>};`.\n",
    "    - What's missing is the `argumentlist`, i.e. the arguments to pass to the kernel function.\n",
    "      - These are set separately, before the kernel launch, with a series of calls to the OpenCL function `clSetKernelArg(kernel, index, size, value)` where:\n",
    "        - `kernel` is the kernel object.\n",
    "        - `index` is the ordinal position of the parameter in the kernel function, e.g. `0` for the first parameter, `1` for the second parameter, etc.\n",
    "        - `size` is the size of the argument in bytes, e.g. `sizeof(int)` for a variable `n` containing the number of elements in an `int` array, \n",
    "        - `value` is the address to the variable, e.g. `&n` for a variable containing the number of elements in an array. \n",
    "    \n",
    "      ```bash\n",
    "      size_t global_size = 1;\n",
    "      size_t local_size = 1;\n",
    "      err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "      checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "      ```\n",
    "\n",
    "<img src=\"images/opencl.png\" width=\"500\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "- An OpenCL-enabled device (e.g. GPU):\n",
    "  - Is called a `Device`.\n",
    "  - Has a number of `Compute Units (CUs)`.\n",
    "  - Each `CU` has a number of `Processing Elements (PEs)`.\n",
    "- During a **kernel launch**:\n",
    "  - OpenCL lauches an `NDRange` (basically a grid). \n",
    "  - An `NDRange` contains a total number of `work items` (threads), i.e. the `Global Size`, specified as `global_size` in the call to `clEnqueueNDRangeKernel`.\n",
    "  - The total number of `work items` (threads) are grouped into a number of `work groups`.\n",
    "  - Each `work group` contains a number of `work items` (threads), specified as `local_size` in the call to `clEnqueueNDRangeKernel`.\n",
    "- OpenCL maps:\n",
    "  - A `work group` to run on a `CU` (multiple `work groups` can be assigned to the same `CU`).\n",
    "  - Each `work item` (thread) within a `work group` to a `PE`.\n",
    "- So, a `work group` runs on a `CU`, and each `work item` (thread) within that `work group` runs on a `PE` within that `CU`.\n",
    "\n",
    "\n",
    "int num_work_groups = get_num_groups(0); // equivalent to CUDA's gridDim.x\n",
    "int work_group_size = get_local_size(0); // equivalent to CUDA's blockDim.x\n",
    "int work_group_id = get_group_id(0);     // equivalent to CUDA's blockIdx.x\n",
    "int work_item_id = get_local_id(0);      // equivalent to CUDA's threadIdx.x\n",
    "int idx = get_global_id(0);              // equivalent to CUDA's threadIdx.x + blockIdx.x * blockDim.x\n",
    "\n",
    "- For each **kernel launch**:\n",
    "  - OpenCL creates an `NDRange` containing `work groups` containing `work items` (threads).\n",
    "  - Each `work item` (thread) runs a copy of the same `kernel` function with the same `argumentlist`.\n",
    "  - 4 OpenCL-specific functions are available within each copy of the `kernel` function:\n",
    "    - `get_num_groups(dim)` returns the number of `work groups` in the kernel launch for the given dimension `dim`,\n",
    "      - Where `dim` can be `0`, `1`, and `2`, depending on the kernel launch parameters' dimensions.\n",
    "    - `get_local_size(dim)` returns the number of `work items` (threads) in a `work group` for the given dimension `dim`,\n",
    "      - Where `dim` can be `0`, `1`, and `2`, depending on the kernel launch parameters' dimensions.\n",
    "    - `get_group_id(dim)` returns the ID of a `work group`, unique within a kernel launch per dimension, for the given dimension `dim`,\n",
    "      - Where `dim` can be `0`, `1`, and `2`, depending on the kernel launch parameters' dimensions.\n",
    "    - `get_local_id(dim)` returns the ID of a `work item` (thread), unique withn a `work group` per dimension, for the given dimension `dim`,\n",
    "      - Where `dim` can be `0`, `1`, and `2`, depending on the kernel launch parameters' dimensions.\n",
    "    - `get_gloabl_id(dim)` returns the ID of the `work item` (thread), unique wthin a kernel launch per dimension, for the given dimension `dim`,\n",
    "      - Where `dim` can be `0`, `1`, and `2`, depending on the kernel launch parameters' dimensions.\n",
    "    \n",
    "  - In the figure, we see that each `CU` contains:\n",
    "    - A `register file` (the blue rectangle).\n",
    "      - The `register file` is divided into `**chunks**, where each `work item` (thread), running in the `work group` assigned to the `CU`, is assigned one **chunk**.\n",
    "      - Each `work item`'s (thread's) chunk of the `register file` is referred to as its `private memory` (only that `work item` (thread) can access that `private memory`).\n",
    "      - In a `kernel` function, we can declare a local variable with the qualifier `__private` which stores that variable in a `work item`'s (thread's) `private memory`.\n",
    "        - If we don't specify a qualifier, the variable is by default stored in the `work items`'s (thread's) `private memory`.\n",
    "    - A `shared memory` buffer (the green rectangle).\n",
    "      - The `shared memory` is shared by all `work items` (threads) running in the `work group` assigned to that `CU`.\n",
    "      - To store a variable in `shared memory`, we declare the variable with the `__local` qualifier.\n",
    "  - There is also `global memory`, which is declared using the qualifier `__global`.\n",
    "  - Finally, there is `constant memory` stored outside a `CU` (just like `global memory`), and is `read-only` memory (it can we written to once before a kernel launch).\n",
    "    - If we declare a variable with the qualifier `__constant`, it is placed in `constant memory`.\n",
    "    - `constant memory` is small, but very effient (the OpenCL compiler can optimize access to it since it is `read-only`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452d38f",
   "metadata": {},
   "source": [
    "### Code Demonstrating NDRanges, Work Groups, and Work Items\n",
    "\n",
    "- Let's write an OpenCL program that demonstrates running an `NDRange` of `work groups`, each with a number of `work items` (threads), on the device (GPU).\n",
    "- The program copies the elements from one 1D `int` array `input` to another 1D `int` array `output`.\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`\n",
    "    - `stdlib.h` for `malloc` and `free`\n",
    "    - `time.h` for `srand` and `rand`\n",
    "    - `utils.h` for our helper functions, which also includes `Cl/cl.h` with function prototypes for all the OpenCL functions, types, etc.\n",
    "    \n",
    "    ```c\n",
    "    #include <stdio.h>\n",
    "    #include <stdlib.h>\n",
    "    #include <time.h>\n",
    "    #include \"utils.h\"\n",
    "    ```\n",
    "  \n",
    "  - We define symbolic constants:\n",
    "    - `N` with the value `5` for the number of elements in each array\n",
    "    - `WORKITEMS_PER_WORKGROUP_0` with the value `2` for the number of `work items` (threads) in each `work group`\n",
    "\n",
    "    ```c\n",
    "    #define N 5\n",
    "    #define WORKITEMS_PER_WORKGROUP_0 2\n",
    "    ```\n",
    "  \n",
    "  - We define a kernel function called `mykernel`.\n",
    "    - It has the `__kernel` qualifier, making it a kernel function, returns `void`, and has three parameters:\n",
    "      - A `const int *` pointer `input` which points to the `input` array in the device's `global memory`, since we use the qualifier `__gloabal`.\n",
    "      - An `int *` pointer `output` which points to the `output` array in the device's `global memory`, since we use the qualifier `__gloabal`.\n",
    "      - An `int` variable `n` with the number of elements in each array.\n",
    "    - In the function's body, we:\n",
    "      - Use the OpenCL-specific functions `get_num_groups`, `get_local_size`, `get_group_id`, `get_local_id` and `get_global_id`.\n",
    "        - They are available to each `work item` (thread) within each copy of the kernel function.\n",
    "        - `get_num_groups(0)` returns the number of `work groups` in the kernel launch for dimension `0`\n",
    "          - In our case we have `3` since we calculate it in the `main` function as:\n",
    "\n",
    "            ```c\n",
    "            size_t localSize = WORKITEMS_PER_WORKGROUP_0; // 2\n",
    "            size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0; // ((5 + 2 - 1) / 2) * 2 = 6\n",
    "            size_t number_of_work_groups = globalSize / localSize // 6 / 2 = 3\n",
    "            ```\n",
    "        - `get_local_size(0)` returns the number of `work items` (threads) in a `work group` for dimension `0`,\n",
    "          - In our case we have `2`, defined by `WORKITEMS_PER_WORKGROUP_0`.\n",
    "        - `get_group_id(0)` returns the ID of a `work group`, unique within a kernel launch per dimension, for the dimension `0`,\n",
    "          - Since the number of `work groups` is `globalSize / localSize = 3`, `get_group_id(0)` will range from `0` to `2` (zero-based, i.e. starting from `0`).\n",
    "        - `get_local_id(0)` returns the ID of a `work item` (thread), unique withn a `work group` per dimension, for dimension `0`,\n",
    "          - Since `localSize = WORKITEMS_PER_WORKGROUP_0 = 2`, `get_local_id(0)` ranges from `0` to `1` (zero-based, i.e. starting from `0`).\n",
    "        - `get_gloabl_id(0)` returns the ID of the `work item` (thread), unique wthin a kernel launch per dimension, for dimension `0`,\n",
    "          - Since `gloablSize = 6`, `get_global_id(0)` ranges from `0` to `5` (zero-based, i.e. starting from `0`).\n",
    "      - We print out the values returned from these functions.\n",
    "        - This shows us which `work item` (thread) is running this specific copy of the kernel function, which `work group` it is in, etc.\n",
    "      - We have a `boundary guard`, i.e. `if(idx >= n) return;`\n",
    "        - This ensures we don't index into an array with `idx` if `idx` is out of bounds.\n",
    "        - We have `6` threads in total, and `5` (defined by `N`) elements in each array, so `idx = 5` is out of bounds.\n",
    "      - Finally, we copy one element from the `input` array into the `output` using `idx` as the index.\n",
    "        - Remember, each thread runs its own `copy` of the kernel function (in parallel), each with the same set of kernel function `arguments`.\n",
    "\n",
    "    ```c\n",
    "    __kernel void mykernel(__global const int *input, __global int *output, int n)\n",
    "    {\n",
    "        int num_work_groups = get_num_groups(0); // equivalent to CUDA's gridDim.x\n",
    "        int work_group_size = get_local_size(0); // equivalent to CUDA's blockDim.x\n",
    "        int work_group_id = get_group_id(0);     // equivalent to CUDA's blockIdx.x\n",
    "        int work_item_id = get_local_id(0);      // equivalent to CUDA's threadIdx.x\n",
    "        int idx = get_global_id(0);              // equivalent to CUDA's threadIdx.x + blockIdx.x * blockDim.x\n",
    "\n",
    "        printf(\"num_work_groups = %d, work_group_size = %d, work_group_id = %d, work_item_id = %d, idx = %d\\n\", num_work_groups, work_group_size, work_group_id, work_item_id, idx);\n",
    "\n",
    "        if (idx >= n)\n",
    "        {\n",
    "            printf(\"Boundary checking avoided indexing outside of the arrays [idx = %d]\\n\", idx);\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        output[idx] = input[idx];\n",
    "    }\n",
    "    ```\n",
    "- In the `main()` function:\n",
    "  - We declare our OpenCL variables and call our help function `setupOpenCL` as previously described.\n",
    "\n",
    "    ```c\n",
    "    // Setup OpenCL\n",
    "        cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "        setupOpenCL(&context, &queue, &program, &kernel);\n",
    "    ```\n",
    "  - We seed the pseudorandom number generator with the value `0` so the random numers we create will be the same every time we run the program.\n",
    "\n",
    "    ```c\n",
    "    srand(0);\n",
    "    ```\n",
    "\n",
    "  - We declare:\n",
    "    - `int` pointer variables `h_input` and `h_output` for the two arrays, which will point to heap memory (RAM) on the host (CPU).\n",
    "    - `int` variable `data_size` and initialize it to `N * sizeof(int)`, i.e. the total size of each array in bytes (with `N` elements of type `int` in each).\n",
    "    \n",
    "    ```c\n",
    "    int *h_input, *h_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "    ```\n",
    "  - We allocate memory on the host (CPU) with `malloc`, storing the pointers to the memory in variables `h_input` and `h_output`.\n",
    "\n",
    "    ```c\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    ```\n",
    "  - We initialize the `h_input` array on the host (CPU) with random values using the `rand()` function.\n",
    "\n",
    "    ```c\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100; // random integers between 0 and 99\n",
    "    }\n",
    "    ```\n",
    "  - We allocate memory on the device (GPU) with `clCreateBuffer` storing the pointers to the memory in variables `d_input` and `d_output`, both of type `cl_mem`.\n",
    "    - `clCreateBuffer` also copies values from host (CPU) mmeory to device (GPU) memory if we specify the `CL_MEM_COPY_HOST_PTR` as a flag.\n",
    "      - This is done for the `input` vector.\n",
    "    - We can also specify is the memory is read-only with flag `CL_MEM_READ_ONLY`, write-only with flag `CL_MEM_WRITE_ONLY` (or both with `CL_MEM_READ_WRITE`).\n",
    "      - The `input` vector is read-only and the `output` vector is write-only on the device (GPU).\n",
    "    - The flags are bitwise or:ed together and passed as the second argument to the function.\n",
    "    - We also recognize the first argument `context` and the last argument `err`.\n",
    "    - The third argument is the size of the memory in bytes for the `input` vector.\n",
    "    - The fourth argument is the `input` vector on the host (CPU).\n",
    "    - The return is a pointer to the `input` vector on the device (GPU).\n",
    "    \n",
    "    ```c\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, N * sizeof(int), h_input, &err); // CL_MEM_COPY_HOST_PTR copies h_input values to device buffer\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, N * sizeof(int), NULL, &err);\n",
    "    ```\n",
    "  - Next, we set the arguments passed to the kernel function, which we do using the `clSetKernelArg` function.\n",
    "    - The signature of our kernel function is `__kernel void mykernel(__global const int *input, __global int *output, int n)`, so we need to set three arguments.\n",
    "      - The first parameter to `clSetKernelArg` is the `kernel` object.\n",
    "      - The second parameter to `clSetKernelArg` is the argument index, e.g. `0` for the first argument, `1` for the second argument, etc.\n",
    "      - The third parameter to `clSetKernelArg` is the size in bytes of what we are passing to the kernel's parameter.\n",
    "      - The fourth parameter to `clSetKernelArg` is the address to the variable containg the value we want to pass the the kernel function's parameter.\n",
    "    - Here we are passing the pointers to the `input` and `output` vectors on the device (GPU) to the kernel function, together with the number of elements.\n",
    "    \n",
    "    ```c\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "    ```\n",
    "  - Next, we create and initialize variables `localSize` and `globalSize` for the launch configuration, both of type `size_t`.\n",
    "    - Here, we are assigning integers to them since we are only using 1 dimension, but we can also declare them as arrays, e.g. `localSize[2]` or `localSize[3]`.\n",
    "    - The launch configuration supports launching `1D`, `2D`, and `3D` work groups and work items (threads), depending on the problem we want to solve, e.g:\n",
    "      - For a `1D` problem such as copying elements between two 1D arrays, we only need to use 1 dimension, which is dimension `0` in the `get_` prefixed functions above.\n",
    "      - For a `2D` problem such as filtering a 2D image, we might need to use 2 dimensions, which are dimensions `0` and `1` in the `get_` prefixed functions above.\n",
    "      - For a `3D` problem such as filtering a 3D MRI-scan volume, we might need to use 3 dimensions, which are dimensions `0`, `1` and `z` in the `get_` prefixed functions above.\n",
    "      - For our 1D problem, we are only using one dimension `0`, which means the other dimensions `1` and `2` aren't available in the `get_` prefixed functions above.\n",
    "    - We create a `dim3` variable `blockDim` and initialize its `x` member variable to `THREADS_PER_BLOCK_X` (member variables `y` and `z` will be set to `1`).\n",
    "      - This means we have `2` threads per block since `THREADS_PER_BLOCK_X` is defined with the value `2`.\n",
    "    - We create a `size_t` variable `localSize` and initialize it to `WORKITEMS_PER_WORKGROUP_0` which is `2` in our example.\n",
    "    - We create a `size_t` variable `globalSize` and initialize it to:\n",
    "      - `((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0` which is `((5 + 2 -1) / 2) * 2 = 6` in our example.\n",
    "      - So, the total number of `work items` (threads) is `globalSize = 6`, and the number of `work items` (threads) per `work group` is `localSize = 2`.\n",
    "      - This means the number of `work groups` is `globalSize / localSize = 6 / 2 = 3`.\n",
    "      - This (standard) construct is commonly used to ensure enough `work items` (threads) and `work groups` are launched to solve a problem.\n",
    "        - But it can launch more `work items` (threads) than there are data elements (`N = 5` in our example).\n",
    "        - So we need a boundary guard in the kernel to prevent out-of-bounds indexing outside of the arrays.\n",
    "\n",
    "    ```c\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "    ```\n",
    "  - Next, we launch the kernel (or really enqueue it on the command queue), be calling the `clEnqueueNDRangeKernel` function, passing in the necessary arguments:\n",
    "    - The first argument is our `queue` object.\n",
    "    - The second argument is our `kernel` object.\n",
    "    - The third argument is the number of dimensions we are using in our launuch configuration, which is `1` in our case.\n",
    "    - The fifth argument is our `globalSize` variable, i.e. the total number of `work items` (threads) which is `6` in our case.\n",
    "    - The sixth argument is our `localSize` variable, i.e. the number of `work items` (threads) in each `work group` which is `2` in our case.\n",
    "    \n",
    "    ```c\n",
    "    // Enqueue kernel\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "    ```\n",
    "  - We copy the elements in the `d_output` array on the device (GPU) back to the array `h_output` on the host (CPU) using `clEnqueueReadBuffer`.\n",
    "    - The first argument is our `queue` object.\n",
    "    - The second argument is our pointer `d_output` to memory on the device (GPU).\n",
    "    - The third argument is the flag `CL_TRUE` which makes this function call asynchronous (it blocks the `main` function's main thread until the memory has been copied).\n",
    "    - The fifth argument is the size in bytes that we want to copy, in our case `N * sizeof(int) = 5 * 32 = 160` (on most systems).\n",
    "    - The sixth argument is our pointer `h_output` to memory on the host (CPU).\n",
    "\n",
    "    ```c\n",
    "    // Read result back (CL_TRUE makes this a synchronous (blocking) call)\n",
    "    err = clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, N * sizeof(int), h_output, 0, NULL, NULL);\n",
    "    ```\n",
    "  - We also have wait for the command queue' enqueued items to finish, i.e. `clFinish` is a synchonous (blocking) function call.\n",
    "    - In this case, it isn't really necessary since `clEnqueueReadBuffer` is a synchronous (blocking) function call due to the `CL_TRUE` flag.\n",
    "\n",
    "    ```c\n",
    "    // Wait for all queued operations to finish (not really needed here because of CL_TRUE in clEnqueueReadBuffer above)\n",
    "    err = clFinish(queue);\n",
    "    ```\n",
    "  - Then we print out the elements in the two arrays `h_input` and `h_output` on the host (CPU).\n",
    "   \n",
    "    ```c\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    ```\n",
    "  - Now we are done, so we free the memory allocated for the arrays:\n",
    "    - We free the `int` pointers (`h_input` and `h_output`) pointing to memory on the host (CPU) with `free`.\n",
    "    - We realse (free) the `int` pointers (`d_input` and `d_output`) pointing to memory on the device (GPU) with `clReleaseMemObject`.\n",
    "    - Both functions take a pointer to the memory \n",
    "    - Notice the naming convention used in this program for pointers to memory on the host (`h_` prefix) and the device (`d_` prefix).\n",
    "\n",
    "    ```c\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    err = clReleaseMemObject(d_input);\n",
    "    err = clReleaseMemObject(d_output);\n",
    "    ```\n",
    "  - Finally, we call our helper function `teardownOpenCL`, passing in the address to our `context`, `queue`, `program`, and `kernel` objects.\n",
    "    - This frees (releases) these objects via our hidden boiler plate code.\n",
    "\n",
    "    ```c\n",
    "    // Cleanup\n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "    ```\n",
    "- Run the three cells below to see the output from the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8cf26c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(__global const int *input, __global int *output, int n)\n",
    "{\n",
    "    int num_work_groups = get_num_groups(0); // equivalent to CUDA's gridDim.x\n",
    "    int work_group_size = get_local_size(0); // equivalent to CUDA's blockDim.x\n",
    "    int work_group_id = get_group_id(0);     // equivalent to CUDA's blockIdx.x\n",
    "    int work_item_id = get_local_id(0);      // equivalent to CUDA's threadIdx.x\n",
    "    int idx = get_global_id(0);              // equivalent to CUDA's threadIdx.x + blockIdx.x * blockDim.x\n",
    "\n",
    "    printf(\"num_work_groups = %d, work_group_size = %d, work_group_id = %d, work_item_id = %d, idx = %d\\n\", num_work_groups, work_group_size, work_group_id, work_item_id, idx);\n",
    "\n",
    "    if (idx >= n)\n",
    "    {\n",
    "        printf(\"Boundary checking avoided indexing outside of the arrays [idx = %d]\\n\", idx);\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    output[idx] = input[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f579afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, N * sizeof(int), h_input, &err); // CL_MEM_COPY_HOST_PTR copies h_input values to device buffer\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, N * sizeof(int), NULL, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "\n",
    "    // Read result back (CL_TRUE makes this a synchronous (blocking) call)\n",
    "    err = clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, N * sizeof(int), h_output, 0, NULL, NULL);\n",
    "\n",
    "    // Wait for all queued operations to finish (not really needed here because of CL_TRUE in clEnqueueReadBuffer above)\n",
    "    err = clFinish(queue);\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    err = clReleaseMemObject(d_input);\n",
    "    err = clReleaseMemObject(d_output);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dc52b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_work_groups = 3, work_group_size = 2, work_group_id = 2, work_item_id = 0, idx = 4\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 2, work_item_id = 1, idx = 5\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 1, work_item_id = 0, idx = 2\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 1, work_item_id = 1, idx = 3\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 0, work_item_id = 0, idx = 0\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 0, work_item_id = 1, idx = 1\n",
      "Boundary checking avoided indexing outside of the arrays [idx = 5]\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32a341",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see:\n",
    "  - `get_num_groups(0)` is `3`, i.e. there are `3` `work groups` in the `NDRange` (grid).\n",
    "  - `get_local_size(0)` is `2`, i.e. there are `2` `work items` (threads) in each `work group`.\n",
    "  - `get_group_id(0)` varies from `0` to `2`, i.e. from `0` to `get_num_groups(0) - 1`, and is a `work group`'s unique ID (i.e. unique within a kernel launch).\n",
    "  - `get_local_id(0)` varies from `0` to `1`, i.e. from `0` to `get_local_size(0) - 1`, and is a `work item`'s (thread's) unique block ID (i.e. unique within a work group).\n",
    "  - `idx = get_global_id(0)` varies from `0` to `5`, i.e. from `0` to `get_num_groups(0) * get_local_size(0) - 1`, and is a `work item`'s (trheads) unique global ID (i.e. unique within a kernel launch).\n",
    "  - The boundary guard was triggered for one work item (thread), i.e. the work item (thread) with `idx = 5`, because we only have `N = 5` elements in each array.\n",
    "    - So, we can have more work items (threads) running than elements in our data/arrays, why **we should always make use of boundary guards in our OpenCL kernels**.\n",
    "  - The `input` and `output` arrays have the same element values, so our OpenCL kernel's logic is functionally correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea72c64",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.5 Error Checking\n",
    "\n",
    "- OpenCL supports checking for errors in device (GPU) code and from calling any OpenCL function.\n",
    "- We have already seen how this works in our helper functions and by using the error code of type `cl_int` returned by, or set within, an OpenCL function call.\n",
    "- This example uses our helper function `checkOpenCL(err, \"Message\")` after each OpenCL function call to check for any errors.\n",
    "  - It's exactly the same code before but with error-checking in place.\n",
    "  - We have also reduced the code in the kernel function to just copy the output array to the input arrary, since we now know how work groups and work items work.\n",
    "  - Also notice the code just before we call our helper function `teardownOpenCL`.\n",
    "    - Here we are intentionally creating an error to see the output from our helper function `checkOpenCL`.\n",
    "    - We try to set a kernel argument with index `99`, but our kernel function only has three parameters, which produces an error.\n",
    "\n",
    "    ```c\n",
    "    err = clSetKernelArg(kernel, 99, sizeof(cl_mem), &d_input);        // Intentionally set a kernel argument with invalid arg index\n",
    "    checkOpenCL(err, \"Intentional clSetKernelArg with invalid index\");\n",
    "    ```\n",
    "- Inspect the code in the cells below, then run them to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd6bc875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(__global const int *input, __global int *output, int n)\n",
    "{\n",
    "    int idx = get_global_id(0);\n",
    "    output[idx] = input[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ed14d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    \n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, N * sizeof(int), h_input, &err); // CL_MEM_COPY_HOST_PTR copies h_input values to device buffer\n",
    "    checkOpenCL(err, \"clCreateBuffer\");\n",
    "    \n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, N * sizeof(int), NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateBuffer\");\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    checkOpenCL(err, \"clSetKernelArg\");\n",
    "    \n",
    "    err = clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    checkOpenCL(err, \"clSetKernelArg\");\n",
    "    \n",
    "    err = clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "    checkOpenCL(err, \"clSetKernelArg\");\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "\n",
    "    // Read result back (CL_TRUE makes this a synchronous (blocking) call)\n",
    "    err = clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, N * sizeof(int), h_output, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueReadBuffer\");\n",
    "\n",
    "    // Wait for all queued operations to finish (not really needed here because of CL_TRUE in clEnqueueReadBuffer above)\n",
    "    err = clFinish(queue);\n",
    "    checkOpenCL(err, \"clFinish\");\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "\n",
    "    err = clReleaseMemObject(d_input);\n",
    "    checkOpenCL(err, \"clReleaseMemObject\");\n",
    "    \n",
    "    err = clReleaseMemObject(d_output);\n",
    "    checkOpenCL(err, \"clReleaseMemObject\");\n",
    "\n",
    "    err = clSetKernelArg(kernel, 99, sizeof(cl_mem), &d_input);        // Intentionally set a kernel argument with invalid arg index\n",
    "    checkOpenCL(err, \"Intentional clSetKernelArg with invalid index\");\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3b45741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "Error: Intentional clSetKernelArg with invalid index (CL_INVALID_ARG_INDEX)\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eba3cb",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see that the result is the same as before (the only difference is that we are NOT printing out `work groups`, `work items`, etc.).\n",
    "- We also see the error `CL_INVALID_ARG_INDEX` returned from `clSetKernelArg()` when we try to set an argument with an index that doesn't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276810cd",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.6 Measuring Execution Time on the Host (CPU) and on the Device (GPU)\n",
    "\n",
    "- A common workflow is to first implement an algorithm in a function on the host (CPU), and then in a kernel on the device (GPU).\n",
    "  - The CPU version can act as a baseline benchmark for GPU kernel performance.\n",
    "  - The CPU version can be used to verify the results of a GPU kernel.\n",
    "  - For inexperienced manycore programmers, it's often easier to start with a CPU version, and then convert it into a GPU version.\n",
    "- Let's use the same code as before, but instrument it with timing code, wrapped around the CPU function call and around the GPU kernel launch.\n",
    "- The imported header files are the same as before:\n",
    "  - `stdlib.h` also contains the function prototype for `clock`, the `clock_t` typedef, and the symbolic constant `CLOCKS_PER_SEC`.\n",
    "    - `clock()` is a parameterless function returning a value of type `clock_t`.\n",
    "    - `clock_t` contains the number of `ticks` elapsed since the program started.\n",
    "    - `CLOCKS_PER_SEC` is defined as the number of `ticks` in a second (`ticks / CLOCKS_PER_SEC * 1000.0` converts `ticks` to milliseconds).\n",
    "  - `Cl/cl.h` contains a typedef `cl_event` and prototypes `clWaitForEvents` and `clGetEventProfilingInfo`.\n",
    "    - `cl_event` represent an OpenCL event, e.g. `cl_event kernel_event` (we won't explore OpenCL events in detail in this notebook).\n",
    "    - `clWaitForEvents` is used wait for an event to finish (it blocks until it does), e.g. `clWaitForEvents(1, &kernel_event)`.\n",
    "    - `clGetEventProfilingInfo` is get profiling information, which we use in the example to record a start and stop time (when the kernel started and stopped).\n",
    "    \n",
    "- We define a host (CPU) function `copy()`, equivalent to the device (GPU) kernel function `mykernel()`\n",
    "  - The GPU kernel function is the same as before.\n",
    "    \n",
    "    ```c\n",
    "    void copy(int *input, int *output, int n)\n",
    "    {\n",
    "        for(int idx = 0; idx < n; idx++)\n",
    "        {\n",
    "            output[idx] = input[idx];\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "- In the `main()` function:\n",
    "  - We wrap the code below around the device (GPU) kernel launch `clEnqueueNDRangeKernel()`.\n",
    "    - First we declare an OpenCL event variable `kernel_event` of type `cl_event`.\n",
    "    - Then we launch (enqueue) the kernel with the kernel event as the final argument.\n",
    "    - Next, we wait for the event to finish (it finished when the kernel finishes), with `clWaitForEvents(1, &kernel_event)`.\n",
    "    - Then we declare two variables `time_start` and `time_end` of type `cl_ulong`.\n",
    "    - We call the function `clGetEventProfilingInfo` passing in the first variable `time_start`, together with the variable `kernel_event`.\n",
    "    - We call the function `clGetEventProfilingInfo` passing in the second variable `time_stop`, together with the variable `kernel_event`.\n",
    "    - The flags `CL_PROFILING_COMMAND_START` and `CL_PROFILING_COMMAND_END` record when the event started and stopped, respectively.\n",
    "    - Finally, we calculate the kernel's execution time, in milliseconds, using the variables time_start` and time_stop`.\n",
    "\n",
    "    ```c\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the device (GPU) kernel execution time\n",
    "    // --------------------------------------------------------------\n",
    "    // Enqueue kernel with event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, &kernel_event);\n",
    "\n",
    "    // Wait for the kernel to finish\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "\n",
    "    // Query profiling info\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double gpu_elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "    // --------------------------------------------------------------\n",
    "    ```\n",
    "  - We wrap the code below around the host (CPU) function call `copy()`.\n",
    "    - We call the function `clock()` to record the current number of `ticks` since the program started, and store the result in a variable `cpu_start` of type `clock_t`.\n",
    "    - Then we call the host (CPU) function `copy()`.\n",
    "    - Next, we call the function `clock()` again to record the current number of `ticks` again and store the result in a variable `cpu_stop` of type `clock_t`.\n",
    "    - Finally, we calculate the elapsed number of milliseconds as `float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0`.\n",
    "  \n",
    "    ```c\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the host (CPU) function execution time\n",
    "    // --------------------------------------------------------------\n",
    "    clock_t cpu_start = clock();\n",
    "\n",
    "    // Host function call\n",
    "    copy(h_input, h_output_cpu, N);\n",
    "    \n",
    "    clock_t cpu_stop = clock();\n",
    "    float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    // --------------------------------------------------------------\n",
    "    ```\n",
    "  - We print out the execution time for the device (GPU) kernel and host (GPU) function.\n",
    "\n",
    "    ```c\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "    ```\n",
    "  - We use a separate `int` pointer variable `h_output_cpu` for storing the output from the host (CPU) function call.\n",
    "    - We verify the output results from the device (GPU) kernel and host (CPU) function are the same.\n",
    "    - This is a common best practice when verifying the correct functionality of an algorithm implemented in a device (GPU) kernel.\n",
    "      - We use the `abs()` function to compute the absolute difference between each eleement pair in the two arrays.\n",
    "      - If we were using `float`s instead of `int`s, we can use the `fabs()` function and compare the difference to e.g. `1e-5`.\n",
    "\n",
    "    ```c\n",
    "    int errorsum = 0;\n",
    "    for (int i = 0; i < N; i++)\n",
    "    {\n",
    "        int error = abs(h_output[i] - h_output_cpu[i]);\n",
    "        if (error > 0)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "    // Print verification result\n",
    "    printf(\"\\nVerification : %s\\n\", (errorsum > 0) ? \"FAILED\" : \"PASSED\");\n",
    "    ```\n",
    "  - We also print out the two arrays as before (same code) after launching the device (GPU) kernel and after calling the host (CPU) function.\n",
    "  - Lastly, we also free all memory, including the OpenCL event.\n",
    "\n",
    "    ```c\n",
    "    clReleaseEvent(kernel_event)\n",
    "    ```\n",
    "- Run the cells below to see the output.\n",
    "  - We won't record time in this notebook going forward (where possible) to make the example code clearer, but now you know how to do it yourself-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9df73918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(__global const int *input, __global int *output, int n)\n",
    "{\n",
    "    int idx = get_global_id(0);\n",
    "    output[idx] = input[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "299d010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "// Host function\n",
    "void copy(int *input, int *output, int n)\n",
    "{\n",
    "    for(int idx = 0; idx < n; idx++)\n",
    "    {\n",
    "        output[idx] = input[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output, *h_output_cpu;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    h_output_cpu = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, N * sizeof(int), h_input, &err); // CL_MEM_COPY_HOST_PTR copies h_input values to device buffer\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, N * sizeof(int), NULL, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the device (GPU) kernel execution time\n",
    "    // --------------------------------------------------------------\n",
    "    // Enqueue kernel with event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, &kernel_event);\n",
    "\n",
    "    // Wait for the kernel to finish\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "\n",
    "    // Query profiling info\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double gpu_elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "    // --------------------------------------------------------------\n",
    "\n",
    "    // Read result back (CL_TRUE makes this a synchronous (blocking) call)\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, N * sizeof(int), h_output, 0, NULL, NULL);\n",
    "    \n",
    "    // Print measured device kernel execution time\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the host (CPU) function execution time\n",
    "    // --------------------------------------------------------------\n",
    "    clock_t cpu_start = clock();\n",
    "\n",
    "    // Host function call\n",
    "    copy(h_input, h_output_cpu, N);\n",
    "    \n",
    "    clock_t cpu_stop = clock();\n",
    "    float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    // --------------------------------------------------------------\n",
    "\n",
    "    // Print measured host function execution time\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output_cpu[i]);\n",
    "    }\n",
    "\n",
    "    // Verify the results in the GPU output with the CPU output\n",
    "    int errorsum = 0;\n",
    "    for (int i = 0; i < N; i++)\n",
    "    {\n",
    "        int error = abs(h_output[i] - h_output_cpu[i]);\n",
    "        if (error > 0)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Print verification result\n",
    "    printf(\"\\nVerification : %s\\n\", (errorsum > 0) ? \"FAILED\" : \"PASSED\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_output_cpu);\n",
    "\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseEvent(kernel_event); // Release the event\n",
    "\n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db25af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU execution time  : 0.010240 ms\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "CPU execution time  : 0.001000 ms\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "Verification : PASSED\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad70c7",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see that the execution time on the GPU is slower than on the CPU.\n",
    "- This is expected since copying 5 elements from one array to another is just a waste of time on a GPU.\n",
    "  - **Not all problems are suitable for a GPU, in which case we should use the CPU instead**.\n",
    "- We also see the results verification `PASSED` so we can rest assured that the kernel function is correct (if the CPU function is correct, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae5f59",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.7 Shared Memory and Thread Synchronization on the Device (GPU)\n",
    "\n",
    "- `Shared memory`, called `Local memory` in OpenCL, is a fast, low-latency memory located on-chip, accessible by all `work items` (threads) in a `work group`.\n",
    "  - **Location**: On-chip, accessible by all `work items` (threads) in a `work group`.\n",
    "  - **Access**: Readable and writeable by all `work items` (threads) in a `work group` (also `writable` from the `host` before kernel launch).\n",
    "  - **Size limit**: Typically `48 KB` per `CU` (Compute Unit).\n",
    "  - **Speed**: Very fast, much faster than `global memory`.\n",
    "  - **Scope**: Shared only among `work items` (threads) in the same `work group`.\n",
    "  - **Lifetime**: Exists for the duration of the `work group`.\n",
    "- Use `shared memory` (`local memory`) when:\n",
    "  - Work items (threads) need to cooperate, such as tiling, caching, or communication between `work items` (threads).\n",
    "    - `Shared memory` (`local memory`) is specified within a kernel function with the qualifier `__local`, or as a kernel function parameter.\n",
    "    - It can be initialized `statically` (inside the kernel function) or `dynamically` (in a kernel function parameter).\n",
    "- `Work item (thread) synchronization` is used to synchronize `work items` (threads), especially `workitems` (threads) in a `work group` when using `shared memory` (`local memory`):\n",
    "  - Purpose: Barrier synchronization — all `work items` (threads) in the `work group` must reach it before any can proceed.\n",
    "    - A barrier (work item/thread synchronization) is done with the command `barrier(CLK_LOCAL_MEM_FENCE)` in a kernel function.\n",
    "  - Ensures all `shared memory` (`local memory`) reads/writes are complete before continuing.\n",
    "  - Used to prevent `race conditions`.\n",
    "- Issues to be aware of when using `shared memory` (`local memory`):\n",
    "  - `Non-coalesced memory access` also applies when accessing `global memory` (actually more important in that case).\n",
    "  - `Wavefront (thread) divergence` applies to `work items` (threads) within the same `wavefront` (a `wavefront` is a group of `32` work items (threads) that are scheduled to run on `PEs` within a `CU`/`work group`).\n",
    "  - `Low occupancy` is related to `shared memory` (`local memory`) but applies more generally to a `kernel launch`.\n",
    "\n",
    "  | Issue                       | Consequence                        | Fix                                |\n",
    "  | --------------------------- | ---------------------------------- | ---------------------------------- |\n",
    "  | Race conditions             | Wrong results                      | Use `barrier(CLK_LOCAL_MEM_FENCE)` or atomics   |\n",
    "  | No synchronization          | Inconsistent reads/writes          | Use `barrier(CLK_LOCAL_MEM_FENCE)`              |\n",
    "  | [Bank conflicts](https://www.youtube.com/watch?v=CZgM3DEBplE)              | Performance slowdown               | Pad arrays, restructure access     |\n",
    "  | Exceeding memory limit      | Kernel launch fails or runs slower | Reduce usage, use fewer threads    |\n",
    "  | Wrong indexing              | Wrong data or crash                | Use `get_local_id()` properly           |\n",
    "  | Uninitialized/out-of-bounds | Undefined behavior                 | Always initialize and guard bounds |\n",
    "  | [Non-coalesced memory access](https://www.youtube.com/watch?v=mLxZyWOI340&list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2&index=97)| Slower execution speed | Coalesce memory access |\n",
    "  | [Wavefront (thread) divergence](https://www.youtube.com/watch?v=bHkFV-YMxxY&list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2&index=106) | Slower execution speed | Avoid branches and loops |\n",
    "  | [Low occupancy](https://www.youtube.com/watch?v=2NGQvnT_3gU) | Slower execution speed | Increase occupancy |  \n",
    "\n",
    "<br />  \n",
    "\n",
    "- Now, let's look at a simple example of using `shared memory` (called `local memory` in OpenCL).\n",
    "- The code is the same as before, but with the following modifications:\n",
    "  - In the `kernel` function, we declare a buffer (array) with the `__local` qualifier.\n",
    "    - The `__local` qualifier means the variable/parameter is stored in `shared memory` (`local memory`).\n",
    "  - The `shared memory` (`local memory`) can be declared with a `static` size or with a `dynamic size`.\n",
    "  - In the sample code, we are using a `dynamic size`, where\n",
    "    - the size of the buffer (shared/local memory) is set in the `main()` function.\n",
    "    - the buffer is passed in as an argument to the kernel function (final parameter in our code), i.e. `__local int *shared`.\n",
    "  - If we wanted a `static` size, we would:\n",
    "    - remove the final parameter in the kernel's function header.\n",
    "    - define the size e.g. using a symbolic constant, e.g. `#define WORKITEMS_PER_WORKGROUP_0 2`\n",
    "    - uncomment the first row in the kernel function's body in our code below.\n",
    "\n",
    "  ```c\n",
    "  //#define WORKITEMS_PER_WORKGROUP_0 2 // when using static local (shared) memory size\n",
    "  \n",
    "  //__kernel void mykernel(__global const int *input, __global int *output, const int n)                    // when using static local (shared) memory size\n",
    "  __kernel void mykernel(__global const int *input, __global int *output, const int n, __local int *shared) // dynamic local (shared) memory size\n",
    "  {\n",
    "      //__local int shared[WORKITEMS_PER_WORKGROUP_0]; // when using static local (shared) memory size\n",
    "  }\n",
    "  ```\n",
    "- Let's look at the complete kernel function:\n",
    "  - We decalare `shared memory` (`local memory`) with a dynamic size via the kernel function's final parameter, i.e. `__local int *shared`\n",
    "  - Then we get a `work item`'s (thread's) global index/ID (`g_idx`) and a `work item`'s (thread's) local/shared index/ID (`s_idx`).\n",
    "    - We have to be careful in how we use the work items (threads) for indexing (`g_idx` is unique within a kernel launch, `s_id` is unique within a `work group` on the same `CU`).\n",
    "    - Remember, if we have `get_local_size(0)` `work items` (threads) per `work group` then `s_id` ranges from `0` to `get_local_size(0) - 1`.\n",
    "  - Our usual `boundary guard` comes next `if(g_idx >= n)`.\n",
    "  - Then we copy elements from the `input` array into `shared` (`local`) memory.\n",
    "    - The index into the `input` array is `g_idx`.\n",
    "    - The index into the `shared` array is `s_idx`.\n",
    "    - Different indexing schemes might ne necessary depending on the problem/algorithm.\n",
    "  - Next, we have a work item (thread) barrier `barrier(CLK_LOCAL_MEM_FENCE)`.\n",
    "    - This ensures no `work item` (thread) within the `work group` can continue past this row until all `work items` (threads) in the `work group` have completed the code above this row.\n",
    "      - This is important, since some `work items` (threads) might not have copied their element from the `input` array into the `shared` array yet.\n",
    "      - In this example, it isn't an issue, because no other `work item` (thread) will read another `work item`'s (thread's) element in the `shared` array in the code below the barrier `barrier(CLK_LOCAL_MEM_FENCE)`.\n",
    "      - For other problems, this might not be the case, so if `work items` (threads) aren't synchronized, they might continue and read stale data from the `shared` array.\n",
    "  - Lastly, when all `work items` (threads) are synchronized, a `work item` (thread) copies an element from the `shared` array into the `output` array.\n",
    "    - The index into the `output` array is `g_idx`.\n",
    "    - The index into the `shared` array is `s_idx`.\n",
    "    - Different indexing schemes might ne necessary depending on the problem/algorithm.\n",
    "\n",
    "  ```C\n",
    "  //#define WORKITEMS_PER_WORKGROUP_0 2 // when using static local (shared) memory size\n",
    "\n",
    "  //__kernel void mykernel(__global const int *input, __global int *output, const int n)                    // when using static local (shared) memory size\n",
    "  __kernel void mykernel(__global const int *input, __global int *output, const int n, __local int *shared) // dynamic local (shared) memory size\n",
    "  {\n",
    "      //__local int shared[WORKITEMS_PER_WORKGROUP_0]; // when using static local (shared) memory size\n",
    "\n",
    "      int g_idx = get_global_id(0); // index in global memory (globally unique)\n",
    "      int s_idx = get_local_id(0);  // index in local (shared) memory (unique within a workgroup)\n",
    "\n",
    "      if (g_idx >= n) return; // boundary guard\n",
    "\n",
    "      // Copy elements in global memory (input) to local (shared) memory\n",
    "      shared[s_idx] = input[g_idx];\n",
    "\n",
    "      // Synchronize workitems (threads)\n",
    "      barrier(CLK_LOCAL_MEM_FENCE); // all workitems (threads) in the same workgroup must be done with the operations above before any workitem (thread) can continue\n",
    "\n",
    "      // Copy elements in local (shared) memory to global memory (output)\n",
    "      output[g_idx] = shared[s_idx];\n",
    "  }\n",
    "  ```\n",
    "- Now, let's look at modifications in the `main()` function (most of the code is the same as before, but with the timing removed for clarity).\n",
    "  - In fact, there is only one modification:\n",
    "    - Since we are using a dynamic size for our `shared memory` (`local memory`), we first define the size of the memory with `#define WORKITEMS_PER_WORKGROUP_0 2`.\n",
    "    - Then pass the size of the `shared memory` (`local memory`) (in bytes) as the last argument to the kernel function using the function `clSetKernelArg`.\n",
    "    - If we were using a static size, we would just remove the final parameter from the kernel function, and remove this `clSetKernelArg` row below.\n",
    "  - Best practice is to use a dynamic size, since we can determine a variable size in the code.\n",
    "\n",
    "    ```c\n",
    "    #define WORKITEMS_PER_WORKGROUP_0 2\n",
    "    clSetKernelArg(kernel, 3, WORKITEMS_PER_WORKGROUP_0 * sizeof(int), NULL); // dynamic local (shared) memory size (remove when using static size)\n",
    "    ```\n",
    "- Run the cells below to see the output (which is exactly the same as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa4f6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "//#define WORKITEMS_PER_WORKGROUP_0 2 // when using static local (shared) memory size\n",
    "\n",
    "//__kernel void mykernel(__global const int *input, __global int *output, const int n)                    // when using static local (shared) memory size\n",
    "__kernel void mykernel(__global const int *input, __global int *output, const int n, __local int *shared) // dynamic local (shared) memory size\n",
    "{\n",
    "    //__local int shared[WORKITEMS_PER_WORKGROUP_0]; // when using static local (shared) memory size\n",
    "\n",
    "    int g_idx = get_global_id(0); // index in global memory (globally unique)\n",
    "    int s_idx = get_local_id(0);  // index in local (shared) memory (unique within a workgroup)\n",
    "\n",
    "    if (g_idx >= n) return; // boundary guard\n",
    "\n",
    "    // Copy elements in global memory (input) to local (shared) memory\n",
    "    shared[s_idx] = input[g_idx];\n",
    "\n",
    "    // Synchronize workitems (threads)\n",
    "    barrier(CLK_LOCAL_MEM_FENCE); // all workitems (threads) in the same workgroup must be done with the operations above before any workitem (thread) can continue\n",
    "\n",
    "    // Copy elements in local (shared) memory to global memory (output)\n",
    "    output[g_idx] = shared[s_idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "322da13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "    clSetKernelArg(kernel, 3, WORKITEMS_PER_WORKGROUP_0 * sizeof(int), NULL); // dynamic local (shared) memory size (remove when using static)\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "\n",
    "    // Read result back\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e88d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b44be",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- The output is exactly the same as before (same algorithm, just using different type of memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855d224",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.8 Constant Memory on the Device (GPU)\n",
    "\n",
    "- `Constant memory` is a special type of GPU memory optimized for cases where many `work items` (threads) read the same values.\n",
    "  - **Location**: On-device, separate from `global memory`.\n",
    "  - **Access**: Readable by all `work items` (threads) and is `read-only` from the `device`, but `writable` from the `host`.\n",
    "  - **Size limit**: `64 KB` (per `device`).\n",
    "  - **Speed**: Very fast if all `work items` (threads) access the same address.\n",
    "  - **Scope**: Globally accessible (like global variables).\n",
    "  - **Lifetime**: Exists for the duration of the `kernel launch`.\n",
    "- Use `constant memory` when:\n",
    "  - All or most `work items` (threads) access the same data (e.g., coefficients, transformation matrices, filters).\n",
    "  - The data is known before kernel launch and doesn't change during execution.\n",
    "  - The data is small (<= `64 KB`).\n",
    "- Let's look at a simple example using `constant memory`.\n",
    "- It's the same code as before, but with the `shared memory` (`local memory`) removed, and with the following modifications:\n",
    "  - In the kernel function:\n",
    "    - We have replaced the final parameter from using `__local` (shared) memory to using `__constant` memory.\n",
    "      - It is declared as `__constant const int *coefficients`.\n",
    "      - The `__constant` qualifier makes this `constant memory`.\n",
    "    - Then we multiply an element in the `input` vector with an element in `constant` memory with the same index, and store the product in the `output` array.\n",
    "      - In the `main()` function, we initialize all the `constant` memory elements to a value of `2`.\n",
    "      - So, compared to the previous code, the `output` vector's values will be twice as large as the `input` vector's values.\n",
    "    - Note that we have declared the size of the `constant memory` to be the same as the number of elements `N`.\n",
    "      - This is perfectly fine for this example where `N = 5`, but `constant memory` is extremely limited (small).\n",
    "      - We wouldn't be able to use `N` as the `constant memory`'s size if, say, `N` was `1000000` (a million elements).\n",
    "\n",
    "      ```c\n",
    "      __kernel void mykernel(__global const int *input, __global int *output, const int n, __constant const int *coefficients) // constant memory\n",
    "      {\n",
    "          int idx = get_global_id(0); // index in global memory (globally unique)\n",
    "\n",
    "          if (idx >= n) return; // boundary guard\n",
    "\n",
    "          // Multiply input elements with coefficients in constant memory and store the product in output\n",
    "          output[idx] = input[idx] * coefficients[idx];\n",
    "      }\n",
    "      ```\n",
    "  - In the main() function, the code is the same as before, but with `shared memory` removed, and with the following modifications:\n",
    "    - We declare an `int` pointer variable on the host (CPU) to define the contents to be copied to the `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      int *h_coefficients;\n",
    "      ```\n",
    "    - We create a variable with the same size (but in bytes) as the statically defined `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      int constant_size = N * sizeof(int);\n",
    "      ```\n",
    "    - We allocate space in host (CPU) memory (RAM) for the data we will be copying to the `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      h_coefficients = (int *)malloc(constant_size);\n",
    "      ```\n",
    "    - We initialize the data we will be copying tp `constant memory`.\n",
    "      - Notice, all the elements in `h_coefficients` are `2` (so the elements in the `output` array from the kernel function will be twice as large as in the `input` array).\n",
    "\n",
    "      ```c\n",
    "      for(int i = 0; i<N; i++)\n",
    "      {\n",
    "         h_coefficients[i] = 2;\n",
    "      } \n",
    "      ```\n",
    "    - Then we copy the host (CPU) memory to the device (GPU) `constant memory` using the OpenCL function `clCreateBuffer` as before.\n",
    "      - We pass the `context` object as the first argument.\n",
    "      - We pass the flag `CL_MEM_READ_ONLY` bitwise or:ed with the flag `CL_MEM_COPY_HOST_PTR` as the second argument (allocate and initialize read-only memory).\n",
    "      - We pass the size (in bytes) of the `constant` memory as the third argument.\n",
    "      - We pass in a pointer to the host (CPU) memory `h_coefficients` as the fourth argument.\n",
    "      - We pass the `error` object as the last argument.\n",
    "    \n",
    "      ```c\n",
    "      cl_mem d_coefficients = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, constant_size, h_coefficients, &err);\n",
    "      ```\n",
    "    - At the very end of the `main()` function, we free the memory on the host (CPU) and device (GPU), allocated for the `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      free(h_coefficients);\n",
    "      clReleaseMemObject(d_coefficients);\n",
    "      ```\n",
    "- Run the cells below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53f7d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "\n",
    "__kernel void mykernel(__global const int *input, __global int *output, const int n, __constant const int *coefficients) // constant memory\n",
    "{\n",
    "    int idx = get_global_id(0); // index in global memory (globally unique)\n",
    "\n",
    "    if (idx >= n) return; // boundary guard\n",
    "\n",
    "    // Multiply input elements with coefficients in constant memory and store the product in output\n",
    "    output[idx] = input[idx] * coefficients[idx];\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4906f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output, *h_coefficients;\n",
    "    int data_size = N * sizeof(int);\n",
    "    int constant_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    h_coefficients = (int *)malloc(constant_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_coefficients[i] = 2;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "    cl_mem d_coefficients = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, constant_size, h_coefficients, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "    clSetKernelArg(kernel, 3, sizeof(cl_mem), &d_coefficients);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "\n",
    "    // Read result back\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_coefficients);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseMemObject(d_coefficients);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a63cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      166   \n",
      "86      172   \n",
      "77      154   \n",
      "15      30    \n",
      "93      186   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf09f0d",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- We see that the values in the `output` array are twice as large compared to the `input` array,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a505b9a",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Sample Problems\n",
    "---\n",
    "\n",
    "## 3.1 1D Vector Addition on the Host (CPU)\n",
    "\n",
    "<img src=\"images/vectoradd_cpu.png\" width=\"500\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Let's start with a simple problem.\n",
    "\n",
    "Problem\n",
    "  - We have three vectors (arrays) `A`, `B`, and `C`, all with `N` elements each.\n",
    "  - We want to compute the elementwise sum of `A` and `B`, and store the sum in `C`.\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576`\n",
    "2. Create a host function `void vectorAdd(float *A, float *B, float *C, int n)`\n",
    "    - Loop through vectors `A` and `B` with `idx=0..N-1`\n",
    "    - Compute `C[idx] = A[idx] + B[idx]`\n",
    "3. Create a host function `main(void)`\n",
    "    - Declare and allocate memory for vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Initialize vectors `h_A` and `h_B` with `N` random floats each.\n",
    "    - Call function `vectorAdd` with `h_A`, `h_B`, `h_C`, `N`, and measure the execution time for `vectorAdd`.\n",
    "    - Verify result is correct.\n",
    "    - Print execution time, verification result, and sample elements in vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Free memory allocated for vectors `h_A`, `h_B`, and `h_C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2abfae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Number of elements (1048576)\n",
    "#define N (1 << 20) \n",
    "\n",
    "// Host function (elementwise addition of vectors A and B, placing the sum in vector C)\n",
    "void vectorAdd(float *A, float *B, float *C, int n)\n",
    "{   \n",
    "    // Loop through vectors and compute sum C = A + B\n",
    "    for (int idx = 0; idx < n; idx++)\n",
    "    {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host main routine\n",
    "int main(void)\n",
    "{\n",
    "    // Seed pseudorandom number generator\n",
    "    srand(0);\n",
    "\n",
    "    // Compute the size of the vectors (in bytes)\n",
    "    size_t size = N * sizeof(float);\n",
    "\n",
    "    // Declare and allocate host vectors A, B, and C   \n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    // Initialize host input vectors A and B with random values between 0 and 1.0\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        h_A[i] = rand() / (float)RAND_MAX;\n",
    "        h_B[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "\n",
    "    // Call function vectorAdd with timing\n",
    "    clock_t start = clock();\n",
    "\n",
    "    vectorAdd(h_A, h_B, h_C, N); // function call\n",
    "    \n",
    "    clock_t end = clock();\n",
    "    double elapsed_ms = (double)(end - start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    \n",
    "    // Verify results in ouput vector C is correct\n",
    "    float errorsum = 0.0f;\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        float error = fabs(h_A[i] + h_B[i] - h_C[i]);\n",
    "        if (error > 1e-5)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Print measured function execution time, verification result, and sample elements from each vector\n",
    "    printf(\"CPU execution time  : %f ms\\n\", elapsed_ms);\n",
    "    printf(\"Verification result : %s\\n\", (errorsum > 1e-5) ? \"FAILED\" : \"PASSED\");\n",
    "    printf(\"Vector samples      : A[0]=%f, B[0]=%f, C[0]=%f\\n\", h_A[0], h_B[0], h_C[0]);\n",
    "    \n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e9bda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU execution time  : 2.252000 ms\n",
      "Verification result : PASSED\n",
      "Vector samples      : A[0]=0.840188, B[0]=0.394383, C[0]=1.234571\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10200a36",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 1D Vector Addition on the Device (GPU)\n",
    "\n",
    "<img src=\"images/vectoradd_gpu1.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Problem\n",
    "  - We have three vectors (arrays) `A`, `B`, and `C`, all with `N` elements each.\n",
    "  - We want to compute the elementwise sum of `A` and `B`, and store the sum in `C`.\n",
    "\n",
    "We Know\n",
    "- In OpenCL, we have access to many `work items` (threads), where `work items` (threads) are organized into `work groups`, and `work groups` are organized into an NDRange`.\n",
    "  - `get_local_id(0)` represents a `work item’s index` along the `0` dimension within a `work group`.\n",
    "  - `get_group_id(0)` represents a `work group’s index` along the `0` dimension within the `NDRange`.\n",
    "  - `get_group_size(0)` represents the `number of work items` along the `0` dimension with a `work group`.\n",
    "  - `get_global_id(0)` represents a `work item`'s (thread's) global ID (`index`) on the device.\n",
    "- `Work groups` are assigned to a Compute Unit (CU) that has a number of Processing Elements (PEs).\n",
    "  - Each `work item` (thread) executes its own copy of the `kernel function`, in parallel, with the same parameter values.\n",
    "  - Each `work item` (thread) should process only one element in the arrays using the `index`.\n",
    "  - If there are more work items (threads) than elements (`index >= N`), those work items (threads) should `return` immediately from the `kernel function`\n",
    "- There can be a maximum of `1024` work items (threads) in a block.\n",
    "  - If we have `N = 1048576` elements, and `WORKITEMS_PER_WORKGROUP_0 = 1024`, we get:\n",
    "  - `local_size = WORKITEMS_PER_WORKGROUP_0 = 1024`.\n",
    "  - `global_size = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0 = ((1048576+1024-1) / 1024) * 1024 = 1024 * 1024 = 1048576`.\n",
    "  - `group_size = global_size / local_size = 1024`.\n",
    "  - And if we have `24` CUs, each will be assigned roughly `1024 / 24 = 42` work groups for maximum efficiency.\n",
    "\n",
    "<img src=\"images/vectoradd_gpu2.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576` and `WORKITEMS_PER_WORKGROUP_0=1024`\n",
    "2. Create a kernel `__kernel void mykernel(__global const float *A, __global const float *B, __global float *C, const int n)`\n",
    "    - Get global work item (thread) ID `idx = get_global_id(0)`\n",
    "    - Return if index is out of bounds (`idx >= n`) which means we have more work items (threads) than elements `n`.\n",
    "      - In this case we won't since `N` is evenly divisible by `WORKITEMS_PER_WORKGROUP_0`.\n",
    "    - Compute `C[idx] = A[idx] + B[idx]`.\n",
    "\n",
    "3. Create a host function `main(void)`\n",
    "    - Declare and allocate memory for host vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Initialize host vectors `h_A` and `h_B` with `N` random floats each.\n",
    "    - Declare and allocate memory for device vectors `d_A`, `d_B`, and `d_C`.\n",
    "      - Copy contents of host vectors `h_A` and `h_B` to device vectors `d_A` and `d_B`.\n",
    "    - Set kernel arguments for `d_A`, `d_B`, `d_C`, and `N`.\n",
    "    - Set launch parameters size_t `localSize` and `globalSize` as described above.\n",
    "    - Enqueue kernel with launch configuration in `localSize` and `globalSize`.\n",
    "    - Copy contents of device vector `d_C` to host vector `h_C`.\n",
    "    - Verify result is correct.\n",
    "    - Print execution time, verification result, and sample elements in host vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Free memory allocated for device vectors `d_A`, `d_B`, and `d_C`.\n",
    "    - Free memory allocated for host vectors `h_A`, `h_B`, and `h_C`.\n",
    "\n",
    "<img src=\"images/coalesced_memory_access.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "No need for shared (local) or constant memory, and the global memory access pattern is **coalesced** in the code, (a) in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b711eb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "// Device kernel (elementwise addition of vectors A and B, placing the sum in vector C)\n",
    "__kernel void mykernel(\n",
    "    __global const float *A,\n",
    "    __global const float *B,\n",
    "    __global float *C,\n",
    "    const int n)\n",
    "{\n",
    "    // Compute index (idx) from global workitem (thread) ID\n",
    "    int idx = get_global_id(0);\n",
    "\n",
    "    // Return if index is out of bounds (means we have more workitems (threads) than elements)\n",
    "    if (idx >= n)\n",
    "        return;\n",
    "\n",
    "    // Compute the sum C = A + B for the element with index idx\n",
    "    C[idx] = A[idx] + B[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb602b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "// Number of elements (1048576)\n",
    "#define N (1 << 20)\n",
    "\n",
    "// Number of threads\n",
    "#define WORKITEMS_PER_WORKGROUP_0 1024\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    // Seed pseudorandom number generator\n",
    "    srand(0);\n",
    "    \n",
    "    // Compute the size of the vectors (in bytes)\n",
    "    size_t size = N * sizeof(float);\n",
    "\n",
    "    // Declare and allocate host vectors A, B, and C   \n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    // Initialize host input vectors A and B with random values between 0 and 1.0\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        h_A[i] = rand() / (float)RAND_MAX;\n",
    "        h_B[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "\n",
    "    // Allocate the device input vectors A, B, and copy data from host vectors A, B\n",
    "    // Allocate the device output vector C\n",
    "    cl_mem d_A, d_B, d_C;\n",
    "    d_A = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, size, h_A, &err);\n",
    "    d_B = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, size, h_B, &err);\n",
    "    d_C = clCreateBuffer(context, CL_MEM_WRITE_ONLY, size, NULL, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_A);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_B);\n",
    "    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_C);\n",
    "    clSetKernelArg(kernel, 3, sizeof(int), &n);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel with timing event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, &kernel_event);\n",
    "    \n",
    "    // Wait for kernel to finish and compute execution time\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "\n",
    "    // Read result back from device output vector C to host output vector C\n",
    "    clEnqueueReadBuffer(queue, d_C, CL_TRUE, 0, size, h_C, 0, NULL, NULL);\n",
    "\n",
    "    // Verify the result vector is correct\n",
    "    float errorsum = 0.0f;\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        float error = fabs(h_A[i] + h_B[i] - h_C[i]);\n",
    "        if (error > 1e-5)\n",
    "        {\n",
    "            //fprintf(stderr, \"Result verification failed at element %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    // Print measured kernel execution time, verification result, and sample elements from each vector\n",
    "    printf(\"GPU execution time  : %f ms\\n\", elapsed_ms);\n",
    "    printf(\"Verification result : %s\\n\", (errorsum > 1e-5) ? \"FAILED\" : \"PASSED\");\n",
    "    printf(\"Vector samples      : A[0]=%f, B[0]=%f, C[0]=%f\\n\", h_A[0], h_B[0], h_C[0]);\n",
    "\n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    // Free device global memory and event\n",
    "    clReleaseMemObject(d_A);\n",
    "    clReleaseMemObject(d_B);\n",
    "    clReleaseMemObject(d_C);\n",
    "    clReleaseEvent(kernel_event);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7557fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU execution time  : 0.033792 ms\n",
      "Verification result : PASSED\n",
      "Vector samples      : A[0]=0.840188, B[0]=0.394383, C[0]=1.234571\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308f4c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.3 1D Convolution on the Host (CPU)\n",
    "\n",
    "<img src=\"images/1dconvolution.gif\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Next, let's tackle the problem of a 1-dimensional (1D) convolution.\n",
    "\n",
    "Problem\n",
    "- We have an `input` vector, a `kernel` (filter), and an `output` vector.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` vector.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` vector.\n",
    "- So the `kernel`'s (filter's) width has to be odd, e.g. `1x3`, `1x5`, `1x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` vector with `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` vector with the same `index` as the current `input` vector.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` vector, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576`\n",
    "2. Create a function:\n",
    "   - `void convolve1D(float *input, float *output, float *filter)`\n",
    "   - Loop through `input` vector.\n",
    "   - Compute `output[idx] = input[idx + offset] = filter[FILTER_WIDTH/2 + offset]`\n",
    "     - Only if `if(idx + offset >= 0 && idx + offset < DATA_WIDTH)`\n",
    "     - Where `offset` ranges from `-FILTER_WIDTH/2` to `+FILTER_WIDTH/2`.\n",
    "   - This computation is equivalent to\n",
    "     - Looping through the `input` vector, zero-padded with `FILTER_WIDTH/2` elements on both sides.\n",
    "     - Centering the `filter` over each original element in the zero-padded `input` vector.\n",
    "     - Computing the weighted sum and storing it in the `output` vector.\n",
    "4. Create a function `main(void)`\n",
    "   - Define a `DATA_WIDTH`, `FILTER_WIDTH` and `FILTER_WIDTH_OFFSET` (which is `FILTER_WIDTH/2`).\n",
    "   - Declare and allocate memory for vectors `input`, `ouput`, and `filter`.\n",
    "   - Initialize vector `input` with `DATA_WIDTH` random floats.\n",
    "   - Initialize vector `filter` with `weights` where each weight is `1.0 / FILTER_WIDTH` (averaging filter).\n",
    "   - Call function `convolve1D` with `input`, `ouput`, and `filter`.\n",
    "   - Measure the execution time for `convolve1D`.\n",
    "   - Print execution time and sample elements in vectors `input` and `output`.\n",
    "   - Free memory allocated for vectors `input`, `output`, and `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a78bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Number of data elements (1048576)\n",
    "#define DATA_WIDTH (1 << 20)\n",
    "\n",
    "// Number of filter elements \n",
    "#define FILTER_WIDTH 3\n",
    "\n",
    "// Number of elements on each side of a centered filter\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "\n",
    "void convolve1D(float *input, float *output, float *filter)\n",
    "{\n",
    "    // Loop through all elements\n",
    "    for(int d_col = 0; d_col < DATA_WIDTH; d_col++)\n",
    "    {\n",
    "        // Apply filter (slide filter over data and compute weighted sum)\n",
    "        float sum = 0.0f;\n",
    "        for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "        {\n",
    "            int f_col = FILTER_WIDTH_OFFSET + offset_col; // f_col: 0..FILTER_WIDTH-1\n",
    "            int i_col = d_col + offset_col;               // i_col: 0-FILTER_WIDTH_OFFSET..DATA_WIDTH-1+FILTER_WIDTH_OFFSET\n",
    "            \n",
    "            if(i_col >= 0 && i_col < DATA_WIDTH)\n",
    "            {\n",
    "                sum += input[i_col] * filter[f_col];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Store the weighted sum in the output array\n",
    "        output[d_col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Seed the random number generator\n",
    "    srand(0);            // use this for same set of random numbers each time the program is run\n",
    "    //srand(time(NULL)); // use this for different set of random numbers each time the program is run\n",
    "\n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * sizeof(float);     // size of data in bytes\n",
    "    int filter_size = FILTER_WIDTH * sizeof(float); // size of filter in bytes\n",
    "   \n",
    "    // Allocate space for host copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "      \n",
    "    // Setup input values\n",
    "    for (int col = 0; col < DATA_WIDTH; col++)\n",
    "    {\n",
    "        h_input[col] = (float)rand() / RAND_MAX; // Random floats between 0 and 1.0\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for (int col = 0; col < FILTER_WIDTH; col++)\n",
    "    {\n",
    "        h_filter[col] = 1.0f / FILTER_WIDTH; // averaging filter\n",
    "    }\n",
    "   \n",
    "    // Call convolve1D() with timing\n",
    "    clock_t start = clock();                                              // record the start time\n",
    "    convolve1D(h_input, h_output, h_filter);                              // call convolve1D()\n",
    "    clock_t stop = clock();                                               // record the stop time\n",
    "    double elapsed_ms = (double)(stop - start) / CLOCKS_PER_SEC * 1000.0; // calculate the elapsed time in millisecond\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, 1x%d filter) took %.2f ms\\n\", DATA_WIDTH, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"Vector samples:\\n\");\n",
    "    for(int i = 0; i < FILTER_WIDTH; i++)\n",
    "    {\n",
    "        printf(\"h_input[%d]=%.2f, h_output[%d]=%.2f\\n\", i, h_input[i], i, h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "    \n",
    "    return 0;\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ba9cf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1048576 elements, 1x3 filter) took 7.04 ms\n",
      "Vector samples:\n",
      "h_input[0]=0.84, h_output[0]=0.41\n",
      "h_input[1]=0.39, h_output[1]=0.67\n",
      "h_input[2]=0.78, h_output[2]=0.66\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5e7ab",
   "metadata": {},
   "source": [
    "- The output shows:\n",
    "  - Given an element with index `idx` in the `output` array.\n",
    "  - It's value is the average of the elements with indices `-FILTER_WIDTH_OFFSET..+FILTER_WIDTH_OFFSET+1` in the `input` array.\n",
    "    - Since an averaging filter was used.\n",
    "  - For example\n",
    "    - If the `FILTER_WIDTH` is `3`, we have `FILTER_WIDTH_OFFSET = FILTER_WIDTH / 2 = 1`.\n",
    "    - The value of an element with index `idx` in the `output` array is the average of the elements with indices `idx-1`, `idx`, and `idx+1` in the `input` array.\n",
    "      - `output[idx] = (input[idx-1] +  nput[idx] + nput[idx+1]) / 3`\n",
    "      - If it's a bounday element, the out-of-bounds indices have zero-padded elements with a value of `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0137e2d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.4 1D Convolution on the Device (GPU)\n",
    "\n",
    "<img src=\"images/tiled_convolution_1d.png\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Problem\n",
    "- We have an `input` vector, a `kernel` (filter), and an `output` vector.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` vector.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` vector.\n",
    "- So the `kernel`'s (filter's) width has to be odd, e.g. `1x3`, `1x5`, `1x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` vector with `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` vector with the same `index` as the current `input` vector.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` vector, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "- We have a 1D `input` vector with `N` elements (vector marked with `N` in the figure).\n",
    "- A `work group` of `wotk items` (threads) will process `get_local_size(0)` number of elements (top row in figure).\n",
    "- We don't want to load elements multiple times from `global memory` during the calulation.\n",
    "  - So each `work item` (thread) in a `work group` loads its `input` element into `shared memory` (called `Tile` in the figure).\n",
    "  - The `shared_memory` size needs to be `TILE_BASE_WITH + 2 * FILTER_WIDTH_OFFSET`, where\n",
    "    - `TILE_BASE_WITH` is the number of original `input` elements in a `work group` (highlighted elements in figure).\n",
    "    - `FILTER_WIDTH_OFFSET` is `FILTER_WIDTH / 2` (called `halo` elements in the figure).\n",
    "    - `FILTER_WIDTH` is `5` (in the figure).\n",
    "\n",
    "<img src=\"images/block_tile_loading_1d.png\" width=\"400\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "  - This ensures the `filter`, when centered on an element, covers all neighbouring elements, e.g.\n",
    "    - In `Work Group 0` the work items (threads) use `Tile 0`, where the original elements are `0`, `1`, `2`, `3` (see figure).\n",
    "    - The `filter` is centered on `0` covering `FILTER_WIDTH_OFFSET` neighbouring elements on each side.\n",
    "    - For border elements we use zero-padding (called `ghost` elements in the figure for the left-most elements).\n",
    "    - So the elements included in the first convolution are `ghost`, `ghost`, `0`, `1`, `2` (where `ghost = 0`).\n",
    "      - When processing element `3`, the `filter` covers elements `1`, `2`, `3`, `4`, `5`.\n",
    "\n",
    "- For these extra `2 * FILTER_WIDTH_OFFSET` elements to be available in a `work group`:\n",
    "    - The `shared memory`, called `Tile`, needs a size of `TILE_BASE_WITH + 2 * FILTER_WIDTH_OFFSET` (see above).\n",
    "      - This is the actual size need for a `work group` of `work items` (threads), i.e. `get_local_size(0)` which includes:\n",
    "      - Work items (threads) for loading the original elements that the `filter` will center on.\n",
    "      - Work items (threads) for the extra `2 * FILTER_WIDTH_OFFSET` border elements.\n",
    "      - This is illustrated in the bottom figure.\n",
    "- We also want to load the `filter` elements into `constant` memory to avoid hitting global memory when accessing them.\n",
    "\n",
    "- So this is what we'll do:\n",
    "1. Define:\n",
    "  - `DATA_WIDTH=1048576` (number of elements in data vectors `input` and `output`)\n",
    "  - `FILTER_WIDTH=3` (number of elements in the `filter` vector)\n",
    "  - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements on each size of a centered `filter`)\n",
    "  - `TILE_WIDTH_BASE=16` (number original elements in a `work group` of `work items` (threads))\n",
    "    - Where the final tile size is `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` to cover border elements.\n",
    "    - This is also the size we will use for the `shared memory` and `work group` size, i.e. `get_local_size(0)`.\n",
    "    - So we have these many `work items` (threads) in each `work group` and we know a `work group` is assigned to a `CU`.\n",
    "2. Define `constant` memory of size `FILTER_WIDTH` for the `filter`.\n",
    "3. Create a kernel function:\n",
    "   ```c\n",
    "   __kernel void mykernel(\n",
    "       __global const float *input,\n",
    "       __global float *output,\n",
    "       __constant const float *filter,\n",
    "       __local float *shared,\n",
    "       const int data_width,\n",
    "       const int filter_width_offset)\n",
    "    ```\n",
    "   - Define `shared` memory of size `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET`.\n",
    "   - Let the `work items` (threads) in a `work group` load their `input` elements into `shared` (`local`) memory.\n",
    "     - For border elements, we load the value `0` into `shared` (`local`) memory (zero-padding).\n",
    "   - Synchronize `work items` (threads) to ensure each `work item` (thread) in a `work group` has loaded its element into `shared` (`local`) memory.\n",
    "   - Compute the convolution as in the CPU solution, but now using `shared` (`local`) memory (input) and `constant` memory (filter).\n",
    "   - Store the result in the `ouput` vector.\n",
    "5. Create a function `main(void)`\n",
    "   - Declare and allocate memory for vectors `input`, `ouput`, and `filter` on the host (CPU).\n",
    "   - Initialize vector `input` with `DATA_WIDTH` random floats on the host (CPU).\n",
    "   - Initialize vector `filter` with `weights` on the host (CPU).\n",
    "     - Each weight is `1.0 / FILTER_WIDTH` (averaging filter).\n",
    "   - Declare and allocate memory for vectors `input` and `ouput`, and `consatnt` memory, on the device (GPU).\n",
    "     - Copy `input` vector in host (CPU) memory to device (GPU) global memory.\n",
    "     - Copy `filter` vector in host (CPU) memory to `constant` device (GPU) memory.\n",
    "   - Set kernel function arguments:\n",
    "\n",
    "      ```c\n",
    "      cl_int data_width = DATA_WIDTH;\n",
    "      cl_int filter_width_offset = FILTER_WIDTH_OFFSET;\n",
    "      int workgroup_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "      int shared_size = workgroup_width * sizeof(float);\n",
    "      clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "      clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "      clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_filter);\n",
    "      clSetKernelArg(kernel, 3, shared_size, NULL);         // dynamic local (shared) memory size\n",
    "      clSetKernelArg(kernel, 4, sizeof(int), &data_width);\n",
    "      clSetKernelArg(kernel, 5, sizeof(int), &filter_width_offset);  \n",
    "      ```\n",
    "   - Set kernel launch configuration:\n",
    "\n",
    "      ```c\n",
    "      // Kernel launch configuration\n",
    "      size_t localSize = workgroup_width;\n",
    "      size_t globalSize = ((DATA_WIDTH + workgroup_width - 1) / workgroup_width) * workgroup_width;\n",
    "      ```\n",
    "   - Launch kernel with launch configuration `globalSize` and `localSize`.\n",
    "   - Measure the execution time for the `kernel function`.\n",
    "   - Copy `output` vector in device (GPU) memory to host (CPU) memory.\n",
    "   - Print execution time and sample elements in vectors `input` and `output`.\n",
    "   - Free memory allocated for vectors `input`, `output`, and `filter` on the host (CPU).\n",
    "   - Free memory allocated for vectors `input`, `output` and `filter` on the device (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3c685f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(\n",
    "    __global const float *input,\n",
    "    __global float *output,\n",
    "    __constant const float *filter,\n",
    "    __local float *shared,\n",
    "    const int data_width,\n",
    "    const int filter_width_offset)\n",
    "{\n",
    "    int s_col = get_local_id(0);             // Workitem's (thread's) index in shared memory\n",
    "    int d_col = get_global_id(0);            // Workitem's (thread's) index in global memory\n",
    "    int i_col = d_col - filter_width_offset; // Workitem's (thread's) offset index in global memory\n",
    "\n",
    "    // Guard against workitems (threads) with IDs that would index outside the arrays\n",
    "    if (d_col >= data_width) return;\n",
    "\n",
    "    // Fill local (shared) memory with elements in global memory\n",
    "    if (i_col >= 0 && i_col < data_width)\n",
    "    {\n",
    "        shared[s_col] = input[i_col];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        shared[s_col] = 0.0f; // zero-padding\n",
    "    }\n",
    "\n",
    "    // Make sure each workitem (thread) in the workgroup has entered its element\n",
    "    // into local (shared) memory before any workitem (thread) continues\n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "    // Apply filter\n",
    "    float sum = 0.0f;\n",
    "    for (int offset_col = -filter_width_offset; offset_col <= filter_width_offset; offset_col++)\n",
    "    {\n",
    "        int f_col = filter_width_offset + offset_col;\n",
    "        int i_col = s_col + f_col;\n",
    "        \n",
    "        if(i_col >= 0 && i_col < get_local_size(0))\n",
    "        {\n",
    "            sum += shared[i_col] * filter[f_col]; // data elements in local (shared) memory + filter weights in constant memory = super fast computation\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Store the weighted sum in the output array\n",
    "    output[d_col] = sum;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "827bd7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "// Number of data elements (1048576)\n",
    "#define DATA_WIDTH (1 << 20)\n",
    "\n",
    "// Number of filter elements\n",
    "#define FILTER_WIDTH 3\n",
    "\n",
    "// Number of elements on each side of a centered filter\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "\n",
    "// Number of elements in local (shared) memory\n",
    "#define TILE_WIDTH_BASE 16\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    //srand(time(NULL));\n",
    "    \n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * sizeof(float);\n",
    "    int filter_size = FILTER_WIDTH * sizeof(float);\n",
    "\n",
    "    // Allocate space for host (CPU) copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "\n",
    "    // Setup input values\n",
    "    for (int col = 0; col < DATA_WIDTH; col++)\n",
    "    {\n",
    "        h_input[col] = (float)rand() / RAND_MAX; // Random floats between 0 and 1.0\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for (int col = 0; col < FILTER_WIDTH; col++)\n",
    "    {\n",
    "        h_filter[col] = 1.0f / FILTER_WIDTH; // averaging filter\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output, d_filter;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "    d_filter = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, filter_size, h_filter, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int data_width = DATA_WIDTH;\n",
    "    cl_int filter_width_offset = FILTER_WIDTH_OFFSET;\n",
    "    int workgroup_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "    int shared_size = workgroup_width * sizeof(float);\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_filter);\n",
    "    clSetKernelArg(kernel, 3, shared_size, NULL);         // dynamic local (shared) memory size\n",
    "    clSetKernelArg(kernel, 4, sizeof(int), &data_width);\n",
    "    clSetKernelArg(kernel, 5, sizeof(int), &filter_width_offset);  \n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = workgroup_width;\n",
    "    size_t globalSize = ((DATA_WIDTH + workgroup_width - 1) / workgroup_width) * workgroup_width;\n",
    "\n",
    "    // Enqueue kernel with timing event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, &kernel_event);\n",
    "\n",
    "    // Wait for kernel to finish and compute execution time\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "\n",
    "    // Copy result back to host\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, 1x%d filter) took %.2f ms\\n\", DATA_WIDTH, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"Vector samples:\\n\");\n",
    "    for(int i = 0; i < FILTER_WIDTH; i++)\n",
    "    {\n",
    "        printf(\"h_input[%d]=%.2f, h_output[%d]=%.2f\\n\", i, h_input[i], i, h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseMemObject(d_filter);\n",
    "    clReleaseEvent(kernel_event);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52c2eef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1048576 elements, 1x3 filter) took 0.10 ms\n",
      "Vector samples:\n",
      "h_input[0]=0.84, h_output[0]=0.41\n",
      "h_input[1]=0.39, h_output[1]=0.67\n",
      "h_input[2]=0.78, h_output[2]=0.66\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21905cb2",
   "metadata": {},
   "source": [
    "In the output we see:\n",
    "- The results are the same for the GPU solution as for the CPU solution.\n",
    "- The execution time for the GPU solution is significantly fast than the CPU solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88e6fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.5 2D Convolution on the Host (CPU)\n",
    "\n",
    "<img src=\"images/2dconvolution.gif\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "**Note**\n",
    "  - This is really just the same problem as a 1D convolution, but with an added second dimension.\n",
    "  - Therefore the problem and solution will be the same, but with the second dimension accounted for.\n",
    "\n",
    "Problem\n",
    "- We have an `input` matrix, a `kernel` (filter), and an `output` matrix.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` matrix.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` matrix.\n",
    "- So the `kernel`'s (filter's) width and height has to be odd, e.g. `3x3`, `5x5`, `7x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` matrix with the `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` matrix with the same `index` as the current `input` matrix.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` matrix, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define:\n",
    "   - `DATA_WIDTH=32` (number of elements in the `col` dimension for the `input` and `output`)\n",
    "   - `DATA_HEIGHT=32` (number of elements in the `row` dimension for the `input` and `output`)\n",
    "   - `FILTER_WIDTH=3` (number of elements int the `col` dimension for the `filter`)\n",
    "   - `FILTER_HEIGHT=3` (number of elements int the `row` dimension for the `filter`)\n",
    "   - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements to the left and right of the centered `filter`)\n",
    "   - `FILTER_HEIGHT_OFFSET=FILTER_HEIGHT/2` (number of elements above and below the centered `filter`)\n",
    "3. Create a function:\n",
    "   - `void convolve2D(float *input, float *output, float *filter)`\n",
    "   - Loop through `input` matrix.\n",
    "   - Compute convolution. Store result in `output` matrix.\n",
    "   - The only difference in the \"D convolution compared to the 1D convolution is the additional dimension.\n",
    "4. Create a function `main(void)`\n",
    "   - Declare and allocate memory for matrices `input`, `ouput`, and `filter`.\n",
    "   - Initialize matrix `input` with `DATA_HEIGHT * DATA_WIDTH` random floats.\n",
    "   - Initialize matrix `filter` with `weights` where each weight is `1.0 / FILTER_HEIGHT * FILTER_WIDTH` (averaging filter).\n",
    "   - Call function `convolve2D` with `input`, `ouput`, and `filter`.\n",
    "   - Measure the execution time for `convolve2D`.\n",
    "   - Print execution time and sample elements in matrices `input` and `output`.\n",
    "   - Free memory allocated for matrices `input`, `output`, and `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "77449720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define DATA_WIDTH 32\n",
    "#define DATA_HEIGHT 32\n",
    "#define FILTER_WIDTH 3\n",
    "#define FILTER_HEIGHT 3\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH/2)\n",
    "#define FILTER_HEIGHT_OFFSET (FILTER_HEIGHT/2)\n",
    "\n",
    "void convolve2D(float *input, float *output, float *filter)\n",
    "{\n",
    "    for(int d_row = 0; d_row < DATA_HEIGHT; d_row++)\n",
    "    {\n",
    "        for(int d_col = 0; d_col < DATA_WIDTH; d_col++)\n",
    "        {\n",
    "            float sum = 0.0f;\n",
    "            for (int offset_row = -FILTER_HEIGHT_OFFSET; offset_row <= FILTER_HEIGHT_OFFSET; offset_row++)\n",
    "            {\n",
    "                for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "                {\n",
    "                    int f_row = FILTER_HEIGHT_OFFSET + offset_row;\n",
    "                    int f_col = FILTER_WIDTH_OFFSET + offset_col;\n",
    "                    int i_row = d_row + offset_row;\n",
    "                    int i_col = d_col + offset_col;\n",
    "\n",
    "                    if(i_row >= 0 && i_row < DATA_HEIGHT && i_col >= 0 && i_col < DATA_WIDTH)\n",
    "                    {\n",
    "                        sum += input[i_row * DATA_WIDTH + i_col] * filter[f_row * FILTER_WIDTH + f_col];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            output[d_row * DATA_WIDTH + d_col] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "\n",
    "    float *h_input = (float *)malloc(DATA_WIDTH * DATA_HEIGHT * sizeof(float));\n",
    "    float *h_output = (float *)malloc(DATA_WIDTH * DATA_HEIGHT * sizeof(float));\n",
    "    float *h_filter = (float *)malloc(FILTER_WIDTH * FILTER_HEIGHT * sizeof(float));\n",
    "\n",
    "    for(int row = 0; row < DATA_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < DATA_WIDTH; col++)\n",
    "        {\n",
    "            h_input[row * DATA_WIDTH + col] = (float)rand() / RAND_MAX;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            h_filter[row * FILTER_WIDTH + col] = 1.0f / (FILTER_WIDTH * FILTER_HEIGHT);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Call convolve2D() with timing\n",
    "    clock_t start = clock();\n",
    "    convolve2D(h_input, h_output, h_filter);\n",
    "    clock_t stop = clock();\n",
    "    double elapsed_ms = (double)(stop - start) / CLOCKS_PER_SEC * 1000.0;\n",
    "\n",
    "    printf(\"Calculation (%d elements, %dx%d filter) took %.2f ms\\n\", DATA_HEIGHT * DATA_WIDTH, FILTER_HEIGHT, FILTER_WIDTH, elapsed_ms);\n",
    "    printf(\"\\nMatrix samples:\\n\");\n",
    "    printf(\"h_input %-12s h_output\\n\", \"\");\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_input[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"%-3s\",\"\");\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_output[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b069a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1024 elements, 3x3 filter) took 0.04 ms\n",
      "\n",
      "Matrix samples:\n",
      "h_input              h_output\n",
      "0.840 0.394 0.783    0.238 0.396 0.382 \n",
      "0.613 0.296 0.638    0.328 0.527 0.568 \n",
      "0.267 0.540 0.375    0.325 0.514 0.616 \n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc51aab",
   "metadata": {},
   "source": [
    "- The output shows:\n",
    "  - Given an element with index `[row, col]` in the `output` matrix.\n",
    "  - It's value is the average of the elements with indices:\n",
    "    - `-FILTER_HEIGHT_OFFSET..+FILTER_HEIGHT_OFFSET+1` in the `input` matrix's `row`.\n",
    "    - `-FILTER_WIDTH_OFFSET..+FILTER_WIDTH_OFFSET+1` in the `input` matrix's `col`.\n",
    "    - Since an averaging filter was used.\n",
    "  - For example\n",
    "    - If the `FILTER_HEIGHT` is `3`, we have `FILTER_HEIGHT_OFFSET = FILTER_HEIGHT / 2 = 1`.\n",
    "    - If the `FILTER_WIDTH` is `3`, we have `FILTER_WIDTH_OFFSET = FILTER_WIDTH / 2 = 1`.\n",
    "    - The value of an element with index `[row, col]` in the `output` matrix is the average of the elements in the `input` matrix with indices:\n",
    "\n",
    "      ```c\n",
    "      [row-1, col-1]  [row-1, col]  [row-1, col+1]\n",
    "      [row  , col-1]  [row  , col]  [row  , col+1]\n",
    "      [row+1, col-1]  [row+1, col]  [row+1, col+1]\n",
    "      ```\n",
    "    - If it's a bounday element, the out-of-bounds indices have zero-padded elements with a value of `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333dd74",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.6 2D Convolution on the Device (GPU)\n",
    "\n",
    "**Note**\n",
    "  - This is really just the same problem as a 1D convolution, but with an added second dimension.\n",
    "  - Therefore the problem and solution will be the same, but with the second dimension accounted for.\n",
    "\n",
    "Problem\n",
    "- We have an `input` matrix, a `kernel` (filter), and an `output` matrix.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` matrix.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` matrix.\n",
    "- So the `kernel`'s (filter's) height and width has to be odd, e.g. `3x3`, `5x5`, `7x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` matrix with the `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` matrix with the same `index` as the current `input` matrix.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` matrix, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define:\n",
    "  -  `DATA_WIDTH=32` (number of elements in the `col` dimension for the `input` and `output`)\n",
    "   - `DATA_HEIGHT=32` (number of elements in the `row` dimension for the `input` and `output`)\n",
    "   - `FILTER_WIDTH=3` (number of elements int the `col` dimension for the `filter`)\n",
    "   - `FILTER_HEIGHT=3` (number of elements int the `row` dimension for the `filter`)\n",
    "   - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements to the left and right of the centered `filter`)\n",
    "   - `FILTER_HEIGHT_OFFSET=FILTER_HEIGHT/2` (number of elements above and below the centered `filter`) \n",
    "   - `TILE_WIDTH_BASE=16` (number original elements in a `work group` of `work items` (threads) in the `col` dimension)\n",
    "   - `TILE_HEIGHT_BASE=16` (number original elements in a `work group` of `work items` (threads) in the `row` dimension)\n",
    "     - Where the final tile size in the `col` dimension is `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` to cover left and right border elements.\n",
    "       - This is also the size we will use for the `col` dimension in `shared memory` (`local memory`) and `work group` size, i.e. `get_local_size(0)`.\n",
    "     - Where the final tile size in the `row` dimension is `TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET` to cover top and bottom border elements.\n",
    "       - This is also the size we will use for the `row` dimension in `shared memory` (`local memory`) and `work group` size, i.e. `get_local_size(1)`.\n",
    "       - Notice we are now using the second dimension `1`.\n",
    "     - So we have these many 2D `work items` (threads) in each `work group` and we know a `work group` is assigned to a `CU`.\n",
    "3. Define a `constant` memory of size `FILTER_HEIGHT * FILTER_WIDTH` for the `filter`.\n",
    "4. Create a kernel function:\n",
    "\n",
    "    ```c\n",
    "    __kernel void mykernel(\n",
    "        __global const float *input,\n",
    "        __global float *output,\n",
    "        __constant const float *filter,\n",
    "        __local float *shared,\n",
    "        const int data_height,\n",
    "        const int data_width,\n",
    "        const int filter_height_offset,\n",
    "        const int filter_width_offset,\n",
    "        const int filter_width)\n",
    "    ```\n",
    "   - Define `shared` (`local`) memory of size `(TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET) * (TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET)`.\n",
    "   - Let the `work items` (threads) in a `work group` load their `input` elements into `shared` (`local`) memory.\n",
    "     - For border elements, we load the value `0` into `shared` (`local`) memory (zero-padding).\n",
    "   - Synchronize `work items` (threads) to ensure each `work item` (thread) in a `work group` has loaded its element into `shared` (`local`) memory.\n",
    "   - Compute the convolution as in the CPU solution, but now using `shared` (`local`) memory (input) and `constant` memory (filter).\n",
    "   - Store the result in the `ouput` matrix.\n",
    "5. Create a function `main(void)`\n",
    "   - Declare and allocate memory for matrices `input`, `ouput`, and `filter` on the host (CPU).\n",
    "   - Initialize matrix `input` with `DATA_HEIGHT * DATA_WIDTH` random floats on the host (CPU).\n",
    "   - Initialize matrix `filter` with `weights` on the host (CPU).\n",
    "     - Each weight is `1.0 / (FILTER_HEIGHT * FILTER_WIDTH)` (averaging filter).\n",
    "   - Declare and allocate memory for matrices `input`, `ouput` and `filter` on the device (GPU).\n",
    "     - Copy `input` matrix in host (CPU) memory to device (GPU) global memory.\n",
    "     - Copy `filter` matrix in host (CPU) memory to `constant` device (GPU) memory.\n",
    "   \n",
    "   - Set kernel function arguments:\n",
    "\n",
    "      ```c\n",
    "      // Set kernel arguments\n",
    "      cl_int data_height = DATA_HEIGHT;\n",
    "      cl_int data_width = DATA_WIDTH;\n",
    "      cl_int filter_height_offset = FILTER_HEIGHT_OFFSET;\n",
    "      cl_int filter_width_offset = FILTER_WIDTH_OFFSET;\n",
    "      cl_int filter_width = FILTER_WIDTH;\n",
    "      int workgroup_height = TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET;\n",
    "      int workgroup_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "      int shared_size = workgroup_height * workgroup_width * sizeof(float);\n",
    "      clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "      clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "      clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_filter);\n",
    "      clSetKernelArg(kernel, 3, shared_size, NULL);\n",
    "      clSetKernelArg(kernel, 4, sizeof(int), &data_height);\n",
    "      clSetKernelArg(kernel, 5, sizeof(int), &data_width);\n",
    "      clSetKernelArg(kernel, 6, sizeof(int), &filter_height_offset);\n",
    "      clSetKernelArg(kernel, 7, sizeof(int), &filter_width_offset);\n",
    "      clSetKernelArg(kernel, 8, sizeof(int), &filter_width);\n",
    "      ```\n",
    "   - Set kernel launch configuration:\n",
    "     - Notice the 2D configuration here by declaring `localSize[2]` and `gloablSize[2]` as 2D arrays.\n",
    "\n",
    "      ```c\n",
    "      // Kernel launch configuration\n",
    "      size_t localSize[2] = { workgroup_width, workgroup_height };\n",
    "      size_t globalSize[2] = {\n",
    "          ((data_width + workgroup_width - 1) / workgroup_width) * workgroup_width,\n",
    "          ((data_height + workgroup_height - 1) / workgroup_height) * workgroup_height\n",
    "      };\n",
    "      ```\n",
    "   - Launch kernel with launch configuration `globalSize` and `localSize`.\n",
    "   - Measure the execution time for the `kernel function`.\n",
    "   - Copy `output` matrix in device (GPU) memory to host (CPU) memory.\n",
    "   - Print execution time and sample elements in matrices `input` and `output`.\n",
    "   - Free memory allocated for matrices `input`, `output`, and `filter` on the host (CPU).\n",
    "   - Free memory allocated for matrices `input`, `output`, and `filter` on the device (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1efbead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(\n",
    "    __global const float *input,\n",
    "    __global float *output,\n",
    "    __constant const float *filter,\n",
    "    __local float *shared,\n",
    "    const int data_height,\n",
    "    const int data_width,\n",
    "    const int filter_height_offset,\n",
    "    const int filter_width_offset,\n",
    "    const int filter_width)\n",
    "{\n",
    "    int s_row = get_local_id(1);\n",
    "    int s_col = get_local_id(0);\n",
    "    \n",
    "    int d_row = get_global_id(1);\n",
    "    int d_col = get_global_id(0);\n",
    "    \n",
    "    int i_row = d_row - filter_height_offset;\n",
    "    int i_col = d_col - filter_width_offset;\n",
    "\n",
    "    if (d_col >= data_width || d_row >= data_height) return;\n",
    "\n",
    "    if (i_row >= 0 && i_row < data_height && i_col >= 0 && i_col < data_width)\n",
    "    {\n",
    "        shared[s_row * get_local_size(0) + s_col] = input[i_row * data_width + i_col];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        shared[s_row * get_local_size(0) + s_col] = 0.0f; // zero-padding\n",
    "    }\n",
    "\n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "    float sum = 0.0f;\n",
    "    for (int offset_row = -filter_height_offset; offset_row <= filter_height_offset; offset_row++)\n",
    "    {\n",
    "        for (int offset_col = -filter_width_offset; offset_col <= filter_width_offset; offset_col++)\n",
    "        {\n",
    "            int f_row = filter_height_offset + offset_row;\n",
    "            int f_col = filter_width_offset + offset_col;\n",
    "            int i_row = s_row + f_row;\n",
    "            int i_col = s_col + f_col;\n",
    "            \n",
    "            if(i_row >= 0 && i_row < get_local_size(1) && i_col >= 0 && i_col < get_local_size(0))\n",
    "            {\n",
    "                sum += shared[i_row * get_local_size(0) + i_col] * filter[f_row * filter_width + f_col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    output[d_row * data_width + d_col] = sum;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17d10183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "// Number of data elements in each dimension\n",
    "#define DATA_WIDTH 32\n",
    "#define DATA_HEIGHT 32\n",
    "\n",
    "// Number of filter elements in each dimension\n",
    "#define FILTER_WIDTH 3\n",
    "#define FILTER_HEIGHT 3\n",
    "\n",
    "// Number of elements on each side of a centered filter in each dimension\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "#define FILTER_HEIGHT_OFFSET (FILTER_HEIGHT / 2)\n",
    "\n",
    "// Number of elements in local (shared) memory in each dimension\n",
    "#define TILE_WIDTH_BASE 16\n",
    "#define TILE_HEIGHT_BASE 16\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    //srand(time(NULL));\n",
    "    \n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * DATA_HEIGHT * sizeof(float);\n",
    "    int filter_size = FILTER_WIDTH * FILTER_HEIGHT * sizeof(float);\n",
    "\n",
    "    // Allocate space for host (CPU) copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "\n",
    "    // Setup input values\n",
    "    for(int row = 0; row < DATA_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < DATA_WIDTH; col++)\n",
    "        {\n",
    "            h_input[row * DATA_WIDTH + col] = (float)rand() / RAND_MAX;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            h_filter[row * FILTER_WIDTH + col] = 1.0f / (FILTER_WIDTH * FILTER_HEIGHT);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output, d_filter;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "    d_filter = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, filter_size, h_filter, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int data_height = DATA_HEIGHT;\n",
    "    cl_int data_width = DATA_WIDTH;\n",
    "    cl_int filter_height_offset = FILTER_HEIGHT_OFFSET;\n",
    "    cl_int filter_width_offset = FILTER_WIDTH_OFFSET;\n",
    "    cl_int filter_width = FILTER_WIDTH;\n",
    "    int workgroup_height = TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET;\n",
    "    int workgroup_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "    int shared_size = workgroup_height * workgroup_width * sizeof(float);\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_filter);\n",
    "    clSetKernelArg(kernel, 3, shared_size, NULL);\n",
    "    clSetKernelArg(kernel, 4, sizeof(int), &data_height);\n",
    "    clSetKernelArg(kernel, 5, sizeof(int), &data_width);\n",
    "    clSetKernelArg(kernel, 6, sizeof(int), &filter_height_offset);\n",
    "    clSetKernelArg(kernel, 7, sizeof(int), &filter_width_offset);\n",
    "    clSetKernelArg(kernel, 8, sizeof(int), &filter_width);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize[2] = { workgroup_width, workgroup_height };\n",
    "    size_t globalSize[2] = {\n",
    "        ((data_width + workgroup_width - 1) / workgroup_width) * workgroup_width,\n",
    "        ((data_height + workgroup_height - 1) / workgroup_height) * workgroup_height\n",
    "    };\n",
    "\n",
    "    // Enqueue kernel with timing event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 2, NULL, globalSize, localSize, 0, NULL, &kernel_event);\n",
    "\n",
    "    // Wait for kernel to finish and compute execution time\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "\n",
    "    // Copy result back to host\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, %dx%d filter) took %.2f ms\\n\", DATA_HEIGHT * DATA_WIDTH, FILTER_HEIGHT, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"\\nMatrix samples:\\n\");\n",
    "    printf(\"h_input %-12s h_output\\n\", \"\");\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_input[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"%-3s\",\"\");\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_output[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseMemObject(d_filter);\n",
    "    clReleaseEvent(kernel_event);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ad36314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1024 elements, 3x3 filter) took 0.01 ms\n",
      "\n",
      "Matrix samples:\n",
      "h_input              h_output\n",
      "0.840 0.394 0.783    0.238 0.396 0.382 \n",
      "0.613 0.296 0.638    0.328 0.527 0.568 \n",
      "0.267 0.540 0.375    0.325 0.514 0.616 \n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fba808",
   "metadata": {},
   "source": [
    "In the output we see:\n",
    "- The results are the same for the GPU solution as for the CPU solution.\n",
    "- The execution time for the GPU solution is significantly fast than the CPU solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79f513",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Cleanup\n",
    "---\n",
    "\n",
    "- Let's remove all files that have been created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "826d62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "dirs = [\"src\", \"include\", \"bin\", \".vscode\"]\n",
    "files = [\"kernel.cl\", \"main.c\", \"main.exe\"]\n",
    "\n",
    "for d in dirs:\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "\n",
    "for f in files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "fsharp",
    "items": [
     {
      "aliases": [],
      "name": "fsharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
