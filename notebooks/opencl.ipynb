{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e27f3f",
   "metadata": {},
   "source": [
    "---\n",
    "# OpenCL Programming in C\n",
    "---\n",
    "\n",
    "This notebook contains an introduction to OpenCL programming in C. For detailed coverage, The Khronos Group's documentation is a good source:\n",
    "\n",
    "- [Khronos Group OpenCL](https://www.khronos.org/opencl)\n",
    "- [OpenCL Specification](https://registry.khronos.org/OpenCL/specs/3.0-unified/html/OpenCL_API.html)\n",
    "- [OpenCL SDK](https://github.com/KhronosGroup/OpenCL-SDK)\n",
    "- [OpenCL Guide](https://github.com/KhronosGroup/OpenCL-Guide?tab=readme-ov-file)\n",
    "\n",
    "**Note! If you don't have an OpenCL-enabled device on your system**\n",
    "\n",
    "- You can run this notebook in Google CoLab.\n",
    "  - Skip ahead to [1.1 Running the Notebook Locally or on Google CoLab](#11-running-the-notebook-locally-or-on-google-coLab)\n",
    "\n",
    "**Note! If you are on Windows**\n",
    "\n",
    "- Make sure you have installed Visual Studio or the Build Tools for Visual Studio.\n",
    "- Make sure you have started VSCode (`code .`) from within a `Visual Studio Developer Command Prompt` to set necessary environment variables.\n",
    "  - This is required when using the MicroSoft Visual C/C++ (MSVC) compiler `cl.exe` in VSCode on Windows.\n",
    "- If you are using PowerShell as your default shell in VSCode, your default PowerShell profile file `profile.ps1` might not be digitally signed.\n",
    "  - This will lead to errors when you compile C code in VSCode.\n",
    "  - If so, you can fix this error by executing either of the two PowerShell commands below:\n",
    "    - `Rename-Item \"$env:USERPROFILE\\Documents\\WindowsPowerShell\\profile.ps1\" -NewName \"profile.ps1.bak\"`\n",
    "    - `Set-ExecutionPolicy RemoteSigned -Scope CurrentUser`\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- [1. Prerequisites](#1-prerequisites) \n",
    "  - [1.1 Running the Notebook Locally or on Google CoLab](#11-running-the-notebook-locally-or-on-google-coLab)\n",
    "  - [1.2 Operating System and VSCode Shell](#12-operating-system-and-vscode-shell)\n",
    "  - [1.3 C Compiler (`gcc`, `clang`, `cl`)](#13-c-compiler-gcc-clang-cl)\n",
    "  - [1.4 OpenCL Library and Header Files](#14-opencl-library-and-header-files)\n",
    "  - [1.5 Configuring `tasks.json`, `launch.json` and `c_cpp_properties.json`](#15-configuring-tasksjson-launchjson-and-c_cpp_propertiesjson)\n",
    "  - [1.6 Create the File `tasks.json`](#16-create-the-file-tasksjson)\n",
    "  - [1.7 Create the File `launch.json`](#17-create-the-file-launchjson)\n",
    "  - [1.8 Create the File `c_cpp_properties.json`](#18-create-the-file-c_cpp_propertiesjson)\n",
    "  - [1.9 VSCode Extensions](#19-vscode-extensions)\n",
    "  - [1.10 Using Built-in Cell Magic `%%writefile`](#110-using-built-in-cell-magic-writefile)\n",
    "  - [1.11 Compiling and Executing an OpenCL Program from a Notebook Code Cell](#111-compiling-and-executing-an-opencl-program-from-a-notebook-code-cell)\n",
    "  - [1.12 Compiling and Debugging a Single-file OpenCL Program](#112-compiling-and-debugging-a-single-file-opencl-program)\n",
    "  - [1.13 Compiling and Debugging a Multi-file OpenCL Program](#113-compiling-and-debugging-a-multi-file-opencl-program)\n",
    "- [2. OpenCL Basics](#2-opencl-basics)\n",
    "  - [2.1 Listing OpenCL-enabled Devices and Properties](#21-listing-opencl-enabled-devices-and-properties)\n",
    "  - [2.2 Hello World in Host Code (CPU)](#22-hello-world-in-host-code-cpu)\n",
    "  - [2.3 Hello World in Device Code (GPU)](#23-hello-world-in-device-code-gpu)\n",
    "  - [2.4 NDRange (Global Size), Work Groups, Work Items, Devices, CUs, and PEs](#24-ndrange-global-size-work-groups-work-items-devices-cus-and-pes)\n",
    "  - [2.5 Error Checking](#25-error-checking)\n",
    "  - [2.6 Measuring Execution Time on the Host (CPU) and on the Device (GPU)](#26-measuring-execution-time-on-the-host-cpu-and-on-the-device-gpu)\n",
    "  - [2.7 Shared Memory and Thread Synchronization on the Device (GPU)](#27-shared-memory-and-thread-synchronization-on-the-device-gpu)\n",
    "  - [2.8 Constant Memory on the Device (GPU)](#28-constant-memory-on-the-device-gpu)\n",
    "- [3. Sample Problems](#3-sample-problems)\n",
    "  - [3.1 1D Vector Addition on the Host (CPU)](#31-1d-vector-addition-on-the-host-cpu)\n",
    "  - [3.2 1D Vector Addition on the Device (GPU)](#32-1d-vector-addition-on-the-device-gpu)\n",
    "  - [3.3 1D Convolution on the Host (CPU)](#33-1d-convolution-on-the-host-cpu)\n",
    "  - [3.4 1D Convolution on the Device (GPU)](#34-1d-convolution-on-the-device-gpu)\n",
    "  - [3.5 2D Convolution on the Host (CPU)](#35-2d-convolution-on-the-host-cpu)\n",
    "  - [3.6 2D Convolution on the Device (GPU)](#36-2d-convolution-on-the-device-gpu)\n",
    "- [4. Cleanup](#4-cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c99674",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Prerequisites\n",
    "---\n",
    "\n",
    "## 1.1 Running the Notebook Locally or on Google CoLab\n",
    "\n",
    "- Run the cell below to check if you have an OpenCL-enabled device and an OpenCL SDK on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "36736fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of platforms                               1\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "  Platform Vendor                                 NVIDIA Corporation\n",
      "  Platform Version                                OpenCL 3.0 CUDA 12.8.90\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Platform Extensions with Version                cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "  Platform Numeric Version                        0xc00000 (3.0.0)\n",
      "  Platform Extensions function suffix             NV\n",
      "  Platform Host timer resolution                  0ns\n",
      "  Platform External memory handle types           Opaque FD\n",
      "  Platform Semaphore types                        <gatherPlatformInfo:11: get CL_PLATFORM_SEMAPHORE_TYPES_KHR size : error -30>\n",
      "  Platform External semaphore import types        Opaque FD\n",
      "  Platform External semaphore export types        <gatherPlatformInfo:13: get CL_PLATFORM_SEMAPHORE_EXPORT_HANDLE_TYPES_KHR : error -30>\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "Number of devices                                 1\n",
      "  Device Name                                     NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "  Device Vendor                                   NVIDIA Corporation\n",
      "  Device Vendor ID                                0x10de\n",
      "  Device Version                                  OpenCL 3.0 CUDA\n",
      "  Device UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Driver UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Valid Device LUID                               No\n",
      "  Device LUID                                     6d69-637300000000\n",
      "  Device Node Mask                                0\n",
      "  Device Numeric Version                          0xc00000 (3.0.0)\n",
      "  Driver Version                                  570.124.06\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 \n",
      "  Device OpenCL C all versions                    OpenCL C                                                         0x400000 (1.0.0)\n",
      "                                                  OpenCL C                                                         0x401000 (1.1.0)\n",
      "                                                  OpenCL C                                                         0x402000 (1.2.0)\n",
      "                                                  OpenCL C                                                         0xc00000 (3.0.0)\n",
      "  Device OpenCL C features                        __opencl_c_fp64                                                  0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_images                                                0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_int64                                                 0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_3d_image_writes                                       0xc00000 (3.0.0)\n",
      "  Latest conformance test passed                  v2023-10-10-00\n",
      "  Device Type                                     GPU\n",
      "  Device Topology (NV)                            PCI-E, 0000:01:00.0\n",
      "  Device PCI bus info (KHR)                       PCI-E, 0000:01:00.0\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               24\n",
      "  Max clock frequency                             1455MHz\n",
      "  Compute Capability (NV)                         8.9\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     1\n",
      "    Supported partition types                     None\n",
      "    Supported affinity domains                    (n/a)\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             1024x1024x64\n",
      "  Max work group size                             1024\n",
      "  Preferred work group size multiple (device)     32\n",
      "  Preferred work group size multiple (kernel)     32\n",
      "  Warp size (NV)                                  32\n",
      "  Max sub-groups per work group                   0\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                 1 / 1       \n",
      "    short                                                1 / 1       \n",
      "    int                                                  1 / 1       \n",
      "    long                                                 1 / 1       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                1 / 1       \n",
      "    double                                               1 / 1        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  External memory handle types                    Opaque FD\n",
      "  Semaphore types                                 <printDeviceInfo:105: get number of CL_DEVICE_SEMAPHORE_TYPES_KHR : error -30>\n",
      "  External semaphore import types                 Opaque FD\n",
      "  External semaphore export types                 (n/a)\n",
      "  Global memory size                              8198619136 (7.636GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           2049654784 (1.909GiB)\n",
      "  Unified memory for Host and Device              No\n",
      "  Integrated memory (NV)                          No\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       4096 bits (512 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           0 bytes\n",
      "    Global                                        0 bytes\n",
      "    Local                                         0 bytes\n",
      "  Atomic memory capabilities                      relaxed, work-group scope\n",
      "  Atomic fence capabilities                       relaxed, acquire/release, work-group scope\n",
      "  Max size for global variable                    0\n",
      "  Preferred total size of global vars             0\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        688128 (672KiB)\n",
      "  Global Memory cache line size                   128 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             32\n",
      "    Max size for 1D images from buffer            268435456 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   0 bytes\n",
      "    Pitch alignment for 2D image buffers          0 pixels\n",
      "    Max 2D image size                             32768x32768 pixels\n",
      "    Max 3D image size                             16384x16384x16384 pixels\n",
      "    Max number of read image args                 256\n",
      "    Max number of write image args                32\n",
      "    Max number of read/write image args           0\n",
      "  Pipe support                                    No\n",
      "  Max number of pipe args                         0\n",
      "  Max active pipe reservations                    0\n",
      "  Max pipe packet size                            0\n",
      "  Local memory type                               Local\n",
      "  Local memory size                               49152 (48KiB)\n",
      "  Registers per block (NV)                        65536\n",
      "  Max number of constant args                     9\n",
      "  Max constant buffer size                        65536 (64KiB)\n",
      "  Generic address space support                   No\n",
      "  Max size of kernel argument                     32764 (32KiB)\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "  Device enqueue capabilities                     (n/a)\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        No\n",
      "    Profiling                                     No\n",
      "    Preferred size                                0\n",
      "    Max size                                      0\n",
      "  Max queues on device                            0\n",
      "  Max events on device                            0\n",
      "  Prefer user sync for interop                    No\n",
      "  Profiling timer resolution                      1000ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            No\n",
      "    Non-uniform work-groups                       No\n",
      "    Work-group collective functions               No\n",
      "    Sub-group independent forward progress        No\n",
      "    Kernel execution timeout (NV)                 No\n",
      "    Concurrent copy and kernel execution (NV)     Yes\n",
      "      Number of async copy engines                2\n",
      "    IL version                                    (n/a)\n",
      "    ILs with version                              (n/a)\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                (n/a)\n",
      "  Built-in kernels with version                   (n/a)\n",
      "  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Device Extensions with Version                  cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "\n",
      "NULL platform behavior\n",
      "  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform\n",
      "  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform\n",
      "  clCreateContext(NULL, ...) [default]            No platform\n",
      "  clCreateContext(NULL, ...) [other]              Success [NV]\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  Invalid device type for platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No platform\n"
     ]
    }
   ],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ba36b",
   "metadata": {},
   "source": [
    "### Inspect the output from the cell above:\n",
    "- If you see a `Number of platforms` listed above with a value of at least `1`.\n",
    "  - Skip to [1.2 Operating System and VSCode Shell](#12-operating-system-and-vscode-shell)\n",
    "- If you don't see a `Number of platforms` listed above with a value of at least `1`, follow the instructions below.\n",
    "\n",
    "  1. Click the icon below to open the notebook in Google CoLab.\n",
    "     \n",
    "     [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paga-hb/C1PD2C_2025/blob/main/notebooks/opencl.ipynb)\n",
    "\n",
    "  2. When the notebook opens in CoLab, choose `File -> Save a copy in Drive` from the main menu.\n",
    "  3. Choose `Runtime -> Change runtime type` from the main menu, select `TP4 GPU` as the hardware accelerator, and click the `Save` button.\n",
    "  4. In a notebook cell run the following code:\n",
    "\n",
    "      ```c\n",
    "      !sudo apt install -y gdb\n",
    "      ```\n",
    "\n",
    "  5. When the cell stops executing:\n",
    "     - Continue executing each cell below, without changing any values when prompted to, until you reach [1.4 OpenCL Library and Header Files](#14-opencl-library-and-header-files)\n",
    "     - In [1.4 OpenCL Library and Header Files](#14-opencl-library-and-header-files), when prompted to choose OpenCL paths, enter the following two values:\n",
    "       - `opencl_include_path = \"/usr/local/cuda-12.5/targets/x86_64-linux/include/CL/cl.h\"`\n",
    "       - `opencl_lib_path = \"/usr/local/cuda-12.5/targets/x86_64-linux/lib/libOpenCL.so\"`\n",
    "     - Then continue executing the cells from [1.4 OpenCL Library and Header Files](#14-opencl-library-and-header-files) and onwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f98c6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 Operating System and VSCode Shell\n",
    "\n",
    "We are going to compose JSON configuration files for VSCode, so let's collect some information about your environment.\n",
    "\n",
    "- Let's start by finding out what OS you are on and what default shell you are using in VSCode.\n",
    "\n",
    "**Linux/Mac**\n",
    "\n",
    "- Run the cell below.\n",
    "\n",
    "**Windows**\n",
    "\n",
    "- Find out (or change) which shell you are using in VSCode.\n",
    "  - Open the Command Palette: `Ctrl + Shift + P`\n",
    "  - Enter the text (and press `<Enter>`): `Preferences: Open Settings (UI)`\n",
    "  - In the search field, enter the text: `terminal.integrated.defaultProfile.windows`\n",
    "  - Choose the tab `User` or `Workspace` (`User` are global settings, `Workspace` only applies to the current workspace)\n",
    "  - Click the link: `Edit in settings.json`\n",
    "  - Set your desired shell:\n",
    "    - `\"terminal.integrated.defaultProfile.windows\": \"PowerShell\"`\n",
    "    - `\"terminal.integrated.defaultProfile.windows\": \"Command Prompt\"`\n",
    "- Choose your VSCode shell in the cell below.\n",
    "  - If you are using Powershell:\n",
    "    - Comment the row `windows_shell = \"cmd\"`\n",
    "    - Uncomment the row `windows_shell = \"powershell\"`\n",
    "- Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "de3f3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System (OS) : linux\n"
     ]
    }
   ],
   "source": [
    "windows_shell_name = \"cmd\"\n",
    "#windows_shell_name = \"powershell\"\n",
    "\n",
    "import platform, os\n",
    "os_name = platform.system()\n",
    "if os_name == \"Darwin\":\n",
    "    os_name = \"osx\"\n",
    "os_name = os_name.lower()\n",
    "\n",
    "print(f\"{'Operating System (OS)':<21} : {os_name}\")\n",
    "if os_name == 'windows':\n",
    "    windows_shell_path = !where {windows_shell_name}\n",
    "    windows_shell_path = windows_shell_path[0]\n",
    "    windows_shell_name = os.path.basename(windows_shell_path)\n",
    "    print(f\"{'Windows Shell Name':<21} : {windows_shell_name}\")\n",
    "    print(f\"{'Windows Shell Path':<21} : {windows_shell_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0549db",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3 C Compiler (`gcc`, `clang`, `cl`)\n",
    "\n",
    "To avoid full paths to the C compiler and debugger in the JSON configuration files, make sure the path to the C compiler is in your `PATH` environment variable.\n",
    "\n",
    "- In the cell below, choose the installed C compiler you want to use.\n",
    "  - If you are using `cl` (the C/C++ compiler, part of Microsoft Visual Studio build tools).\n",
    "    - Make sure you have launched VSCode from within a `Developer Command Prompt for VS`.\n",
    "      - Search in your Start Menu for `Developer Command Prompt for VS` (the version depends on your installed Visual Studio version).\n",
    "      - Open it => it launches a command prompt with all environment variables (paths, includes, libs) configured to run `cl.exe` and other build tools.\n",
    "      - Open VSCode from the command prompt: `code .`\n",
    "    - Comment the row `c_compiler = \"gcc\"`\n",
    "    - Uncomment the row `c_compiler = \"cl\"`\n",
    "  - If you are using `clang` (the C compiler, part of the LLVM project).\n",
    "    - Comment the row `c_compiler = \"gcc\"`\n",
    "    - Uncomment the row `c_compiler = \"clang\"`\n",
    "  - If you are using `gcc` (GNU Compiler Collection), you're all set.\n",
    "- Run the cell below to get the path to the C compiler.\n",
    "- If nothing shows up, you need to install a C compiler (and/or make sure the C compiler is in your `PATH` environment variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "7e700521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Compiler Name : gcc\n",
      "C Compiler Path : /usr/bin/gcc\n",
      "C Debugger Name : gdb\n",
      "C Debugger Path : /usr/bin/gdb\n"
     ]
    }
   ],
   "source": [
    "c_compiler = \"gcc\"\n",
    "#c_compiler = \"clang\"\n",
    "#c_compiler = \"cl\"\n",
    "\n",
    "import os\n",
    "if os_name == 'windows':\n",
    "    c_compiler_path = !where {c_compiler}\n",
    "else:\n",
    "    c_compiler_path = !which {c_compiler}\n",
    "c_compiler_path = c_compiler_path[0]\n",
    "c_compiler_name = os.path.basename(c_compiler_path)\n",
    "\n",
    "if c_compiler == 'cl':\n",
    "    c_debugger_name = \"cdb.exe\"\n",
    "    c_debugger_path = \"<integrated>\"\n",
    "if c_compiler == \"gcc\":\n",
    "    c_debugger_name = \"gdb\"\n",
    "if c_compiler == \"clang\":\n",
    "    c_debugger_name = \"lldb\"\n",
    "\n",
    "if os_name == 'windows':\n",
    "    if c_compiler != 'cl':\n",
    "        c_debugger_path = !where {c_debugger_name}\n",
    "else:\n",
    "    c_debugger_path = !which {c_debugger_name}\n",
    "\n",
    "if c_compiler != 'cl':\n",
    "    c_debugger_path = c_debugger_path[0]\n",
    "    c_debugger_name = os.path.basename(c_debugger_path)\n",
    "\n",
    "print(f\"{'C Compiler Name':<15} : {c_compiler_name}\")\n",
    "print(f\"{'C Compiler Path':<15} : {c_compiler_path}\")\n",
    "print(f\"{'C Debugger Name':<15} : {c_debugger_name}\")\n",
    "print(f\"{'C Debugger Path':<15} : {c_debugger_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c2c68",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.4 OpenCL Library and Header Files\n",
    "\n",
    "Let's find out where your OpenCL library and header files are located on your system.\n",
    "\n",
    "- Run the cell below to get the path to OpenCL's library and header files.\n",
    "- If nothing shows up, you need to install the OpenCL SDK for at least one device on your computer (and/or make sure environment variables are set up correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "2d0048ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/include/CL/cl.h\n",
      "/usr/local/cuda-12.8/targets/x86_64-linux/include/CL/cl.h\n",
      "/usr/local/cuda-12.8/targets/x86_64-linux/lib/libOpenCL.so\n",
      "/usr/lib/x86_64-linux-gnu/libOpenCL.so\n"
     ]
    }
   ],
   "source": [
    "if os_name == \"linux\":\n",
    "    !find /usr -name cl.h 2>/dev/null\n",
    "    !find /usr -name libOpenCL.so 2>/dev/null\n",
    "\n",
    "if os_name == \"osx\":\n",
    "    print(\"/System/Library/Frameworks/OpenCL.framework/Headers/\")\n",
    "    print(\"/System/Library/Frameworks/OpenCL.framework/OpenCL/\")\n",
    "\n",
    "if os_name == \"windows\":\n",
    "    pass\n",
    "    #!where cl.h\n",
    "    #!where OpenCL.lib\n",
    "    #!echo %INCLUDE%\n",
    "    #!echo %LIB%\n",
    "    #!echo %PATH%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caafe2",
   "metadata": {},
   "source": [
    "### From the paths listed above:\n",
    "- Choose one path to the header files (`.h`) listed above (enter the full path as listed above).\n",
    "- Choose one path to the library file (`.so` or `.lib`) listed above (enter the full path as listed above).\n",
    "- Enter the paths in the cell below.\n",
    "- Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "20e53001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCL Include Path : /usr/local/cuda-12.8/targets/x86_64-linux/include\n",
      "OpenCL Lib Path     : /usr/local/cuda-12.8/targets/x86_64-linux/lib\n"
     ]
    }
   ],
   "source": [
    "opencl_include_path = \"/usr/local/cuda-12.8/targets/x86_64-linux/include/CL/cl.h\"\n",
    "opencl_lib_path = \"/usr/local/cuda-12.8/targets/x86_64-linux/lib/libOpenCL.so\"\n",
    "\n",
    "import os\n",
    "opencl_include_dir = os.path.dirname(opencl_include_path)\n",
    "if os.path.basename(opencl_include_dir) == \"CL\":\n",
    "    opencl_include_dir = os.path.dirname(opencl_include_dir)\n",
    "\n",
    "opencl_lib_dir = os.path.dirname(opencl_lib_path)\n",
    "print(f\"{'OpenCL Include Path':<19} : {opencl_include_dir}\")\n",
    "print(f\"{'OpenCL Lib Path':<19} : {opencl_lib_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4407b8e",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.5 Configuring `tasks.json`, `launch.json`, and `c_cpp_properties.json`\n",
    "\n",
    "- To develop OpenCL programs in C with VSCode, we need to configure three VSCode workspace configuration files.\n",
    "  - In the file `tasks.json` we can configure various tasks, such as build tasks for compiling OpenCL programs in C with our chosen C compiler.\n",
    "  - In the file `launch.json` we can configure various debug options, such as debugging C programs with our chosen C debugger.\n",
    "  - In the file `c_cpp_properties.json` we can configure the compiler to use for linting purposes (intellisense).\n",
    "    - It isn't strictly necessary to create this configuration file to be able to run and debug C programs in VSCode.\n",
    "- VSCode workspace configuration files (`.json`) are stored in the subfolder `.vscode`.\n",
    "- Run the cell below to create the folder `.vscode`.\n",
    "\n",
    "**Note**\n",
    "\n",
    "- This notebook doesn't dscribe the contents of these three files in detail. To learn more, visit: \n",
    "  - [task.json](https://code.visualstudio.com/docs/debugtest/tasks)\n",
    "  - [launch.json](https://code.visualstudio.com/docs/debugtest/debugging)\n",
    "  - [c_cpp_properties.json](https://code.visualstudio.com/docs/cpp/configure-intellisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "034371f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\".vscode\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205a57b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.6 Create the File `tasks.json`\n",
    "\n",
    "- Run the cell below to create the file `tasks.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "12375bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "src_path = \"${workspaceFolder}/src/*.c\"\n",
    "include_path = \"${workspaceFolder}/include\"\n",
    "bin_path = \"${workspaceFolder}/bin/main.exe\"\n",
    "if os_name == \"windows\":\n",
    "    src_path = \"${workspaceFolder}\\\\src\\\\*.c\"\n",
    "    include_path = \"${workspaceFolder}\\\\include\"\n",
    "    bin_path = \"${workspaceFolder}\\\\bin\\\\main.exe\"\n",
    "\n",
    "makedir_command = \"mkdir\"\n",
    "makedir_args = [\"-p\", \"src\", \"include\", \"bin\"]\n",
    "if os_name == \"windows\":\n",
    "    makedir_command = windows_shell_path\n",
    "    if windows_shell_name == \"powershell.exe\":\n",
    "        makedir_args = [\"-NoProfile\", \"-ExecutionPolicy\", \"Bypass\", \"-Command\", \"New-Item -ItemType Directory -Path 'src','include','bin' -Force -ErrorAction SilentlyContinue\"]\n",
    "    else:\n",
    "        makedir_args = [\"/c\", \"if not exist src mkdir src & if not exist include mkdir include & if not exist bin mkdir bin\"]\n",
    "\n",
    "clean_command = \"find\"\n",
    "clean_args = [\"./bin\", \"-type\", \"f\", \"-name\", \"*.exe\", \"-delete\"]\n",
    "if os_name == \"windows\":\n",
    "    clean_command = windows_shell_path\n",
    "    if windows_shell_name == \"powershell.exe\":\n",
    "        clean_args = [\"-NoProfile\", \"-ExecutionPolicy\", \"Bypass\", \"-Command\", \"Get-ChildItem -Path .\\\\bin -Include *.exe, *.ilk, *.pdb, *.obj -Recurse | Remove-Item -Force\"]\n",
    "    else:\n",
    "        clean_args = [\"/c\", \"del /s /q /f .\\\\bin\\\\*.exe 2>nul .\\\\bin\\\\*.ilk 2>nul .\\\\bin\\\\*.pdb 2>nul .\\\\bin\\\\*.obj pdb 2>nul\"]\n",
    "\n",
    "c_build_command = c_compiler_path\n",
    "c_build_multi_args = [\"-std=c17\", \"-Wall\", \"-g\", src_path, \"-I\", include_path, \"-o\", bin_path] \n",
    "c_build_active_args = [\"-std=c17\", \"-Wall\", \"-g\", \"${file}\", \"-o\", bin_path]\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    c_build_multi_args = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"src\\\\*.c\", \"/I\", \"include\"]\n",
    "    c_build_active_args = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"${file}\"]\n",
    "\n",
    "c_build_multi_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", src_path, \"-I\", include_path, \"-I\", opencl_include_dir, \"-L\", opencl_lib_dir, \"-l\", \"OpenCL\", \"-o\", bin_path] \n",
    "c_build_active_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", \"${file}\", \"-I\", opencl_include_dir, \"-L\", opencl_lib_dir, \"-l\", \"OpenCL\", \"-o\", bin_path]\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    c_build_multi_args_opencl = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"src\\\\*.c\", \"/I\", \"include\", \"/I\", opencl_include_dir, \"/link\", opencl_lib_dir, \"OpenCL.lib\"]\n",
    "    c_build_active_args_opencl = [\"/std:c17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"${file}\", \"/I\", opencl_include_dir, \"/link\", opencl_lib_dir, \"OpenCL.lib\"]\n",
    "if os_name == \"osx\" and c_compiler_name == \"clang\":\n",
    "    c_build_multi_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", src_path, \"-I\", include_path, \"-framework\", \"OpenCL\", \"-o\", bin_path] \n",
    "    c_build_active_args_opencl = [\"-std=c17\", \"-Wall\", \"-g\", \"${file}\", \"-framework\", \"OpenCL\", \"-o\", bin_path]\n",
    "\n",
    "tasks_json = {\n",
    "    \"version\": \"2.0.0\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"Make directories\",\n",
    "            \"command\": makedir_command,\n",
    "            \"args\": makedir_args,\n",
    "            \"problemMatcher\": []\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"Clean .exe files\",\n",
    "            \"dependsOn\": [\"Make directories\"],\n",
    "            \"command\": clean_command,\n",
    "            \"args\": clean_args,\n",
    "            \"problemMatcher\": []\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"opencl: build multi file\",\n",
    "            \"dependsOn\": [\"Clean .exe files\"],\n",
    "            \"command\": c_build_command,\n",
    "            \"args\": c_build_multi_args_opencl,\n",
    "            \"options\": {\n",
    "                \"cwd\": \"${workspaceFolder}\"\n",
    "            },\n",
    "            \"problemMatcher\": [\n",
    "                \"$gcc\"\n",
    "            ],\n",
    "            \"group\": {\n",
    "                \"kind\": \"build\",\n",
    "                \"isDefault\": False\n",
    "            },\n",
    "            \"detail\": f\"compiler: {c_compiler_path}\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"opencl: build active file\",\n",
    "            \"dependsOn\": [\"Clean .exe files\"],\n",
    "            \"command\": c_build_command,\n",
    "            \"args\": c_build_active_args_opencl,\n",
    "            \"options\": {\n",
    "                \"cwd\": \"${fileDirname}\"\n",
    "            },\n",
    "            \"problemMatcher\": [\n",
    "                \"$gcc\"\n",
    "            ],\n",
    "            \"group\": {\n",
    "                \"kind\": \"build\",\n",
    "                \"isDefault\": True\n",
    "            },\n",
    "            \"detail\": f\"compiler: {c_compiler_path}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "json_string = json.dumps(tasks_json, indent=4)\n",
    "with open(\".vscode/tasks.json\", \"w\") as f:\n",
    "    json.dump(tasks_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88dea1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.7 Create the File `launch.json`\n",
    "\n",
    "- Run the cell below to create the file `launch.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "70ff14b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "if \"gdb\" in c_debugger_name:\n",
    "    c_debugger_type = \"cppdbg\"\n",
    "    c_debugger_mi_mode = \"gdb\"\n",
    "    stop_at_entry_name = \"stopAtEntry\"\n",
    "    environment = True\n",
    "    console = False\n",
    "    setupcommands = [{\"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": True}]\n",
    "if \"lldb\" in c_debugger_name:\n",
    "    c_debugger_type = \"lldb\"\n",
    "    c_debugger_mi_mode = \"lldb\"\n",
    "    stop_at_entry_name = \"stopOnEntry\"\n",
    "    environment = False\n",
    "    console = False\n",
    "    setupcommands = None\n",
    "if c_debugger_name == \"cdb.exe\":\n",
    "    c_debugger_name = \"msvc\"\n",
    "    c_debugger_path = None\n",
    "    c_debugger_type = \"cppvsdbg\"\n",
    "    c_debugger_mi_mode = None\n",
    "    stop_at_entry_name = \"stopAtEntry\"\n",
    "    environment = True\n",
    "    console = True\n",
    "    setupcommands = None\n",
    "\n",
    "launch_json = {\n",
    "    \"version\": \"0.2.0\",\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": \"opencl: launch multi file\",\n",
    "            \"preLaunchTask\": \"opencl: build multi file\",\n",
    "            \"type\": c_debugger_type,\n",
    "            \"request\": \"launch\",\n",
    "            \"program\": bin_path,\n",
    "            \"args\": [],\n",
    "            f\"{stop_at_entry_name}\": False,\n",
    "            \"cwd\": \"${workspaceFolder}\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"opencl: launch active file\",\n",
    "            \"preLaunchTask\": \"opencl: build active file\",\n",
    "            \"type\": c_debugger_type,\n",
    "            \"request\": \"launch\",\n",
    "            \"program\": bin_path,\n",
    "            \"args\": [],\n",
    "            f\"{stop_at_entry_name}\": False,\n",
    "            \"cwd\": \"${workspaceFolder}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "if environment:\n",
    "    for i in range(2):\n",
    "        launch_json[\"configurations\"][i][\"environment\"] = []\n",
    "if c_debugger_name != \"lldb\":\n",
    "    if console:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"console\"] = \"integratedTerminal\" # \"externalTerminal\"\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"externalConsole\"] = False # True\n",
    "    if c_debugger_mi_mode:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"MIMode\"] = c_debugger_mi_mode\n",
    "    if c_debugger_path:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"miDebuggerPath\"] = c_debugger_path\n",
    "    if setupcommands:\n",
    "        for i in range(2):\n",
    "            launch_json[\"configurations\"][i][\"setupCommands\"] = setupcommands\n",
    "\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "with open(\".vscode/launch.json\", \"w\") as f:\n",
    "    json.dump(launch_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feec20f",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.8 Create the File `c_cpp_properties.json`\n",
    "\n",
    "- Run the cell below to create the file `c_cpp_properties.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "86f2d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "if os_name == \"linux\" and c_compiler_name == \"gcc\":\n",
    "    intelliSenseMode = \"linux-gcc-x64\"\n",
    "    # intelliSenseMode = \"linux-gcc-arm64\"\n",
    "if os_name == \"linux\" and c_compiler_name == \"clang\":\n",
    "    intelliSenseMode = \"linux-clang-x64\"\n",
    "    # intelliSenseMode = \"linux-clang-arm64\"\n",
    "if os_name == \"osx\" and c_compiler_name == \"gcc\":\n",
    "    intelliSenseMode = \"macos-gcc-x64\"\n",
    "    # intelliSenseMode = \"macos-gcc-arm64\"\n",
    "if os_name == \"osx\" and c_compiler_name == \"clang\":\n",
    "    intelliSenseMode = \"macos-clang-x64\"\n",
    "    # intelliSenseMode = \"macos-clang-arm64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"gcc.exe\":\n",
    "    intelliSenseMode = \"windows-gcc-x64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"clang.exe\":\n",
    "    intelliSenseMode = \"windows-clang-x64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    intelliSenseMode = \"windows-msvc-x64\"\n",
    "\n",
    "launch_json = {\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": \"Linter\",\n",
    "            \"includePath\": [\n",
    "                \"${workspaceFolder}/**\"\n",
    "            ],\n",
    "            \"defines\": [],\n",
    "            \"compilerPath\": c_compiler_path,\n",
    "            \"cStandard\": \"c17\",\n",
    "            \"cppStandard\": \"c++17\",\n",
    "            \"intelliSenseMode\": intelliSenseMode\n",
    "        }\n",
    "    ],\n",
    "    \"version\": 4\n",
    "}\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "with open(\".vscode/c_cpp_properties.json\", \"w\") as f:\n",
    "    json.dump(launch_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6828fd9",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.9 VSCode Extensions\n",
    "\n",
    "- To develop OpenCL programs in C with VSCode, we need a few VSCode extensions (the last two are only needed for Jupyter Notebooks).\n",
    "  - C/C++ Extension Pack: https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools-extension-pack\n",
    "  - CodeLLDB: https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb\n",
    "  - Makefile Tools: https://marketplace.visualstudio.com/items?itemName=ms-vscode.makefile-tools\n",
    "  - Jupyter: https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter\n",
    "  - Python: https://marketplace.visualstudio.com/items?itemName=ms-python.python\n",
    "\n",
    "- Run the cell below to install any missing VSCode extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "9d66c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing extensions...\n",
      "Extension 'ms-vscode.cpptools-extension-pack' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'vadimcn.vscode-lldb' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-vscode.makefile-tools' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-toolsai.jupyter' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-python.python' is already installed.\n"
     ]
    }
   ],
   "source": [
    "!code --install-extension ms-vscode.cpptools-extension-pack --force\n",
    "!code --install-extension vadimcn.vscode-lldb --force\n",
    "!code --install-extension ms-vscode.makefile-tools --force\n",
    "!code --install-extension ms-toolsai.jupyter --force\n",
    "!code --install-extension ms-python.python --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca922521",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.10 Using Built-in Cell Magic `%%writefile`\n",
    "\n",
    "- The cell magic `%%writefile filename`, writes the contents of a notebook cell to the specified `filename` (or file path).\n",
    "  - This functionality is built-in to Jupyter Notebooks (it's not an extension).\n",
    "  - We can use it to write any code cell contents to a file in the file system.\n",
    "  - Let's write some OpenCL code to the file `main.c` with a kernel in `kernel.c`.\n",
    "- Run the two cells below.\n",
    "- Then inspect the resulting files `kernel.cl` and `main.c`, which contain each code cell's contents (except for the cell magic command `%%writefile filename`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "6e14b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "const char *source =\n",
    "\"__kernel void mykernel()\\n\"\n",
    "\"{\\n\"\n",
    "\"    printf(\\\"Hello World!\\\\n\\\");\\n\"\n",
    "\"}\\n\";\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &err);\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "\n",
    "    clFinish(queue);\n",
    "\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae19c0",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.11 Compiling and Executing an OpenCL Program From a Notebook Code Cell\n",
    "\n",
    "- We can compile and execute an OpenCL program from a notebook code cell using the syntax `!<shell command>`, where:\n",
    "  - `!` indicates that the succeeding text on the same row should be sent to the shell (terminal).\n",
    "  - `<shell command>` is the shell (terminal) command we want to execute.\n",
    "  - Standard output is redirected to the cell output.\n",
    "\n",
    "- Run the cell below to see what the build command and execute command is in your shell:\n",
    "  - The *build single file* command compiles and links the file `main.c` in your workspace folder and places the executable file `main.exe` in the `bin` folder.\n",
    "  - The *build multi file* command compiles and links all `.c` files in the `src` and `.h` files in the `include` folder and places the executable file `main.exe` in the `bin` folder.\n",
    "  - The *execute* command executes the file `main.exe` in the `bin` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "95fadcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build single file command : \"/usr/bin/gcc\" -std=c17 -Wall -g main.c -I /usr/local/cuda-12.8/targets/x86_64-linux/include -L /usr/local/cuda-12.8/targets/x86_64-linux/lib -l OpenCL -o ./bin/main.exe\n",
      "Build multi file command  : \"/usr/bin/gcc\" -std=c17 -Wall -g ./src/*.c -I ./include -I /usr/local/cuda-12.8/targets/x86_64-linux/include -L /usr/local/cuda-12.8/targets/x86_64-linux/lib -l OpenCL -o ./bin/main.exe\n",
      "Execute command           : ./bin/main.exe\n"
     ]
    }
   ],
   "source": [
    "build_single_file_command = [f'\"{c_build_command}\"'] + c_build_active_args_opencl\n",
    "build_single_file_command = \" \".join(build_single_file_command).replace('${file}', 'main.c').replace('${workspaceFolder}', '.')\n",
    "\n",
    "build_multi_file_command = [f'\"{c_build_command}\"'] + c_build_multi_args_opencl\n",
    "build_multi_file_command = \" \".join(build_multi_file_command).replace('${file}', 'main.c').replace('${workspaceFolder}', '.')\n",
    "\n",
    "execute_command = bin_path.replace('${workspaceFolder}', '.')\n",
    "\n",
    "print(f'Build single file command : {build_single_file_command}')\n",
    "print(f'Build multi file command  : {build_multi_file_command}')\n",
    "print(f'Execute command           : {execute_command}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7298415",
   "metadata": {},
   "source": [
    "- Run the cell below to:\n",
    "  - Create the folder `bin` in your workspace folder if it doesn't already exist.\n",
    "  - Build the single source code file `main.c` in your workspace folder into the executable file `main.exe` in the `bin` folder.\n",
    "  - Run the executable file `main.exe` in the `bin` folder.\n",
    "- Notice the file `main.exe` has been created in the file system (in the `bin` folder), and the program's output is shown as the cell's output in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "d6645bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"bin\", exist_ok=True)\n",
    "\n",
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68970095",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.12 Compiling and Debugging a Single-file OpenCL Program\n",
    "\n",
    "Let's see `tasks.json` and `launch.json` in action for a single-file (`.c`) OpenCL program.\n",
    "\n",
    "- First, let's create the file `main.c` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "f7574216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "const char *source =\n",
    "\"__kernel void mykernel()\\n\"\n",
    "\"{\\n\"\n",
    "\"    printf(\\\"Hello World!\\\\n\\\");\\n\"\n",
    "\"}\\n\";\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        fprintf(stderr, \"%s failed: %d\\n\", msg, err);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "\n",
    "    // 3. Context\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "\n",
    "    // 4. Command queue\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "\n",
    "    // 5. Program\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateProgramWithSource\");\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "\n",
    "    // (Optional) Check build log if needed\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    // 6. Kernel\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "    checkOpenCL(err, \"clCreateKernel\");\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "\n",
    "    // 8. Wait and finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d672e0",
   "metadata": {},
   "source": [
    "- Now, let's debug the file `main.c`.\n",
    "  - Open the file `main.c` in VSCode's editor.\n",
    "  - Set a breakpoint on the two `return` statements (`F9`).\n",
    "  - Switch to the `Run and Debug` view (Linux/Windows: `Ctrl + Shift + D`, Mac: `Cmd + Shift + D`).\n",
    "  - In the drop-down combobox, select the launch configuration `<COMPILER>: launch active file`, where `<COMPILER>`is the name of your C compiler.\n",
    "  - Click the green `Play` icon.\n",
    "  - Use the debug toolbar in the top-middle of VSCode to debug the code.\n",
    "    - Notice the debugger stops at the breakpoints.\n",
    "      - This is because we are using a C debugger compatible with your chosen C compiler.\n",
    "    - Notice you can view variables (local, registers), watch variables, view the call stack, and toggle breakpoints in the `Run and Debug` view.\n",
    "  - Stop debugging (red `Square` icon in the debug toolbar).\n",
    "- Next, look at the status bar (at the bottom of VSCode) where you will see the name of the launch configuration `<COMPILER>: launch active file`.\n",
    "  - Click on it, and select `<COMPILER>: launch active file` again (make sure `main.c` is the active file in the editor, not the notebook).\n",
    "    - This is an alternative method to start a debug session.\n",
    "  - Stop debugging.\n",
    "- Press `F5` (make sure `main.c` is active in VSCode's editor), which is a third alternative to launch the debugger.\n",
    "  - This launches the debug configuration with `preLaunchTask` set to the default task (in `tasks.json`).\n",
    "  - Stop debugging.\n",
    "- Press (Linux/Windows: `Ctrl + Shift + B`, Mac: `Ctrl + Shift + B`) to execute the default build task (in `tasks.json`).\n",
    "  - Make sure `main.c` is active in VSCode's editor (since the default build task is set to the active file task).\n",
    "  - Notice the compiled executable `main.exe` is placed in the subfolder `bin` (configured in the default build task).\n",
    "    - This is also where the debugger finds the executable `main.exe` (configured in `launch.json`).\n",
    "\n",
    "- Remeber, you can always compile a single `main.c` file in your workspace folder and run it using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "8d05101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3a935",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.13 Compiling and Debugging a Multi-file OpenCL Program\n",
    "\n",
    "- Let's see `tasks.json` and `launch.json` in action for a multi-file (`*.c`) OpenCL program.\n",
    "  - First We will create two source code files `.c` in the `src` folder, and one header file `.h` in the `include` folder.\n",
    "    - We will use the same code as before, but will place the OpenCL kernel (function) code in its own `.cl` file, together with a source code file `.c` for loading it and its prototype in a header file `.h`.\n",
    "  - Then we will use:\n",
    "    - The other (non-default) build task in `tasks.json` to build the executable.\n",
    "    - The other launch configuration (linked to the non-default build task) in `launch.json` to debug it.\n",
    "\n",
    "- Run the four cells below to create:\n",
    "  - The folder structure `src`, `include`, and `bin` (if it hasn't already been created).\n",
    "  - The main source code file `main.c` in the folder `src`.\n",
    "  - The kernel source code file `helloKernel.cl` in the folder `src`.\n",
    "  - The source code file `utils.c` in the folder `src`.\n",
    "  - The header file `utils.h` in the folder `include`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "84d878c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "os.makedirs(\"include\", exist_ok=True)\n",
    "os.makedirs(\"bin\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "5300a14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel()\n",
    "{\n",
    "    printf(\"Hello World!\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "881eae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "#define KERNEL_FILE \"src/kernel.cl\"\n",
    "\n",
    "char* read_kernel_source(const char* filename)\n",
    "{\n",
    "    FILE* fp = fopen(filename, \"r\");\n",
    "    if (!fp)\n",
    "    {\n",
    "        fprintf(stderr, \"Failed to open kernel file: %s\\n\", filename);\n",
    "        exit(1);\n",
    "    }\n",
    "    fseek(fp, 0, SEEK_END);\n",
    "    long size = ftell(fp);\n",
    "    rewind(fp);\n",
    "    char* src = (char*)malloc(size + 1);\n",
    "    fread(src, 1, size, fp);\n",
    "    src[size] = '\\0';\n",
    "    fclose(fp);\n",
    "    return src;\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "\n",
    "    // 3. Context\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "\n",
    "    // 4. Command queue\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "\n",
    "    // 5. Program\n",
    "    char* source = read_kernel_source(KERNEL_FILE); // Read kernel source\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, (const char**)&source, NULL, &err);\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "\n",
    "    // (Optional) Check build log if needed\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "\n",
    "    // 6. Kernel\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "\n",
    "    // 8. Wait and finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    free(source);\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f5850",
   "metadata": {},
   "source": [
    "- Build the multi-file C program.\n",
    "  - Notice the multi-file build task in `tasks.json` isn't the default build task (`isDefault` is not set to `true` under `group`).\n",
    "  - Therefore, we can't use (Linux/Windows: `Ctrl + Shift + B`, mac: `Cmd + Shift + B`).\n",
    "  - Instead we can:\n",
    "    - Bring up the Command Palette (Linux/Windows: `Ctrl + Shift + P`, mac: `Cmd + Shift + P`).\n",
    "    - Choose `Tasks: Run Task` and select the task `<COMPILER>: build multi file`, where `<COMPILER>` is the name of your chosen C compiler.\n",
    "  - The executable `main.exe` is placed in the `bin` folder (as configured in `tasks.json`).\n",
    "- Debug the multi-file C program.\n",
    "  - Open `main.c` and `math_utils.c` in the `src` folder and set breakpoints on the two `return` statements.\n",
    "  - Notice the multi-file launch task in `launch.json` isn't linked to the default build task (in `tasks.json`).\n",
    "    - Therefore, we can't use `F5`.\n",
    "    - Instead we can:\n",
    "      - Switch the the `Run and Debug` view, select `<COMPILER>: launch multi file` from the drop-down list, and click the green `Play` icon.\n",
    "      - Or select `<COMPILER>: launch multi file` from the status bar (at the bottom of VSCode).\n",
    "    - The C program is built and the debugger lauched, attaching to the executable `main.exe` in the `bin` folder.\n",
    "  - Stop debugging.\n",
    "\n",
    "- Remeber, you can always compile a multi-file C program (`src/*.c`, `include/*.h`) and run it using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "3aae85f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa3231",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. OpenCL Basics\n",
    "---\n",
    "\n",
    "- Now we know how to create CUDA programs in C (single-file, multi-file, and in a Jupyter Notebook cell).\n",
    "- Going forward, we will explore fundamental CUDA programming concepts as single-file programs in notebook cells using the `%%cuda` cell magic command.\n",
    "  - The code in each `%%cuda` cell can be placed in a `main.cu` file by:\n",
    "    - Manually copying the cell contents and removing the `%%cuda` row.\n",
    "    - Replacing the `%%cuda` row with `%%writefile main.cu` and running the cell.\n",
    "  - Then you can manually compile it to `main.exe` with `nvcc main.cu -o main.exe`.\n",
    "- NVidia's documentation is a good source to learn more about CUDA:\n",
    "    - [Cuda C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/contents.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a988e3b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 Listing OpenCL-enabled Devices and Properties\n",
    "\n",
    "- First, let's find out what OpenCL-enabled devices are available on your computer.\n",
    "\n",
    "### Using `clinfo`\n",
    "\n",
    "- The simplest way to list OpenCL-enabled devices is using the tool `clinfo`.\n",
    "  - The top row shows you the number of platforms on your system:\n",
    "    - Example platforms: Nvidia, AMD, Intel, etc.\n",
    "  - For each platform, it shows you which devices are available:\n",
    "    - Example devices: An Nvidia GPU, an AMD CPU, an AMD GPU, an Intel CPU, etc.\n",
    "  - For each device, it shows you information about that device.\n",
    "- Run the cell below to see the CUDA-enabled devices on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "7af0c17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of platforms                               1\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "  Platform Vendor                                 NVIDIA Corporation\n",
      "  Platform Version                                OpenCL 3.0 CUDA 12.8.90\n",
      "  Platform Profile                                FULL_PROFILE\n",
      "  Platform Extensions                             cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Platform Extensions with Version                cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "  Platform Numeric Version                        0xc00000 (3.0.0)\n",
      "  Platform Extensions function suffix             NV\n",
      "  Platform Host timer resolution                  0ns\n",
      "  Platform External memory handle types           Opaque FD\n",
      "  Platform Semaphore types                        <gatherPlatformInfo:11: get CL_PLATFORM_SEMAPHORE_TYPES_KHR size : error -30>\n",
      "  Platform External semaphore import types        Opaque FD\n",
      "  Platform External semaphore export types        <gatherPlatformInfo:13: get CL_PLATFORM_SEMAPHORE_EXPORT_HANDLE_TYPES_KHR : error -30>\n",
      "\n",
      "  Platform Name                                   NVIDIA CUDA\n",
      "Number of devices                                 1\n",
      "  Device Name                                     NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "  Device Vendor                                   NVIDIA Corporation\n",
      "  Device Vendor ID                                0x10de\n",
      "  Device Version                                  OpenCL 3.0 CUDA\n",
      "  Device UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Driver UUID                                     f62a0cf5-6bd9-a4c7-ec96-e1d06cc6b3c0\n",
      "  Valid Device LUID                               No\n",
      "  Device LUID                                     6d69-637300000000\n",
      "  Device Node Mask                                0\n",
      "  Device Numeric Version                          0xc00000 (3.0.0)\n",
      "  Driver Version                                  570.124.06\n",
      "  Device OpenCL C Version                         OpenCL C 1.2 \n",
      "  Device OpenCL C all versions                    OpenCL C                                                         0x400000 (1.0.0)\n",
      "                                                  OpenCL C                                                         0x401000 (1.1.0)\n",
      "                                                  OpenCL C                                                         0x402000 (1.2.0)\n",
      "                                                  OpenCL C                                                         0xc00000 (3.0.0)\n",
      "  Device OpenCL C features                        __opencl_c_fp64                                                  0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_images                                                0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_int64                                                 0xc00000 (3.0.0)\n",
      "                                                  __opencl_c_3d_image_writes                                       0xc00000 (3.0.0)\n",
      "  Latest conformance test passed                  v2023-10-10-00\n",
      "  Device Type                                     GPU\n",
      "  Device Topology (NV)                            PCI-E, 0000:01:00.0\n",
      "  Device PCI bus info (KHR)                       PCI-E, 0000:01:00.0\n",
      "  Device Profile                                  FULL_PROFILE\n",
      "  Device Available                                Yes\n",
      "  Compiler Available                              Yes\n",
      "  Linker Available                                Yes\n",
      "  Max compute units                               24\n",
      "  Max clock frequency                             1455MHz\n",
      "  Compute Capability (NV)                         8.9\n",
      "  Device Partition                                (core)\n",
      "    Max number of sub-devices                     1\n",
      "    Supported partition types                     None\n",
      "    Supported affinity domains                    (n/a)\n",
      "  Max work item dimensions                        3\n",
      "  Max work item sizes                             1024x1024x64\n",
      "  Max work group size                             1024\n",
      "  Preferred work group size multiple (device)     32\n",
      "  Preferred work group size multiple (kernel)     32\n",
      "  Warp size (NV)                                  32\n",
      "  Max sub-groups per work group                   0\n",
      "  Preferred / native vector sizes                 \n",
      "    char                                                 1 / 1       \n",
      "    short                                                1 / 1       \n",
      "    int                                                  1 / 1       \n",
      "    long                                                 1 / 1       \n",
      "    half                                                 0 / 0        (n/a)\n",
      "    float                                                1 / 1       \n",
      "    double                                               1 / 1        (cl_khr_fp64)\n",
      "  Half-precision Floating-point support           (n/a)\n",
      "  Single-precision Floating-point support         (core)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "    Correctly-rounded divide and sqrt operations  Yes\n",
      "  Double-precision Floating-point support         (cl_khr_fp64)\n",
      "    Denormals                                     Yes\n",
      "    Infinity and NANs                             Yes\n",
      "    Round to nearest                              Yes\n",
      "    Round to zero                                 Yes\n",
      "    Round to infinity                             Yes\n",
      "    IEEE754-2008 fused multiply-add               Yes\n",
      "    Support is emulated in software               No\n",
      "  Address bits                                    64, Little-Endian\n",
      "  External memory handle types                    Opaque FD\n",
      "  Semaphore types                                 <printDeviceInfo:105: get number of CL_DEVICE_SEMAPHORE_TYPES_KHR : error -30>\n",
      "  External semaphore import types                 Opaque FD\n",
      "  External semaphore export types                 (n/a)\n",
      "  Global memory size                              8198619136 (7.636GiB)\n",
      "  Error Correction support                        No\n",
      "  Max memory allocation                           2049654784 (1.909GiB)\n",
      "  Unified memory for Host and Device              No\n",
      "  Integrated memory (NV)                          No\n",
      "  Shared Virtual Memory (SVM) capabilities        (core)\n",
      "    Coarse-grained buffer sharing                 Yes\n",
      "    Fine-grained buffer sharing                   No\n",
      "    Fine-grained system sharing                   No\n",
      "    Atomics                                       No\n",
      "  Minimum alignment for any data type             128 bytes\n",
      "  Alignment of base address                       4096 bits (512 bytes)\n",
      "  Preferred alignment for atomics                 \n",
      "    SVM                                           0 bytes\n",
      "    Global                                        0 bytes\n",
      "    Local                                         0 bytes\n",
      "  Atomic memory capabilities                      relaxed, work-group scope\n",
      "  Atomic fence capabilities                       relaxed, acquire/release, work-group scope\n",
      "  Max size for global variable                    0\n",
      "  Preferred total size of global vars             0\n",
      "  Global Memory cache type                        Read/Write\n",
      "  Global Memory cache size                        688128 (672KiB)\n",
      "  Global Memory cache line size                   128 bytes\n",
      "  Image support                                   Yes\n",
      "    Max number of samplers per kernel             32\n",
      "    Max size for 1D images from buffer            268435456 pixels\n",
      "    Max 1D or 2D image array size                 2048 images\n",
      "    Base address alignment for 2D image buffers   0 bytes\n",
      "    Pitch alignment for 2D image buffers          0 pixels\n",
      "    Max 2D image size                             32768x32768 pixels\n",
      "    Max 3D image size                             16384x16384x16384 pixels\n",
      "    Max number of read image args                 256\n",
      "    Max number of write image args                32\n",
      "    Max number of read/write image args           0\n",
      "  Pipe support                                    No\n",
      "  Max number of pipe args                         0\n",
      "  Max active pipe reservations                    0\n",
      "  Max pipe packet size                            0\n",
      "  Local memory type                               Local\n",
      "  Local memory size                               49152 (48KiB)\n",
      "  Registers per block (NV)                        65536\n",
      "  Max number of constant args                     9\n",
      "  Max constant buffer size                        65536 (64KiB)\n",
      "  Generic address space support                   No\n",
      "  Max size of kernel argument                     32764 (32KiB)\n",
      "  Queue properties (on host)                      \n",
      "    Out-of-order execution                        Yes\n",
      "    Profiling                                     Yes\n",
      "  Device enqueue capabilities                     (n/a)\n",
      "  Queue properties (on device)                    \n",
      "    Out-of-order execution                        No\n",
      "    Profiling                                     No\n",
      "    Preferred size                                0\n",
      "    Max size                                      0\n",
      "  Max queues on device                            0\n",
      "  Max events on device                            0\n",
      "  Prefer user sync for interop                    No\n",
      "  Profiling timer resolution                      1000ns\n",
      "  Execution capabilities                          \n",
      "    Run OpenCL kernels                            Yes\n",
      "    Run native kernels                            No\n",
      "    Non-uniform work-groups                       No\n",
      "    Work-group collective functions               No\n",
      "    Sub-group independent forward progress        No\n",
      "    Kernel execution timeout (NV)                 No\n",
      "    Concurrent copy and kernel execution (NV)     Yes\n",
      "      Number of async copy engines                2\n",
      "    IL version                                    (n/a)\n",
      "    ILs with version                              (n/a)\n",
      "  printf() buffer size                            1048576 (1024KiB)\n",
      "  Built-in kernels                                (n/a)\n",
      "  Built-in kernels with version                   (n/a)\n",
      "  Device Extensions                               cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_fp64 cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_icd cl_khr_gl_sharing cl_nv_compiler_options cl_nv_device_attribute_query cl_nv_pragma_unroll cl_nv_copy_opts cl_khr_gl_event cl_nv_create_buffer cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_nv_kernel_attribute cl_khr_device_uuid cl_khr_pci_bus_info cl_khr_external_semaphore cl_khr_external_memory cl_khr_external_semaphore_opaque_fd cl_khr_external_memory_opaque_fd cl_khr_semaphore\n",
      "  Device Extensions with Version                  cl_khr_global_int32_base_atomics                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_global_int32_extended_atomics                             0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_base_atomics                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_local_int32_extended_atomics                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_fp64                                                      0x400000 (1.0.0)\n",
      "                                                  cl_khr_3d_image_writes                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_byte_addressable_store                                    0x400000 (1.0.0)\n",
      "                                                  cl_khr_icd                                                       0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_sharing                                                0x400000 (1.0.0)\n",
      "                                                  cl_nv_compiler_options                                           0x400000 (1.0.0)\n",
      "                                                  cl_nv_device_attribute_query                                     0x400000 (1.0.0)\n",
      "                                                  cl_nv_pragma_unroll                                              0x400000 (1.0.0)\n",
      "                                                  cl_nv_copy_opts                                                  0x400000 (1.0.0)\n",
      "                                                  cl_khr_gl_event                                                  0x400000 (1.0.0)\n",
      "                                                  cl_nv_create_buffer                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_base_atomics                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_int64_extended_atomics                                    0x400000 (1.0.0)\n",
      "                                                  cl_nv_kernel_attribute                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_device_uuid                                               0x400000 (1.0.0)\n",
      "                                                  cl_khr_pci_bus_info                                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore                                        0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory                                           0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_semaphore_opaque_fd                              0x400000 (1.0.0)\n",
      "                                                  cl_khr_external_memory_opaque_fd                                 0x400000 (1.0.0)\n",
      "                                                  cl_khr_semaphore                                                 0x400000 (1.0.0)\n",
      "\n",
      "NULL platform behavior\n",
      "  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform\n",
      "  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform\n",
      "  clCreateContext(NULL, ...) [default]            No platform\n",
      "  clCreateContext(NULL, ...) [other]              Success [NV]\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  No platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  Invalid device type for platform\n",
      "  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  No platform\n"
     ]
    }
   ],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cc78b",
   "metadata": {},
   "source": [
    "### Using C Code\n",
    "\n",
    "- We can also find out what Platforms and OpenCL-enabled devices are available using C code.\n",
    "  - The code below lists important properties that will become familiar the more you learn about OpenCL (for optimization purposes).\n",
    "- Run the cell below to list platforms, OpenCL-enabled devices, and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "394e2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        fprintf(stderr, \"%s failed: %d\\n\", msg, err);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_uint numPlatforms;\n",
    "    cl_int err;\n",
    "\n",
    "    err = clGetPlatformIDs(0, NULL, &numPlatforms);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs (count)\");\n",
    "\n",
    "    if (numPlatforms == 0)\n",
    "    {\n",
    "        puts(\"No OpenCL platforms found.\");\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    cl_platform_id *platforms = (cl_platform_id *) malloc(sizeof(cl_platform_id) * numPlatforms);\n",
    "    err = clGetPlatformIDs(numPlatforms, platforms, NULL);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs\");\n",
    "\n",
    "    for (cl_uint p = 0; p < numPlatforms; ++p)\n",
    "    {\n",
    "        char platformName[128];\n",
    "        clGetPlatformInfo(platforms[p], CL_PLATFORM_NAME, sizeof(platformName), platformName, NULL);\n",
    "        printf(\"Platform %u: %s\\n\", p, platformName);\n",
    "\n",
    "        cl_uint numDevices = 0;\n",
    "        err = clGetDeviceIDs(platforms[p], CL_DEVICE_TYPE_ALL, 0, NULL, &numDevices);\n",
    "        if (err != CL_SUCCESS || numDevices == 0)\n",
    "        {\n",
    "            puts(\"  No devices found.\");\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        cl_device_id *devices = (cl_device_id *) malloc(sizeof(cl_device_id) * numDevices);\n",
    "        err = clGetDeviceIDs(platforms[p], CL_DEVICE_TYPE_ALL, numDevices, devices, NULL);\n",
    "        checkOpenCL(err, \"clGetDeviceIDs\");\n",
    "\n",
    "        for (cl_uint i = 0; i < numDevices; ++i)\n",
    "        {\n",
    "            char name[128];\n",
    "            cl_uint compute_units, max_work_group_size, clock_frequency;\n",
    "            cl_ulong global_mem, local_mem, constant_mem;\n",
    "            cl_device_type type;\n",
    "\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(name), name, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(type), &type, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_COMPUTE_UNITS, sizeof(compute_units), &compute_units, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(max_work_group_size), &max_work_group_size, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_CLOCK_FREQUENCY, sizeof(clock_frequency), &clock_frequency, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_GLOBAL_MEM_SIZE, sizeof(global_mem), &global_mem, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_LOCAL_MEM_SIZE, sizeof(local_mem), &local_mem, NULL);\n",
    "            clGetDeviceInfo(devices[i], CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE, sizeof(constant_mem), &constant_mem, NULL);\n",
    "\n",
    "            printf(\"  - Device %u: %s\\n\", i, name);\n",
    "            printf(\"    - %-31s : %s\\n\", \"Type\",\n",
    "                (type == CL_DEVICE_TYPE_GPU) ? \"GPU\" :\n",
    "                (type == CL_DEVICE_TYPE_CPU) ? \"CPU\" :\n",
    "                (type == CL_DEVICE_TYPE_ACCELERATOR) ? \"Accelerator\" : \"Other\");\n",
    "            printf(\"    - %-31s : %u\\n\", \"Compute Units (CU)\", compute_units);\n",
    "            printf(\"    - %-31s : %u\\n\", \"Max Work Group Size\", max_work_group_size);\n",
    "            printf(\"    - %-31s : %.2f MHz\\n\", \"Clock Frequency\", clock_frequency * 1.0);\n",
    "            printf(\"    - %-31s : %lu bytes\\n\", \"Global Memory Size\", global_mem);\n",
    "            printf(\"    - %-31s : %lu bytes\\n\", \"Local Memory Size (shared)\", local_mem);\n",
    "            printf(\"    - %-31s : %lu bytes\\n\", \"Constant Memory Size\", constant_mem);\n",
    "            printf(\"\\n\");\n",
    "        }\n",
    "\n",
    "        free(devices);\n",
    "    }\n",
    "\n",
    "    free(platforms);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "89a47e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform 0: NVIDIA CUDA\n",
      "  - Device 0: NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "    - Type                            : GPU\n",
      "    - Compute Units (CU)              : 24\n",
      "    - Max Work Group Size             : 32767\n",
      "    - Clock Frequency                 : 1455.00 MHz\n",
      "    - Global Memory Size              : 8198619136 bytes\n",
      "    - Local Memory Size (shared)      : 49152 bytes\n",
      "    - Constant Memory Size            : 65536 bytes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd8e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Hello World in Host Code (CPU)\n",
    "\n",
    "- The program in the cell below is a simple C program (no CUDA code) that runs on the host (CPU).\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`.\n",
    "\n",
    "      ```c\n",
    "      #include <stdio.h>\n",
    "      ```\n",
    "  - Then we define the `main()` function:\n",
    "    - We print out the text `Hello World!`.\n",
    "    - Then we return the exit code `0` to the operating system.\n",
    "    \n",
    "      ```c\n",
    "      int main(void)\n",
    "      {\n",
    "        printf(\"Hello World!\\n\");\n",
    "        \n",
    "        return 0;\n",
    "      }\n",
    "      ```\n",
    "- Run the cell below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "8d0042a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "\n",
    "// Host entry point (a normal C main function)\n",
    "int main(void)\n",
    "{\n",
    "   printf(\"Hello World!\\n\");\n",
    "   \n",
    "   return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "63150b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763d82e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Hello World in Device Code (GPU)\n",
    "\n",
    "- The program in the cell below is a simple CUDA program that runs code on the host (CPU) and the device (GPU).\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`.\n",
    "    - `cuda_runtime.h` for `cudaDeviceSynchronize`.\n",
    "\n",
    "      ```c\n",
    "      #include <stdio.h>\n",
    "      #include <cuda_runtime.h> // contains CUDA function prototypes\n",
    "      ```\n",
    "  - Then we define a CUDA kernel function:\n",
    "    - `mykernel()` is the name of the **kernel function** (it can be any name we like).\n",
    "    - A **kernel function** runs on the device (GPU) and is **launched** (called) from the host (CPU).\n",
    "    - `__global__` is a CUDA **qualifier** (qualifying a function with `__global__` makes the function a **kernel function**.\n",
    "    - The kernel function takes no arguments (`void`), prints the text `Hello World!`, and does NOT return a value (`void`).\n",
    "    - All **kernel functions** MUST have a `void` return type.\n",
    "\n",
    "      ```c\n",
    "      __global__ void mykernel(void) // the __global__ qualifier makes this a kernel function\n",
    "      {\n",
    "          printf(\"Hello World!\\n\");\n",
    "      }\n",
    "      ```\n",
    "  - Lastly we define the `main()` function:\n",
    "    - First we **launch** (call) the **kernel function** `mykernel()` using the syntax `kernel<<<n_blocks, n_threads>>>()`.\n",
    "      - `kernel` is the name of the **kernel function** (`mykernel` in our code), which is called from the host (CPU) and runs on the device (GPU).\n",
    "      - `<<<n_blocks, n_threads>>>` is a CUDA **launch configuration** (`<<<1, 1>>>` in our code), which makes this a **kernel launch** (kernel function call).\n",
    "      - `n_blocks` is the number of `blocks` to use when launching the kernel function.\n",
    "        - In Nvidia terminology, a CUDA-enabled GPU contains a `grid` of `blocks`, where each `block` contains a number of `threads`.\n",
    "      - `n_threads` is the number of `threads` to use when launching the kernel function.\n",
    "        - Each `block` contains this number of `threads`.\n",
    "      - The code `mykernel<<<1,1>>>()` launches the kernel (calls the function) `mykernel()` on the GPU with 1 `block` containing 1 `thread` in that `block`.\n",
    "        - It's the *equivalent* of creating 1 new thread and running the function on that thread in a traditional C program running on the host (CPU).\n",
    "        - The kernel launch is an asynchronous function call, so it immediately returns control to the host (CPU).\n",
    "    - Then we call the CUDA function `cudaDeviceSynchronize()`.\n",
    "      - This function call is a synchronous call, which blocks the host's (CPU's) main thread until the kernel function completes (returns) on the device (GPU).\n",
    "      - This is necessary, otherwise the `main()` function would go out of scope (terminate) before we have a chance to retrieve any results from the kernel function.\n",
    "    - Finally we return the exit code `0` to the operating system.\n",
    "    \n",
    "      ```c\n",
    "      int main(void)\n",
    "      {\n",
    "        mykernel<<<1, 1>>>();    // kernel launch (calls the kernel function mykernel, and is asynchronous)\n",
    "        cudaDeviceSynchronize(); // blocks the host's main thread (CPU) until the kernel function completes (returns)\n",
    "        \n",
    "        return 0;\n",
    "      }\n",
    "      ```\n",
    "- Run the cell below to see the output.\n",
    "\n",
    "**TL;DR**\n",
    "\n",
    "- The CUDA function prototypes are declared in header file `cuda_runtime.h`.\n",
    "- A function with the `__global__` qualifier is called a **kernel function** that **runs on the device (GPU)** and is **called from the host (CPU)**.\n",
    "- A **kernel launch** calls a **kernel function** using the syntax `kernelfunction<<<n_blocks, n_threads>>>()` and is an **asynchrounous call**.\n",
    "- The function `cudaDeviceSynchronize()` is a **synchronous** call that blocks the host code until the **kernel function** is complete (returns).\n",
    "- The Nvidia compiler `nvcc` separates source code into **host** and **device** components.\n",
    "  - Device functions (e.g. `mykernel()`) is processed by the NVIDIA compiler `nvcc`.\n",
    "  - Host functions (e.g. `main()`) are processed by a standard host C compiler (e.g. `gcc`, `clang`, or `cl.exe`).\n",
    "    - NVCC instructs the underlying C compiler to compile host code during the compilation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "1d5e242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "const char *source =\n",
    "\"__kernel void mykernel()\\n\"\n",
    "\"{\\n\"\n",
    "\"    printf(\\\"Hello World!\\\\n\\\");\\n\"\n",
    "\"}\\n\";\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        fprintf(stderr, \"%s failed: %d\\n\", msg, err);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "void checkOpenCLBuildLog(cl_int err, cl_program program, cl_device_id device)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs\");\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "    checkOpenCL(err, \"clGetDeviceIDs\");\n",
    "\n",
    "    // 3. Context\n",
    "    cl_context context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateContext\");\n",
    "\n",
    "    // 4. Command queue\n",
    "    cl_command_queue queue = clCreateCommandQueueWithProperties(context, device, 0, &err);\n",
    "    checkOpenCL(err, \"lCreateCommandQueueWithProperties\");\n",
    "\n",
    "    // 5. Program\n",
    "    cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateProgramWithSource\");\n",
    "    err = clBuildProgram(program, 1, &device, NULL, NULL, NULL);\n",
    "    checkOpenCLBuildLog(err, program, device);\n",
    "\n",
    "    // 6. Kernel\n",
    "    cl_kernel kernel = clCreateKernel(program, \"mykernel\", &err);\n",
    "    checkOpenCL(err, \"clCreateKernel\");\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "\n",
    "    // 8. Wait and finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    clReleaseKernel(kernel);\n",
    "    clReleaseProgram(program);\n",
    "    clReleaseCommandQueue(queue);\n",
    "    clReleaseContext(context);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "0e45c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722cac4",
   "metadata": {},
   "source": [
    "### Creating a Utility Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "c0ebd150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define KERNEL_FILE \"src/kernel.cl\"\n",
    "#define KERNEL_NAME \"mykernel\"\n",
    "\n",
    "char* read_kernel_source(const char* filename)\n",
    "{\n",
    "    FILE* fp = fopen(filename, \"r\");\n",
    "    if (!fp)\n",
    "    {\n",
    "        fprintf(stderr, \"Failed to open kernel file: %s\\n\", filename);\n",
    "        exit(1);\n",
    "    }\n",
    "    fseek(fp, 0, SEEK_END);\n",
    "    long size = ftell(fp);\n",
    "    rewind(fp);\n",
    "    char* src = (char*)malloc(size + 1);\n",
    "    fread(src, 1, size, fp);\n",
    "    src[size] = '\\0';\n",
    "    fclose(fp);\n",
    "    return src;\n",
    "}\n",
    "\n",
    "const char* openclGetErrorString(cl_int err)\n",
    "{\n",
    "    switch (err)\n",
    "    {\n",
    "        case CL_SUCCESS:                                  return \"CL_SUCCESS\";\n",
    "        case CL_DEVICE_NOT_FOUND:                         return \"CL_DEVICE_NOT_FOUND\";\n",
    "        case CL_DEVICE_NOT_AVAILABLE:                     return \"CL_DEVICE_NOT_AVAILABLE\";\n",
    "        case CL_COMPILER_NOT_AVAILABLE:                   return \"CL_COMPILER_NOT_AVAILABLE\";\n",
    "        case CL_MEM_OBJECT_ALLOCATION_FAILURE:            return \"CL_MEM_OBJECT_ALLOCATION_FAILURE\";\n",
    "        case CL_OUT_OF_RESOURCES:                         return \"CL_OUT_OF_RESOURCES\";\n",
    "        case CL_OUT_OF_HOST_MEMORY:                       return \"CL_OUT_OF_HOST_MEMORY\";\n",
    "        case CL_PROFILING_INFO_NOT_AVAILABLE:             return \"CL_PROFILING_INFO_NOT_AVAILABLE\";\n",
    "        case CL_MEM_COPY_OVERLAP:                         return \"CL_MEM_COPY_OVERLAP\";\n",
    "        case CL_IMAGE_FORMAT_MISMATCH:                    return \"CL_IMAGE_FORMAT_MISMATCH\";\n",
    "        case CL_IMAGE_FORMAT_NOT_SUPPORTED:               return \"CL_IMAGE_FORMAT_NOT_SUPPORTED\";\n",
    "        case CL_BUILD_PROGRAM_FAILURE:                    return \"CL_BUILD_PROGRAM_FAILURE\";\n",
    "        case CL_MAP_FAILURE:                              return \"CL_MAP_FAILURE\";\n",
    "        case CL_MISALIGNED_SUB_BUFFER_OFFSET:             return \"CL_MISALIGNED_SUB_BUFFER_OFFSET\";\n",
    "        case CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST:return \"CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST\";\n",
    "        case CL_COMPILE_PROGRAM_FAILURE:                  return \"CL_COMPILE_PROGRAM_FAILURE\";\n",
    "        case CL_LINKER_NOT_AVAILABLE:                     return \"CL_LINKER_NOT_AVAILABLE\";\n",
    "        case CL_LINK_PROGRAM_FAILURE:                     return \"CL_LINK_PROGRAM_FAILURE\";\n",
    "        case CL_DEVICE_PARTITION_FAILED:                  return \"CL_DEVICE_PARTITION_FAILED\";\n",
    "        case CL_KERNEL_ARG_INFO_NOT_AVAILABLE:            return \"CL_KERNEL_ARG_INFO_NOT_AVAILABLE\";\n",
    "\n",
    "        // clCreateProgramWithSource, clBuildProgram, etc.\n",
    "        case CL_INVALID_VALUE:                            return \"CL_INVALID_VALUE\";\n",
    "        case CL_INVALID_DEVICE_TYPE:                      return \"CL_INVALID_DEVICE_TYPE\";\n",
    "        case CL_INVALID_PLATFORM:                         return \"CL_INVALID_PLATFORM\";\n",
    "        case CL_INVALID_DEVICE:                           return \"CL_INVALID_DEVICE\";\n",
    "        case CL_INVALID_CONTEXT:                          return \"CL_INVALID_CONTEXT\";\n",
    "        case CL_INVALID_QUEUE_PROPERTIES:                 return \"CL_INVALID_QUEUE_PROPERTIES\";\n",
    "        case CL_INVALID_COMMAND_QUEUE:                    return \"CL_INVALID_COMMAND_QUEUE\";\n",
    "        case CL_INVALID_HOST_PTR:                         return \"CL_INVALID_HOST_PTR\";\n",
    "        case CL_INVALID_MEM_OBJECT:                       return \"CL_INVALID_MEM_OBJECT\";\n",
    "        case CL_INVALID_IMAGE_FORMAT_DESCRIPTOR:          return \"CL_INVALID_IMAGE_FORMAT_DESCRIPTOR\";\n",
    "        case CL_INVALID_IMAGE_SIZE:                       return \"CL_INVALID_IMAGE_SIZE\";\n",
    "        case CL_INVALID_SAMPLER:                          return \"CL_INVALID_SAMPLER\";\n",
    "        case CL_INVALID_BINARY:                           return \"CL_INVALID_BINARY\";\n",
    "        case CL_INVALID_BUILD_OPTIONS:                    return \"CL_INVALID_BUILD_OPTIONS\";\n",
    "        case CL_INVALID_PROGRAM:                          return \"CL_INVALID_PROGRAM\";\n",
    "        case CL_INVALID_PROGRAM_EXECUTABLE:               return \"CL_INVALID_PROGRAM_EXECUTABLE\";\n",
    "        case CL_INVALID_KERNEL_NAME:                      return \"CL_INVALID_KERNEL_NAME\";\n",
    "        case CL_INVALID_KERNEL_DEFINITION:                return \"CL_INVALID_KERNEL_DEFINITION\";\n",
    "        case CL_INVALID_KERNEL:                           return \"CL_INVALID_KERNEL\";\n",
    "        case CL_INVALID_ARG_INDEX:                        return \"CL_INVALID_ARG_INDEX\";\n",
    "        case CL_INVALID_ARG_VALUE:                        return \"CL_INVALID_ARG_VALUE\";\n",
    "        case CL_INVALID_ARG_SIZE:                         return \"CL_INVALID_ARG_SIZE\";\n",
    "        case CL_INVALID_WORK_DIMENSION:                   return \"CL_INVALID_WORK_DIMENSION\";\n",
    "        case CL_INVALID_WORK_GROUP_SIZE:                  return \"CL_INVALID_WORK_GROUP_SIZE\";\n",
    "        case CL_INVALID_WORK_ITEM_SIZE:                   return \"CL_INVALID_WORK_ITEM_SIZE\";\n",
    "        case CL_INVALID_GLOBAL_OFFSET:                    return \"CL_INVALID_GLOBAL_OFFSET\";\n",
    "        case CL_INVALID_EVENT_WAIT_LIST:                  return \"CL_INVALID_EVENT_WAIT_LIST\";\n",
    "        case CL_INVALID_EVENT:                            return \"CL_INVALID_EVENT\";\n",
    "        case CL_INVALID_OPERATION:                        return \"CL_INVALID_OPERATION\";\n",
    "        case CL_INVALID_GL_OBJECT:                        return \"CL_INVALID_GL_OBJECT\";\n",
    "        case CL_INVALID_BUFFER_SIZE:                      return \"CL_INVALID_BUFFER_SIZE\";\n",
    "        case CL_INVALID_MIP_LEVEL:                        return \"CL_INVALID_MIP_LEVEL\";\n",
    "        case CL_INVALID_GLOBAL_WORK_SIZE:                 return \"CL_INVALID_GLOBAL_WORK_SIZE\";        \n",
    "        default:\n",
    "        {\n",
    "            static char unknown[64];\n",
    "            snprintf(unknown, sizeof(unknown), \"Unknown OpenCL error code %d\", err);\n",
    "            return unknown;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void checkOpenCL(cl_int err, const char *msg)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        printf(\"Error: %s (%s)\\n\", msg, openclGetErrorString(err));\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "void checkOpenCLBuildLog(cl_int err, cl_program program, cl_device_id device)\n",
    "{\n",
    "    if (err != CL_SUCCESS)\n",
    "    {\n",
    "        char log[2048];\n",
    "        clGetProgramBuildInfo(program, device, CL_PROGRAM_BUILD_LOG, sizeof(log), log, NULL);\n",
    "        printf(\"Build log:\\n%s\\n\", log);\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "void setupOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel)\n",
    "{\n",
    "    cl_int err;\n",
    "    \n",
    "    // 1. Platform\n",
    "    cl_platform_id platform;\n",
    "    cl_uint num_platforms;\n",
    "    err = clGetPlatformIDs(1, &platform, &num_platforms);\n",
    "    checkOpenCL(err, \"clGetPlatformIDs\");\n",
    "\n",
    "    // 2. Device\n",
    "    cl_device_id device;\n",
    "    cl_uint num_devices;\n",
    "    err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_DEFAULT, 1, &device, &num_devices);\n",
    "    checkOpenCL(err, \"clGetDeviceIDs\");\n",
    "\n",
    "    // 3. Context\n",
    "    /*cl_context*/ *context = clCreateContext(NULL, 1, &device, NULL, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateContext\");\n",
    "\n",
    "    // 4. Command queue\n",
    "    const cl_queue_properties props[] = { CL_QUEUE_PROPERTIES, CL_QUEUE_PROFILING_ENABLE, 0 };\n",
    "    /*cl_command_queue*/ *queue = clCreateCommandQueueWithProperties(*context, device, props, &err);\n",
    "    checkOpenCL(err, \"lCreateCommandQueueWithProperties\");\n",
    "\n",
    "    // 5. Program\n",
    "    char* source = read_kernel_source(KERNEL_FILE); // Read kernel source\n",
    "    /*cl_program*/ *program = clCreateProgramWithSource(*context, 1, (const char**)&source, NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateProgramWithSource\");\n",
    "    err = clBuildProgram(*program, 1, &device, NULL, NULL, NULL);\n",
    "    checkOpenCLBuildLog(err, *program, device);\n",
    "\n",
    "    // 6. Kernel\n",
    "    /*cl_kernel*/ *kernel = clCreateKernel(*program, KERNEL_NAME, &err);\n",
    "    checkOpenCL(err, \"clCreateKernel\");\n",
    "}\n",
    "\n",
    "void teardownOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel)\n",
    "{\n",
    "    clReleaseKernel(*kernel);\n",
    "    clReleaseProgram(*program);\n",
    "    clReleaseCommandQueue(*queue);\n",
    "    clReleaseContext(*context);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "fd690e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing include/utils.h\n"
     ]
    }
   ],
   "source": [
    "%%writefile include/utils.h\n",
    "#pragma once\n",
    "#define CL_TARGET_OPENCL_VERSION 300\n",
    "#include <CL/cl.h>\n",
    "\n",
    "char* read_kernel_source(const char* filename);\n",
    "const char* openclGetErrorString(cl_int err);\n",
    "void checkOpenCL(cl_int err, const char *msg);\n",
    "void checkOpenCLBuildLog(cl_int err, cl_program program, cl_device_id device);\n",
    "void setupOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel);\n",
    "void teardownOpenCL(cl_context *context, cl_command_queue *queue, cl_program *program, cl_kernel *kernel);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7f344",
   "metadata": {},
   "source": [
    "### Using the Utility Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "74d308f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel()\n",
    "{\n",
    "    printf(\"Hello World!\\n\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "e93c19f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "int main()\n",
    "{\n",
    "    cl_int err;\n",
    "\n",
    "    // 1. Platform, 2. Device, 3. Context, 4. Command queue, 5. Program, 6. Kernel\n",
    "    cl_context context;\n",
    "    cl_command_queue queue;\n",
    "    cl_program program;\n",
    "    cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    // 7. Launch\n",
    "    size_t global_size = 1;\n",
    "    size_t local_size = 1;\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global_size, &local_size, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "\n",
    "    // 8. Wait and finish\n",
    "    clFinish(queue);\n",
    "\n",
    "    // 9. Cleanup\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "78e4fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d198326",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.4 NDRange (Global Size), Work Groups, Work Items, Devices, CUs, and PEs\n",
    "\n",
    "- In the previous CUDA code, we saw:\n",
    "  - A CUDA **kernel**, where:\n",
    "    - `__global__` is a CUDA-specific qualifier that makes a function a **kernel function**.\n",
    "      - A **kernel function** is executed on the host (GPU), and called from the host (GPU).\n",
    "    - `kernel` is the name of the **kernel function**.\n",
    "    - `parameterlist` is the kernel function's comma-separated list of parameters.\n",
    "\n",
    "      ```c\n",
    "      __global__ kernel(parameterlist)\n",
    "      {\n",
    "\n",
    "      }\n",
    "      ```\n",
    "  - A CUDA **kernel launch**, where:\n",
    "    - `kernel` is the name of the **kernel** function.\n",
    "    - `<<<  >>>` is a special syntax used to **launch** (call) a **kernel function** and contains **launch parameters**.\n",
    "    - `blocks` is a **launch parameter** with the number of **blocks per grid**.\n",
    "    - `threads` is a **launch parameter** with the number of **threads per block**.\n",
    "    - `argumentlist` are the arguments passed to the kernel function (and must match its parameterlist above).\n",
    "  \n",
    "      ```bash\n",
    "      kernel<<<blocks, threads>>>(argumentlist);\n",
    "      ```\n",
    "\n",
    "<img src=\"images/opencl.png\" width=\"500\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "- A CUDA-enabled GPU:\n",
    "  - Is called a `Device`.\n",
    "  - Has a number of `Streaming Multiprocessors (SMs)`.\n",
    "  - Each `SM` has a number of `Streaming Processors (SPs)`.\n",
    "- During a **kernel launch** `kernel<<<blocks, threads>>>(argumentlist)`:\n",
    "  - CUDA lauches a `grid`. \n",
    "  - A `grid` contains a number of `blocks` (specified as `blocks` within `<<<blocks, threads>>>`).\n",
    "  - Each `block` contains a number of `threads` (specified as `threads` within `<<<blocks, threads>>>`).\n",
    "- CUDA maps:\n",
    "  - A `block` to run on an `SM` (multiple `blocks` can be assigned to the same `SM`).\n",
    "  - Each `thread` within a `block` to an `SP`.\n",
    "- So, a `block` runs on an `SM`, and each `thread` within that `block` runs on a `SP` within that `SM`.\n",
    "\n",
    "- For each **kernel launch**:\n",
    "  - CUDA creates a `grid` containing `blocks` containing `threads`.\n",
    "  - Each `thread` runs a copy of the same `kernel` function with the same `argumentlist`.\n",
    "  - 4 CUDA-specific global variables are available within each copy of the `kernel` function:\n",
    "    - `gridDim` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is the number of `blocks` in a `grid`, and can be specified as a 1D (`x`), 2D (`y`), or 3D (`z`) set of `blocks`.\n",
    "    - `blockDim` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is the number of `threads` in a `block`, and can be specified as a 1D (`x`), 2D (`y`), or 3D (`z`) set of `threads`.\n",
    "    - `blockIdx` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is unique ID of a `block` within the `grid`, and has a 1D (`x`), 2D (`y`), and 3D (`z`) ID.\n",
    "    - `threadIdx` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is unique ID of a `thread` within a `block`, and has a 1D (`x`), 2D (`y`), and 3D (`z`) ID.\n",
    "\n",
    "      ```c\n",
    "      typedef struct\n",
    "      {\n",
    "        int x;\n",
    "        int y;\n",
    "        int z;\n",
    "      } dim3;\n",
    "      ```\n",
    "    \n",
    "  - In the figure, we see that each `SM` contains:\n",
    "    - A `register file` (the blue rectangle).\n",
    "      - The `register file` is divided into `**chunks**, where each `thread` (running in the `block` assigned to the `SM`) is assigned one **chunk**.\n",
    "      - Each `thread`'s chunk of the `register file` is referred to as the `thread`'s `private memory` (only that `thread` can access that `private memory`).\n",
    "      - In a `kernel` function, we can declare a local variable with the qualifier `__private__` which stores that variable in a `thread`'s `private memory`.\n",
    "        - If we don't specify a qualifier, the variable is by default stored in the `thread`'s `private memory`.\n",
    "    - A `shared memory` buffer (the green rectangle).\n",
    "      - The `shared memory` is shared by all `threads` running in the `block` assigned to that `SM`.\n",
    "      - To store a variable in `shared memory`, we declare the variable with the `__shared__` qualifier.\n",
    "  - There is also `global memory`, which is declared using the qualifier `__global__`.\n",
    "    - If we don't explicitly qualify a parameter in the kernel function's parameterlist with a qualifier, it is by default `__global__`, i.e. referring to `global memory`.\n",
    "  - Finally, there is `constant memory` stored outside an `SM` (just like `global memory`), and is `read-only` memory (it can we written to once before a kernel launch).\n",
    "    - `constant memory` is small, but very effient (the CUDA compiler can optimize access to it since it is `read-only`).\n",
    "\n",
    "**TL;DR**\n",
    "- A CUDA-enabled GPU is referred to as a `device`, and has an array of `SMs`, each with a number of `SPs`.\n",
    "- A kernel launch specifies the number of `blocks` and `threads` to use within `<<<blocks, threads>>>`.\n",
    "- CUDA maps a `block` to an `SM`.\n",
    "- Each `thread` within a `block` is run on an `SP` within that `SM`.\n",
    "- A CUDA-enabled GPU has `global memory` and `constant memory` located outside of any `SM`.\n",
    "- Each `SM` has `shared memory` (shared by all `threads` running in the `block` assigned to an `SM`).\n",
    "- Each `SM` has a `register file`, divided into chunks, where each `thread` is a assigned a chunk referred to as `private memory` (private to that `thread`).\n",
    "- Each `thread` runs a **copy** of the same `kernel` function with the same `argumentlist`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452d38f",
   "metadata": {},
   "source": [
    "### Code Demonstrating NDRanges, Work Groups, and Work Items\n",
    "\n",
    "- Let's write a CUDA program that demonstrates running a `grid` of `blocks`, each with a number of `threads`, on the device (GPU).\n",
    "- The program copies the elements from one 1D `int` array `input` to another 1D `int` array `output`.\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`\n",
    "    - `stdlib.h` for `malloc` and `free`\n",
    "    - `time.h` for `srand` and `rand`\n",
    "    - `cuda_runtime.h` for `cudaMalloc`, `cudaMemcpy`, and `cudaFree`\n",
    "    \n",
    "    ```c\n",
    "    #include <stdio.h>\n",
    "    #include <stdlib.h>\n",
    "    #include <time.h>\n",
    "    #include <cuda_runtime.h>\n",
    "    ```\n",
    "  \n",
    "  - We define symbolic constants:\n",
    "    - `N` with the value `5` for the number of elements in each array\n",
    "    - `THREADS_PER_BLOCK_X` with the value `2` for the number of `threads` in each `block`\n",
    "\n",
    "    ```c\n",
    "    #define N 5\n",
    "    #define THREADS_PER_BLOCK_X 2\n",
    "    ```\n",
    "  \n",
    "  - We define a kernel function called `kernel`.\n",
    "    - It has the `__global__` qualifier, making it a kernel function, returns `void`, and has three parameters:\n",
    "      - An `int *` pointer `input` which points to the `input` array in the device's `global memory`.\n",
    "      - An `int *` pointer `output` which points to the `output` array in the device's `global memory`.\n",
    "      - An `int` variable `n` with the number of elements in each array.\n",
    "    - In the function's body, we:\n",
    "      - Use the CUDA-specific global variables `gridDim`, `blockDim`, `blockIdx`, and `threadIdx`, all of type `dim3` (with `int` member variables `x`, `y`, `z`).\n",
    "        - CUDA sets these as global variables, behind the scenes, during a kernel launch.\n",
    "        - They are available to each thread within each copy of the kernel function.\n",
    "        - `gridDim.x` is the number of `blocks` in a `grid`.\n",
    "          - In our case we have `(5 + 2 - 1) / 2 = 3` since we calculate it as `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X` in the `main` function.\n",
    "        - `blockDim.x` is the number of `threads` in a `block`.\n",
    "          - In our case we have `2`, defined by `THREADS_PER_BLOCK_X`.\n",
    "        - `blockIdx.x` is the unique `ID` of a `block` within the `grid`.\n",
    "          - Since `gridDim.x` is `3`, `blockIdx.x` ranges from `0` to `2` (zero-based, i.e. from `0` to `gridDim.x - 1`).\n",
    "        - `threadIdx.x` is the unique `ID` of a `thread` within a `block`.\n",
    "          - Since `blockDim.x` is `2`, `threadIdx.x` ranges from `0` to `1` (zero-based, i.e. from `0` to `blockDim.x.x - 1`).\n",
    "      - We calculate the global ID for a `thread` as `int idx = threadIdx.x + blockIdx.x * blockDim.x`.\n",
    "        - We have `gridDim.x * blockDim.x = 3 * 2 = 6` threads in total, so `idx` ranges from `0` to `5` (zero-based).\n",
    "      - We print out the values of `gridDim.x`, `blockDim.x`, `blockIdx.x`, `threadIdx.x`, and `idx`.\n",
    "        - This shows us which `thread` is running this specific copy of the kernel function, which `block` it is in, etc.\n",
    "      - We have a `boundary guard`, i.e. `if(idx >= n) return;`\n",
    "        - The ensures we don't index into an array with `idx` if `idx` is out of bounds.\n",
    "        - We have `6` threads in total, and `5` (defined by `N`) elements in each array, so `idx = 5` is out of bounds.\n",
    "      - Finally, we copy one element from the `input` array into the `output` using `idx` as the index.\n",
    "        - Remember, each thread runs its own `copy` of the kernel function (in parallel), each with the same set of kernel function `arguments`.\n",
    "\n",
    "    ```c\n",
    "    __global__ void kernel(int *input, int *output, int n)\n",
    "    {       \n",
    "        int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "        \n",
    "        printf(\"gridDim.x = %d, blockDim.x = %d, blockIdx.x = %d, threadIdx.x = %d, idx = %d\\n\", gridDim.x, blockDim.x, blockIdx.x, threadIdx.x, idx);\n",
    "        \n",
    "        if(idx >= n)\n",
    "        {\n",
    "            printf(\"Boundary checking avoided indexing outside of the arrays [idx = %d]\\n\", idx);\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        output[idx] = input[idx];\n",
    "    }\n",
    "    ```\n",
    "- In the `main()` function:\n",
    "  - We seed the pseudorandom number generator with the value `0` so the random numers we create will be the same every time we run the program.\n",
    "\n",
    "    ```c\n",
    "    srand(0);\n",
    "    ```\n",
    "\n",
    "  - We declare:\n",
    "    - `int` pointer variables `h_input` and `h_output` for the two arrays, which will point to heap memory (RAM) on the host (CPU).\n",
    "    - `int` pointer variables `d_input` and `d_output` for the two arrays, which will point to global memory on the device (GPU).\n",
    "    - `int` variable `data_size` and initialize it to `N * sizeof(int)`, i.e. the total size of each array in bytes (with `N` elements of type `int` in each).\n",
    "    \n",
    "    ```c\n",
    "    int *h_input, *h_output;\n",
    "    int *d_input, *d_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "    ```\n",
    "  - We allocate memory on the host (GPU) with `malloc`, storing the pointers to the memory in variables `h_input` and `h_output`.\n",
    "\n",
    "    ```c\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    ```\n",
    "  - We allocate memory on the device (GPU) with `cudaMalloc` storing the pointers to the memory in variables `d_input` and `d_output`.\n",
    "\n",
    "    ```c\n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "    ```\n",
    "  - We initialize the `h_input` array on the host (CPU) with random values using the `rand()` function.\n",
    "\n",
    "    ```c\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100; // random integers between 0 and 99\n",
    "    }\n",
    "    ```\n",
    "  - We copy the elements of both arrays stored in host (GPU) memory (RAM) to device (GPU) global memory with `cudaMemcpy`.\n",
    "    - Its first argument is a pointer to the memory to `copy to`.\n",
    "    - Its second argument is a pointer to the memory to `copy from`.\n",
    "    - Its third argument is the `size (in bytes)` of memory to copy (all `N` elements in our case).\n",
    "    - Its fourth argument is a symbolic constant which determines the direction of the copy operation.\n",
    "      - `cudaMemcpyHostToDevice` copies memory from the host (CPU) to the device (GPU).\n",
    "      - `cudaMemcpyDeviceToHost` copies memory from the device (GPU) to the host (CPU).\n",
    "    - Here we are copying the `input` and `output` arrays from the host (`h_input`, `h_output`) to the device (`d_input`, `d_output`).\n",
    "\n",
    "    ```c\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice);\n",
    "    ```\n",
    "  - We are now using CUDA's `dim3` struct to define the `gridDim` (number of blocks) and `blockDim` (number of threads in each block).\n",
    "    - Before we just used `int` litterals in the launch configuration `<<<1, 1>>>`, but we can also use `dim3` variables `<<<gridDim, blockDim>>>`.\n",
    "    - The launch configuration supports launching `1D`, `2D`, and `3D` blocks and threads, depending on the problem we want to solve, e.g:\n",
    "      - For a `1D` problem such as copying elements between two 1D arrays, we only need to use 1 dimension, which is the `x` member variable in `dim3` structs.\n",
    "      - For a `2D` problem such as filtering a 2D image, we might need to use 2 dimensions, which are the `x` and `y` member variables in `dim3` structs.\n",
    "      - For a `3D` problem such as filtering a 3D MRI-scan volume, we might need to use 3 dimensions, which are the `x`, `y` and `z` member variables in `dim3` structs.\n",
    "      - For our 1D problem, we are only using the `x` member variable, which means the other dimensions `y` and `z` will be set to the value `1`.\n",
    "    - We create a `dim3` variable `blockDim` and initialize its `x` member variable to `THREADS_PER_BLOCK_X` (member variables `y` and `z` will be set to `1`).\n",
    "      - This means we have `2` threads per block since `THREADS_PER_BLOCK_X` is defined with the value `2`.\n",
    "    - We create a `dim3` variable `gridDim` and initialize its `x` member variable to `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X`.\n",
    "      - This means we have `3` blocks per grid since `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X = (5 + 2 -1) / 2 = 3` (and there is only ever 1 grid).\n",
    "      - This (standard) construct is commonly used to ensure enough `threads` are launched to solve a problem (but can launch more `threads` than data elements).\n",
    "\n",
    "    ```c\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "    ```\n",
    "  - Next, we launch the kernel with:\n",
    "    - Launch configuration `<<<gridDim, blockDim>>>`, where `gridDim` and `blockDim` are our two `dim3` variables.\n",
    "    - Argument list `d_input, d_output, N`  `d_input`, where `d_input` and `d_output` are the `int` pointers to the arrays on the device (GPU), and `N` the number of elements.\n",
    "\n",
    "    ```c\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "    ```\n",
    "  - We copy the elements in the `d_output` array on the device (GPU) back to the array `h_output` on the host (CPU) using `cudaMemcpy`.\n",
    "    - Notice the final argument is now `cudaMemcpyDeviceToHost`, i.e. the direction of the copy operation is from the device (GPU) to the host (CPU).\n",
    "\n",
    "    ```c\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);   \n",
    "    ```\n",
    "  - Then we print out the elements in the two arrays `h_input` and `h_output` on the host (CPU).\n",
    "   \n",
    "    ```c\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    ```\n",
    "  - Finally, we free the mmeory allocated for the arrays:\n",
    "    - We free the `int` pointers (`d_input` and `d_output`) pointing to memory on the device (GPU) with `cudaFree`.\n",
    "    - We free the `int` pointers (`h_input` and `h_output`) pointing to memory on the host (CPU) with `free`.\n",
    "    - Both functions take a pointer to the memory \n",
    "    - Notice the naming convention used in this program for pointers to memory on the host (`h_` prefix) and the device (`d_` prefix).\n",
    "\n",
    "    ```c\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    ```\n",
    "- Run the cell below to see the output from the program.\n",
    "\n",
    "**TL;DR**\n",
    "- A kernel launch `kernel<<<blocks, threads>>>(argumentlist)` has:\n",
    "  - A launch configuration `<<<gridDim, blockDim>>>` that specifies how man `blocks` (`gridDim`) and `threads` per `block` (`blockDim`) to launch.\n",
    "    - It accepts `int` parameters, e.g. `<<<3, 2>>>` or `dim3` parameters, e.g. `<<<gridDim, blockDim>>>`.\n",
    "    - `dim3` is a struct containing `int` member variables `x`, `y`, and `z`, used to structure `blocks` and `threads` for `1D`, `2D`, or `3D` problems.\n",
    "  - An argument list `argumentlist` which must match the kernel function's parameter list.\n",
    "    - Each `thread` runs a `copy` of the same kernel function, with the exact same `argumentlist`, in parallel (at the same time).\n",
    "- A kernel function is run for each `thread`, where each `thread` has access to 4 global `dim3` variables `gridDim`, `blockDim`, `blockIdx`, and `threadIdx`\n",
    "  - `gridDim` and `blockDim` are from the launch configuration and contain the number of `blocks` (`gridDim`) and number of `threads` per `block` (`blockDim`).\n",
    "  - `blockIdx` and `threadIdx` contain unique `block` IDs within a grid (`blockIdx`) and unique `thread` IDs within a `block` (`threadIdx`).\n",
    "- The construct `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X`:\n",
    "  - Is commonly used ensure enough (at least as many) `threads` are launched needed to solve a problem (cover all data elements).\n",
    "  - But can launch more `threads` than the total number of data elements.\n",
    "- Since we can have more `threads` than data elements, we **always use bounday guards in CUDA kernels** to avoid out-of-bounds indexing.\n",
    "- We use `malloc` and `free` for managing memory on the host (CPU).\n",
    "- We use `cudaMalloc` and `cudaFree` for managing memory on the device (GPU).\n",
    "- We use `cudaMemcpy` to copy memory between the host (CPU) and device (GPU), where the fourth argument determines the direction of the copy operation.\n",
    "  - `cudaMemcpyHostToDevice` copies memory from the `host` (CPU) to the `device` (GPU).\n",
    "  - `cudaMemcpyDeviceToHost` copies memory from the `device` (GPU) to the `host` (CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "b8cf26c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(__global const int *input, __global int *output, int n)\n",
    "{\n",
    "    int num_work_groups = get_num_groups(0); // equivalent to CUDA's gridDim.x\n",
    "    int work_group_size = get_local_size(0); // equivalent to CUDA's blockDim.x\n",
    "    int work_group_id = get_group_id(0);     // equivalent to CUDA's blockIdx.x\n",
    "    int work_item_id = get_local_id(0);      // equivalent to CUDA's threadIdx.x\n",
    "    int idx = get_global_id(0);              // equivalent to CUDA's threadIdx.x + blockIdx.x * blockDim.x\n",
    "\n",
    "    printf(\"num_work_groups = %d, work_group_size = %d, work_group_id = %d, work_item_id = %d, idx = %d\\n\", num_work_groups, work_group_size, work_group_id, work_item_id, idx);\n",
    "\n",
    "    if (idx >= n) {\n",
    "        printf(\"Boundary checking avoided indexing outside of the arrays [idx = %d]\\n\", idx);\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    output[idx] = input[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "f579afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, N * sizeof(int), h_input, &err); // CL_MEM_COPY_HOST_PTR copies h_input values to device buffer\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, N * sizeof(int), NULL, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "\n",
    "    // Read result back (CL_TRUE makes this a synchronous (blocking) call)\n",
    "    err = clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, N * sizeof(int), h_output, 0, NULL, NULL);\n",
    "\n",
    "    // Wait for all queued operations to finish (not really needed here because of CL_TRUE in clEnqueueReadBuffer above)\n",
    "    err = clFinish(queue);\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    err = clReleaseMemObject(d_input);\n",
    "    err = clReleaseMemObject(d_output);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "4dc52b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_work_groups = 3, work_group_size = 2, work_group_id = 2, work_item_id = 0, idx = 4\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 2, work_item_id = 1, idx = 5\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 1, work_item_id = 0, idx = 2\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 1, work_item_id = 1, idx = 3\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 0, work_item_id = 0, idx = 0\n",
      "num_work_groups = 3, work_group_size = 2, work_group_id = 0, work_item_id = 1, idx = 1\n",
      "Boundary checking avoided indexing outside of the arrays [idx = 5]\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32a341",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see:\n",
    "  - `gridDim.x` is `3`, i.e. there a `3` `blocks` in the grid.\n",
    "  - `blockDim.x` is `2`, i.e. there a `2` `threads` in each `block`.\n",
    "  - `blockIdx.x` varies from `0` to `2`, i.e. from `0` to `gridDim.x - 1`, and is a `block`'s unique ID (i.e. unique within a kernel launch).\n",
    "  - `threadIdx.x` varies from `0` to `1`, i.e. from `0` to `blockDim.x - 1`, and is a `thread`'s unique block ID (i.e. unique within a block).\n",
    "  - `idx` varies from `0` to `5`, i.e. from `0` to `gridDim.x * blockDim.x`, and is a `thread`'s unique global ID (i.e. unique within a kernel launch).\n",
    "  - The boundary guard was triggered for one thread, i.e. the thread with `idx = 5`, because we only have `N = 5` elements in each array.\n",
    "    - So, we can have more threads running than elements in our data/arrays, why **we should always make use of boundary guards in our CUDA kernels**.\n",
    "  - The `input` and `output` arrays have the same element values, so our CUDA kernel's logic is functionally correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea72c64",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.5 Error Checking\n",
    "\n",
    "- CUDA supports checking for errors in device (GPU) code and from calling any CUDA function.\n",
    "- The `#include`s, `#define`s, and the kernel function are the same as in the previous example.\n",
    "  - `stdlib.h` also includes the function prototype for `exit` and the symbolic constant `EXIT_FAILURE` used in this example.\n",
    "  - `cuda_runtime` also includes:\n",
    "    - The the function prototypes for `cudaMalloc`, `cudaFree`, and `cudaMemcpy`, since we are back to NOT using `unified memory`.\n",
    "    - The function prototypes for `cudaGetLastError` and `cudaGetErrorString`, which we use for checking errors from CUDA function calls.\n",
    "    - The typedef `cudaError_t` and symbolic constant `cudaSuccess`, also used for checking CUDA errors.\n",
    "- The only modifications in the `main()` function are:\n",
    "  - We are using the code from the example we used before the `unified memory` example, where we don't use `unified memory`.\n",
    "  - We have wrapped all CUDA function calls as arguments in a function called `checkCuda()`, explained below, e.g.\n",
    "\n",
    "    ```c\n",
    "    checkCuda(cudaMalloc((void **)&d_input, data_size), \"cudaMalloc\");\n",
    "    ```\n",
    "  - After the kernel launch, we use the code below to check for CUDA errors in the kernel function.\n",
    "\n",
    "    ```c\n",
    "    checkCuda(cudaGetLastError(), \"kernel\");\n",
    "    ```\n",
    "  - Then, after freeing all allocated memory, we deliberately produce an error:\n",
    "    - We call `cudaFree()` on the `d_output` pointer twice, which produces an error the second time since that memory has already been freed.\n",
    "      - All CUDA functions return a value of type `cudaError_t` which can be checked to see if an error occured in CUDA code on the device (GPU).\n",
    "      - The only CUDA operation that doesn't have a return value of type `cudaError_t` is the kernel launch.\n",
    "      - For that reason, CUDA provides the function `cudaGetLastError()` which will return the latest `cudaError_t` (we can use it after any CUDA function call). \n",
    "    ```c\n",
    "    checkCuda(cudaFree(d_output), \"cudaFree\");\n",
    "    ```\n",
    "- We have wrapped all CUDA function calls, returning a value of type `cudaError_t`, in the function `checkCuda()`.\n",
    "  - We pass the `cudaError_t` value as the first argument, and a string message as the second (the wrapped function name has been used).\n",
    "- The function `checkCuda()` is defined by ourselves (it's been placed between the kernel function and the `main()` function the sample code):\n",
    "  - It takes a CUDA error (`cudaError_t`) as its first argument and a message (string) as its second argument, returning `void`.\n",
    "  - It checks if the value of the `cudaError_t` error is different from the symbolic constant `cudaSuccess` (`cudaSuccess` means there is no error).\n",
    "    - If so, it retrieves a string-representation of the error by calling the function `cudaGetErrorString()`, which takes the error as an argument.\n",
    "    - Then the error string is printed out together with an optional message (second argument to `checkCuda()`).\n",
    "    - Finally, it terminates the program by calling the `exit()` function, passing in the symbolic constant `EXIT_FAILURE` as the return value to the operating system.\n",
    "  - We can use `checkCuda()` by wrapping it around any CUDA function call, e.g. `checkCuda(cudaFree(d_output), \"cudaFree\")`.\n",
    "    - Error checking will not be used going forward in this notebook to make the examples clearer, but good practice is to check for errors after each CUDA function call.\n",
    "\n",
    "    ```c\n",
    "    void checkCuda(cudaError_t err, const char *msg)\n",
    "    {\n",
    "        if (err != cudaSuccess)\n",
    "        {\n",
    "            printf(\"Error: %s (%s)\\n\", msg, cudaGetErrorString(err));\n",
    "            exit(EXIT_FAILURE);\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "- Run the cell below to see the output (it will be the same as before, except for the error message that we deliberately produced).\n",
    "\n",
    "**TL:DR**\n",
    "\n",
    "- Each CUDA function returns a value of type `cudaError_t`.\n",
    "  - If it's value is different from the symbolic constant `cudaSuccess`, and error occurred.\n",
    "  - We can retrieve a string representation of a CUDA error by passing a `cudaError_t` instance as an argument the the function `cudaGetErrorString`.\n",
    "  - We can retrieve the last CUDA error after a CUDA function call, including the kernel launch, using the function `cudaGetLastError`.\n",
    "- We can exit from a C program prematurely, by calling the C function `exit`, passing in an exit code to the operating system.\n",
    "  - The symbolic constant `EXIT_FAILURE` can be used as an exit code, representing a general error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "fd6bc875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(__global const int *input, __global int *output, int n)\n",
    "{\n",
    "    int idx = get_global_id(0);\n",
    "    output[idx] = input[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "2ed14d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    \n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, N * sizeof(int), h_input, &err); // CL_MEM_COPY_HOST_PTR copies h_input values to device buffer\n",
    "    checkOpenCL(err, \"clCreateBuffer\");\n",
    "    \n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, N * sizeof(int), NULL, &err);\n",
    "    checkOpenCL(err, \"clCreateBuffer\");\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    checkOpenCL(err, \"clSetKernelArg\");\n",
    "    \n",
    "    err = clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    checkOpenCL(err, \"clSetKernelArg\");\n",
    "    \n",
    "    err = clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "    checkOpenCL(err, \"clSetKernelArg\");\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueNDRangeKernel\");\n",
    "\n",
    "    // Read result back (CL_TRUE makes this a synchronous (blocking) call)\n",
    "    err = clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, N * sizeof(int), h_output, 0, NULL, NULL);\n",
    "    checkOpenCL(err, \"clEnqueueReadBuffer\");\n",
    "\n",
    "    // Wait for all queued operations to finish (not really needed here because of CL_TRUE in clEnqueueReadBuffer above)\n",
    "    err = clFinish(queue);\n",
    "    checkOpenCL(err, \"clFinish\");\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "\n",
    "    err = clReleaseMemObject(d_input);\n",
    "    checkOpenCL(err, \"clReleaseMemObject\");\n",
    "    \n",
    "    err = clReleaseMemObject(d_output);\n",
    "    checkOpenCL(err, \"clReleaseMemObject\");\n",
    "\n",
    "    err = clSetKernelArg(kernel, 99, sizeof(cl_mem), &d_input);        // Intentionally set a kernel argument with invalid arg index\n",
    "    checkOpenCL(err, \"Intentional clSetKernelArg with invalid index\");\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "c3b45741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "Error: Intentional clSetKernelArg with invalid index (CL_INVALID_ARG_INDEX)\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eba3cb",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see that the result is the same as before (the only difference is that we are NOT using `unified memory`).\n",
    "- We also see the error message `invalid argument` returned from `cudaFree()` when we try to deallocate device (GPU) memory that has already been freed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276810cd",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.6 Measuring Execution Time on the Host (CPU) and on the Device (GPU)\n",
    "\n",
    "- A common workflow is to first implement an algorithm in a function on the host (CPU), and then in a kernel on the device (GPU).\n",
    "  - The CPU version can act as a baseline benchmark for GPU kernel performance.\n",
    "  - The CPU version can be used to verify the results of a GPU kernel.\n",
    "  - For inexperienced manycore programmers, it's often easier to start with a CPU version, and then convert it into a GPU version.\n",
    "- Let's use the same code as before, but instrument it with timing code, wrapped around the CPU function call and around the GPU kernel launch.\n",
    "- The imported header files are the same as before:\n",
    "  - `stdlib.h` also contains the function prototype for `clock`, the `clock_t` typedef, and the symbolic constant `CLOCKS_PER_SEC`.\n",
    "    - `clock()` is a parameterless function returning a value of type `clock_t`.\n",
    "    - `clock_t` contains the number of `ticks` elapsed since the program started.\n",
    "    - `CLOCKS_PER_SEC` is defined as the number of `ticks` in a second (`ticks / CLOCKS_PER_SEC * 1000.0` converts `ticks` to milliseconds).\n",
    "  - `cuda_runtime.h` contains a typedef `cudaEvent_t` and prototypes `cudaEventCreate`, `cudaEventRecord`, `cudaEventElapsedTime`, `cudaEventSynchronize`, and `cudaEventDestroy`.\n",
    "    - `cudaEvent_t` represent a CUDA event, e.g. `cudaEvent_t start` (we won't explore CUDA events (or CUDA streams) in detail in this notebook).\n",
    "    - `cudaEventCreate` is used to initialize a CUDA event, e.g. `cudaEventCreate(&start)`\n",
    "    - `cudaEventRecord` is used to start recording (monitoring) a CUDA event, e.g. `cudaEventRecord(start)`\n",
    "    - `cudaEventElapsedTime` is used to compute and return the elapsed time in milliseconds between to CUDA events, e.g. `cudaEventElapsedTime(&elapsed_ms, start, stop)`\n",
    "    - `cudaEventSynchronize` blocks the CPU's main thread until an event has completed (in our code, when the kernel is done), e.g. `cudaEventSynchronize(stop)`\n",
    "    - `cudaEventDestroy` frees (destroys) a CUDA event, e.g. `cudaEventDestroy(start)`\n",
    "    \n",
    "- We define a host (CPU) function `copy()`, equivalent to the device (GPU) kernel function `kernel()`\n",
    "  - The GPU kernel function is the same as before.\n",
    "    \n",
    "    ```c\n",
    "    void copy(int *input, int *output, int n)\n",
    "    {\n",
    "        for(int idx = 0; idx < n; idx++)\n",
    "        {\n",
    "            output[idx] = input[idx];\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "- In the `main()` function:\n",
    "  - We wrap the code below around the device (GPU) kernel launch `kernel()`.\n",
    "    - First we declare two CUDA event variables `start` and `stop`, and initialize them with `cudaEventCreate(&start)` and `cudaEventCreate(&stop)`.\n",
    "    - Then we record the `start` event with `cudaEventRecord(start)` (this records the current time in the kernel and stores it in the `start` event).\n",
    "    - Next, the device (GPU) kernel is launched as usual.\n",
    "    - Then we record the `stop` event with `cudaEventRecord(stop)` (this records the current time in the kernel and stores it in the `stop` event).\n",
    "    - We call `cudaEventSynchronize(stop)` to block the host (CPU) main thread until the `stop`event is done (i.e. until the kernel is done).\n",
    "    - Finally, we declare a variable `float gpu_elapsed_ms` and pass it to the function `cudaEventElapsedTime(&gpu_elapsed_ms, start, stop);`\n",
    "      - We also pass in `start`and `stop`, where the function will store the elapsed time in milliseonds in the variable `gpu_elapsed_ms`.\n",
    "\n",
    "    ```c\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the device (GPU) kernel execution time\n",
    "    // --------------------------------------------------------------\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start);\n",
    "  \n",
    "    // Device kernel() launch\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "  \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float gpu_elapsed_ms;\n",
    "    cudaEventElapsedTime(&gpu_elapsed_ms, start, stop);\n",
    "    // --------------------------------------------------------------\n",
    "    ```\n",
    "  - We wrap the code below around the host (CPU) function call `copy()`.\n",
    "    - We call the function `clock()` to record the current number of `ticks` since the program started, and store the result in a variable `cpu_start` of type `clock_t`.\n",
    "    - Then we call the host (CPU) function `copy()`.\n",
    "    - Next, we call the function `clock()` again to record the current number of `ticks` again and store the result in a variable `cpu_stop` of type `clock_t`.\n",
    "    - Finally, we calculate the elapsed number of milliseconds as `float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0`.\n",
    "  \n",
    "    ```c\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the host (CPU) function execution time\n",
    "    // --------------------------------------------------------------\n",
    "    clock_t cpu_start = clock();\n",
    "\n",
    "    // Host function call\n",
    "    copy(h_input, h_output_cpu, N);\n",
    "    \n",
    "    clock_t cpu_stop = clock();\n",
    "    float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    // --------------------------------------------------------------\n",
    "    ```\n",
    "  - We print out the execution time for the device (GPU) kernel and host (GPU) function.\n",
    "\n",
    "    ```c\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "    ```\n",
    "  - We print out the execution time for the device (GPU) kernel and host (GPU) function.\n",
    "\n",
    "    ```c\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "    ```\n",
    "  - We use a separate `int` pointer variable `h_output_cpu` for storing the output from the host (CPU) function call.\n",
    "    - We verify the output results from the device (GPU) kernel and host (CPU) function are the same.\n",
    "    - This is a common best practice when verifying the correct functionality of an algorithm implemented in a device (GPU) kernel.\n",
    "      - We use the `abs()` function to compute the absolute difference between each eleement pair in the two arrays.\n",
    "      - If we were using `float`s instead of `int`s, we can use the `fabs()` function and compare the difference to e.g. `1e-5`.\n",
    "\n",
    "    ```c\n",
    "    int errorsum = 0;\n",
    "    for (int i = 0; i < N; i++)\n",
    "    {\n",
    "        int error = abs(h_output[i] - h_output_cpu[i]);\n",
    "        if (error > 0)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "    // Print verification result\n",
    "    printf(\"\\nVerification : %s\\n\", (errorsum > 0) ? \"FAILED\" : \"PASSED\");\n",
    "    ```\n",
    "  - We also print out the two arrays as before (same code) after launching the device (GPU) kernel and after calling the host (CPU) function.\n",
    "  - Lastly, we also free all memory, including the two CUDA events.\n",
    "\n",
    "    ```c\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    ```\n",
    "- Run the cell below to see the output.\n",
    "  - We won't record time in this notebook going forward to make the example code clearer, but now you know how to do it yourself-\n",
    "\n",
    "**TL:DR**\n",
    "\n",
    "- We can measure the execution time for a CUDA kernel with types and function prototypes declared in `cuda_runtime.h`.\n",
    "- We can measure the execution time for a C function (or any C code) with types and function prototypes declared in `stdlib.h`.\n",
    "- We can cmopute the absolute difference between two results to determine if they are correct (given at least one of the results is correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "9df73918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(__global const int *input, __global int *output, int n)\n",
    "{\n",
    "    int idx = get_global_id(0);\n",
    "    output[idx] = input[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "299d010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "// Host function\n",
    "void copy(int *input, int *output, int n)\n",
    "{\n",
    "    for(int idx = 0; idx < n; idx++)\n",
    "    {\n",
    "        output[idx] = input[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output, *h_output_cpu;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    h_output_cpu = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, N * sizeof(int), h_input, &err); // CL_MEM_COPY_HOST_PTR copies h_input values to device buffer\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, N * sizeof(int), NULL, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the device (GPU) kernel execution time\n",
    "    // --------------------------------------------------------------\n",
    "    // Enqueue kernel with event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, &kernel_event);\n",
    "\n",
    "    // Wait for the kernel to finish\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "\n",
    "    // Query profiling info\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double gpu_elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "    // --------------------------------------------------------------\n",
    "\n",
    "    // Read result back (CL_TRUE makes this a synchronous (blocking) call)\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, N * sizeof(int), h_output, 0, NULL, NULL);\n",
    "    \n",
    "    // Print measured device kernel execution time\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the host (CPU) function execution time\n",
    "    // --------------------------------------------------------------\n",
    "    clock_t cpu_start = clock();\n",
    "\n",
    "    // Host function call\n",
    "    copy(h_input, h_output_cpu, N);\n",
    "    \n",
    "    clock_t cpu_stop = clock();\n",
    "    float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    // --------------------------------------------------------------\n",
    "\n",
    "    // Print measured host function execution time\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output_cpu[i]);\n",
    "    }\n",
    "\n",
    "    // Verify the results in the GPU output with the CPU output\n",
    "    int errorsum = 0;\n",
    "    for (int i = 0; i < N; i++)\n",
    "    {\n",
    "        int error = abs(h_output[i] - h_output_cpu[i]);\n",
    "        if (error > 0)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Print verification result\n",
    "    printf(\"\\nVerification : %s\\n\", (errorsum > 0) ? \"FAILED\" : \"PASSED\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_output_cpu);\n",
    "\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseEvent(kernel_event); // Release the event\n",
    "\n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "db25af84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU execution time  : 0.010240 ms\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "CPU execution time  : 0.001000 ms\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "Verification : PASSED\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad70c7",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see that the execution time on the GPU is slower than on the CPU.\n",
    "- This is expected since copying 5 elements from one array to another is just a waste of time on a GPU.\n",
    "  - **Not all problems are suitable for a GPU, in which case we should use the CPU instead**.\n",
    "- We also see the results verification `PASSED` so we can rest assured that the kernel function is correct (if the CPU function correct, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae5f59",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.7 Shared Memory and Thread Synchronization on the Device (GPU)\n",
    "\n",
    "- `Shared memory` is a fast, low-latency memory located on-chip, accessible by all `threads` in a `block`.\n",
    "  - **Location**: On-chip, accessible by all `threads` in a `block`.\n",
    "  - **Access**: Readable and writeable by all `threads` in a `block` (also `writable` from the `host` before kernel launch).\n",
    "  - **Size limit**: Typically `48 KB` per `SM` (Streaming Multiprocessor).\n",
    "  - **Speed**: Very fast, much faster than `global memory`.\n",
    "  - **Scope**: Shared only among `threads` in the same `block`.\n",
    "  - **Lifetime**: Exists for the duration of the `block`.\n",
    "- Use `shared memory` when:\n",
    "  - Threads need to cooperate, such as tiling, caching, or communication between `threads`.\n",
    "    - `Shared memory` is specified within a kernel function with the qualifier `__shared__`.\n",
    "    - It can be initialized `statically` or `dynamically`.\n",
    "- `Thread synchronization` is used to synchronize `threads`, especially `threads`in a `block` when using `shared memory`:\n",
    "  - Purpose: Barrier synchronization — all `threads` in the `block` must reach it before any can proceed.\n",
    "    - A barrier (thread synchronization) is done with the command `__syncthreads()` in kernel function.\n",
    "  - Ensures all `shared memory` reads/writes are complete before continuing.\n",
    "  - Used to prevent `race conditions`.\n",
    "- Issues to be aware of when using `shared memory`:\n",
    "  - `Non-coalesced memory access` also applies when accessing `global memory` (actually more important in that case).\n",
    "  - `Wavefront (thread) divergence` applies to `threads` within the same `wavefront` (a `wavefront` is a group of `32` threads that are scheduled to run on `SPs` within a `SM`/`block`).\n",
    "  - `Low occupancy` is related to `shared memory` but applies more generally to a `kernel launch`.\n",
    "\n",
    "  | Issue                       | Consequence                        | Fix                                |\n",
    "  | --------------------------- | ---------------------------------- | ---------------------------------- |\n",
    "  | Race conditions             | Wrong results                      | Use `__syncthreads()` or atomics   |\n",
    "  | No synchronization          | Inconsistent reads/writes          | Use `__syncthreads()`              |\n",
    "  | [Bank conflicts](https://www.youtube.com/watch?v=CZgM3DEBplE)              | Performance slowdown               | Pad arrays, restructure access     |\n",
    "  | Exceeding memory limit      | Kernel launch fails or runs slower | Reduce usage, use fewer threads    |\n",
    "  | Wrong indexing              | Wrong data or crash                | Use `threadIdx` properly           |\n",
    "  | Uninitialized/out-of-bounds | Undefined behavior                 | Always initialize and guard bounds |\n",
    "  | [Non-coalesced memory access](https://www.youtube.com/watch?v=mLxZyWOI340&list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2&index=97)| Slower execution speed | Coalesce memory access |\n",
    "  | [Wavefront (thread) divergence](https://www.youtube.com/watch?v=bHkFV-YMxxY&list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2&index=106) | Slower execution speed | Avoid branches and loops |\n",
    "  | [Low occupancy](https://www.youtube.com/watch?v=2NGQvnT_3gU) | Slower execution speed | Increase occupancy |  \n",
    "\n",
    "<br />  \n",
    "\n",
    "- Now, let's look at a simple example of using `shared memory`.\n",
    "- The code is the same as before, but with the following modifications:\n",
    "  - In the `kernel` function, we declare a buffer (array) with the `__shared__` qualifier.\n",
    "  - The `shared memory` can be declared with a `static` size or with a `dynamic size`.\n",
    "  - In the sample code, we are using a `dynamic size`, where\n",
    "    - the size isn't provided within the square brackets `[]`\n",
    "    - the keyword `extern` is used infront of the `__static__` qualifier\n",
    "      - this means the size id declared elsewhere (as an additional launch configuration parameter)\n",
    "  - If we wanted a `static` size, we could use the commented-out row below instead, where\n",
    "    - the sizs is provided within the square brackets `[THREADS_PER_BLOCK_X]` (`THREADS_PER_BLOCK_X` in this case).\n",
    "\n",
    "  ```c\n",
    "  extern __shared__ int shared[];               // dynamic size\n",
    "  //__shared__ int shared[THREADS_PER_BLOCK_X]; // static size\n",
    "  ```\n",
    "- Let's look at the complete kernel function:\n",
    "  - At the top, we decalare `shared memory` with a dynamic size.\n",
    "  - Then we calculate a `thread`'s global index/ID (`g_idx`) and a `thread`'s local/shared index/ID (`s_idx`).\n",
    "    - We have to be careful in how we use the threads for indexing (`g_idx` is unique within a kernel launch, `s_id` is unique within a `block` on the same `SM`).\n",
    "    - Remember, if we have `blockDim.x` `threads` per `block` (with a `s_id` ranging from `0` to `blockDim.x` - 1).\n",
    "  - Our usual `boundary guard` comes next `if(g_idx >0 n)`.\n",
    "  - Then we copy elements from the `input` array into `shared` memory.\n",
    "    - The index into the `input` array is `g_idx`.\n",
    "    - The index into the `shared` array is `s_idx`.\n",
    "    - Different indexing schemes might ne necessary depending on the problem/algorithm.\n",
    "  - Next, we have a thread barrier `__syncthreads()`.\n",
    "    - This ensures no `thread` within the `block` can continue past this row until all `threads` in the `block` have completed the code above this row.\n",
    "      - This is important, since some `threads`might not have copied their element from the `input` array into the `shared` array yet.\n",
    "      - In this example, it isn't an issue, because no other `thread` will read another `thread`'s element in the `shared` array in the code below the barrier `__syncthreads`.\n",
    "      - For other problems, this might not be the case, so if `threads` aren't synchronized, they might continue and read stale data from the `shared` array.\n",
    "  - Lastly, when all `threads` are synchronized, a `thread` copies an element from the `shared` array into the `output` array.\n",
    "    - The index into the `output` array is `g_idx`.\n",
    "    - The index into the `shared` array is `s_idx`.\n",
    "    - Different indexing schemes might ne necessary depending on the problem/algorithm.\n",
    "\n",
    "  ```C\n",
    "  __global__ void kernel(int *input, int *output, int n)\n",
    "  {\n",
    "      // Shared memory\n",
    "      extern __shared__ int shared[];               // dynamic size\n",
    "      //__shared__ int shared[THREADS_PER_BLOCK_X]; // static size\n",
    "      \n",
    "      int g_idx = threadIdx.x + blockIdx.x * blockDim.x; // index in global memory (globally unique)\n",
    "      int s_idx = threadIdx.x;                           // index in shared memory (unique within a block)\n",
    "\n",
    "      if(g_idx >= n) return; // boundary guard\n",
    "\n",
    "      // Copy elements in global memory (input) to shared memory (shared)\n",
    "      shared[s_idx] = input[g_idx];\n",
    "\n",
    "      // Synchronize threads\n",
    "      __syncthreads();       // all threads in the same block must be done with the operations above before any thread can continue\n",
    "\n",
    "      // Copy elements in shared memory (shared) to global memory (output)\n",
    "      output[g_idx] = shared[s_idx];\n",
    "  }\n",
    "  ```\n",
    "- Now, let's look at modifications in the `main()` function (most of the code is the same as before, but with the timing removed for clarity).\n",
    "  - In fact, there is only one modification:\n",
    "    - Since we are using a dynamic size for our `shared memory`, we first define the size of the memory with `int shared_size = THREADS_PER_BLOCK_X * sizeof(int)`.\n",
    "    - Then we supply the size `shared_size` (in bytes) as a third parameter in the launch configuration `<<<gridDim, blockDim, shared_size>>>`.\n",
    "    - If we were using a static size, we would comment these two rows, uncomment the last row, and use the same launch configuration as before (i.e. no change).\n",
    "  - Best practice is to use a dynamic size, since we can determine a variable size in the code (without relying on e.g. a `#define` preprocessing directive).\n",
    "\n",
    "    ```c\n",
    "    // Device kernel() launch\n",
    "    int shared_size = THREADS_PER_BLOCK_X * sizeof(int);\n",
    "    kernel<<<gridDim, blockDim, shared_size>>>(d_input, d_output, N);\n",
    "    //kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "\n",
    "    ```\n",
    "- Run the cell below to see the output (which is exactly the same as before).\n",
    "\n",
    "**TL;DR**\n",
    "\n",
    "- `Shared memory` can be declared using either:\n",
    "  - A dynamic size\n",
    "    - We use the keyword `extern`and the qualifier `__shared__` infront of the local variable in the kernel function.\n",
    "    - We don't specify the size when declaring the variable in the kernel function, e.g. `extern __shared__ int shared[]`\n",
    "    - We pass the size (in bytes) of the `shared memory` as a third parameter in the launch configuration `<<<blocks, threads, shared_size>>>`.\n",
    "  - A static size\n",
    "    - We use the qualifier `__shared__` infront of the local variable in the kernel function.\n",
    "    - We include the size when declaring the variable in the kernel function, e.g. `__shared__ int shared[THREADS_PER_BLOCK_X]`\n",
    "    - We call the kernel function without passing a third parameter to the launch configuration (or the value `0`) `<<<blocks, threads, shared_size>>>`.\n",
    "- `Thread synchronization` is important when using `shared memory`.\n",
    "  - We can synchronize `threads` with the statement `__syncthreads();`\n",
    "    - No `thread` in a `block` can continue past that row until all `threads` have completed their tasks in the code above that row.\n",
    "- Remember this regarding indexing:\n",
    "  - A `thread`'s unique ID within a `block` is `threadIdx.x` (a specific `block` only runs on one `SM`, the `SM` with the `shared memory`).\n",
    "  - A `thread`'s globally unique ID within a grid is calculated as `blockIdx.x * blockDim.x + threadIdx.x`.\n",
    "- Multiple issues are related to `shared memory` (is one isn't aware of them).\n",
    "  - We won't explore these issues (e.g. memory coalescence, warp divergence, bank conflicts, occupancy, etc.) in detail in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "fa4f6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "//#define WORKITEMS_PER_WORKGROUP_0 2 // when using static local (shared) memory size\n",
    "\n",
    "//__kernel void mykernel(__global const int *input, __global int *output, const int n)                    // when using static local (shared) memory size\n",
    "__kernel void mykernel(__global const int *input, __global int *output, const int n, __local int *shared) // dynamic local (shared) memory size\n",
    "{\n",
    "    //__local int shared[WORKITEMS_PER_WORKGROUP_0]; // when using static local (shared) memory size\n",
    "\n",
    "    int g_idx = get_global_id(0); // index in global memory (globally unique)\n",
    "    int s_idx = get_local_id(0);  // index in local (shared) memory (unique within a workgroup)\n",
    "\n",
    "    if (g_idx >= n) return; // boundary guard\n",
    "\n",
    "    // Copy elements in global memory (input) to local (shared) memory\n",
    "    shared[s_idx] = input[g_idx];\n",
    "\n",
    "    // Synchronize workitems (threads)\n",
    "    barrier(CLK_LOCAL_MEM_FENCE); // all workitems (threads) in the same workgroup must be done with the operations above before any workitem (thread) can continue\n",
    "\n",
    "    // Copy elements in local (shared) memory to global memory (output)\n",
    "    output[g_idx] = shared[s_idx];\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "322da13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "    clSetKernelArg(kernel, 3, WORKITEMS_PER_WORKGROUP_0 * sizeof(int), NULL); // dynamic local (shared) memory size (remove when using static)\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "\n",
    "    // Read result back\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "9e88d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b44be",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- The output is exactly the same as before (same algorithm, just using different type of memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855d224",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.8 Constant Memory on the Device (GPU)\n",
    "\n",
    "- `Constant memory` is a special type of GPU memory optimized for cases where many `threads` read the same values.\n",
    "  - **Location**: On-device, separate from `global memory`.\n",
    "  - **Access**: Readable by all `threads` and is `read-only` from the `device`, but `writable` from `host`.\n",
    "  - **Size limit**: `64 KB` (per `device`).\n",
    "  - **Speed**: Very fast if all `threads` access the same address.\n",
    "  - **Scope**: Globally accessible (like global variables).\n",
    "  - **Lifetime**: Exists for the duration of the `kernel launch`.\n",
    "- Use `constant memory` when:\n",
    "  - All or most `threads` access the same data (e.g., coefficients, transformation matrices, filters).\n",
    "  - The data is known before kernel launch and doesn't change during execution.\n",
    "  - The data is small (<= `64 KB`).\n",
    "- Let's look at a simple example using `constant memory`.\n",
    "- It's the same code as before, but with the `shared memory` removed, and with the following modifications:\n",
    "  - Above the kernel function (not inside it), we declare a constant memory buffer (array) using the `__constant__` qualifier.\n",
    "  - In the kernel function:\n",
    "    - We multiply an element in the `input` array with an elements in the `constant` array, both with the same index.\n",
    "    - Then we assigning the product to the `output` array using the same index.\n",
    "    - Note that we have declared the size of the `constant memory` to be the same as the number of elements `N`.\n",
    "      - This is perfectly fine for this example where `N = 5`, but `constant memory` is extremely limited (small).\n",
    "      - We wouldn't be able to use `N` as the `constant memory`'s size if, say, `N` was `1000000` (a million elements).\n",
    "\n",
    "      ```c\n",
    "      // constant memory\n",
    "      __constant__ int constant[N];\n",
    "      \n",
    "      // Device kernel\n",
    "      __global__ void kernel(int *input, int *output, int n)\n",
    "      {   \n",
    "          int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "          \n",
    "          if(idx >= n) return; // boundary guard\n",
    "          \n",
    "          // Multiply input elements with coefficients in constant memory and store the product in output\n",
    "          output[idx] = input[idx] * constant[idx];\n",
    "      }\n",
    "      ```\n",
    "  - In the main() function, the code is the same as before (but with `shared memory` removed), but with the following modifications:\n",
    "    - We declare an `int` pointer variable on the host (CPU) to define the contents to be copied to the `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      int *h_coefficients;\n",
    "      ```\n",
    "    - We create a variable with the same size (but in bytes) as the statically defined `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      int constant_size = N * sizeof(int);\n",
    "      ```\n",
    "    - We allocate space in host (CPU) memory (RAM) the data with will be copying to the `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      h_coefficients = (int *)malloc(constant_size);\n",
    "      ```\n",
    "    - We initialize the data we will be copying tp `constant memory`.\n",
    "      - Notice, all the elements in `h_coefficients` are two (so the elements in the `output` array from the kernel function will be twice as large as in the `input` array).\n",
    "\n",
    "      ```c\n",
    "      for(int i = 0; i<N; i++)\n",
    "      {\n",
    "         h_coefficients[i] = 2;\n",
    "      } \n",
    "      ```\n",
    "    - Then we copy the host (CPU) memory to the device (GPU) `constant mmeory` using the CUDA function `cudaMemcpyToSymbol`.\n",
    "      - Notice that we aren't using the `cudaMemcpy` function when copying host (CPU) memory to device (GPU) `constant memory`.\n",
    "      - We pass in a pointer to the `constant` memory as the first argument.\n",
    "      - We pass in a pointer to the host (CPU) memory `h_coefficients` as the second argument.\n",
    "      - We pass in the size (in bytes) of the `constant memory` as the third argument.\n",
    "    \n",
    "      ```c\n",
    "      cudaMemcpyToSymbol(constant, h_coefficients, constant_size); // copy host (CPU) memory to device (GPU) constant memory\n",
    "      ```\n",
    "    - At the very end of the `main()` function, we free the memory on the host (CPU), allocated to store the values copied to `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      free(h_coefficients);\n",
    "      ```\n",
    "- Run the cell below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "53f7d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(__global const int *input, __global int *output, const int n, __constant const int *coefficients) // constant memory\n",
    "{\n",
    "    int idx = get_global_id(0); // index in global memory (globally unique)\n",
    "\n",
    "    if (idx >= n) return; // boundary guard\n",
    "\n",
    "    // Multiply input elements with coefficients in constant memory and store the product in output\n",
    "    output[idx] = input[idx] * coefficients[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "a4906f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "#define N 5\n",
    "#define WORKITEMS_PER_WORKGROUP_0 2\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output, *h_coefficients;\n",
    "    int data_size = N * sizeof(int);\n",
    "    int constant_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    h_coefficients = (int *)malloc(constant_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_coefficients[i] = 2;\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "    cl_mem d_coefficients = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, constant_size, h_coefficients, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(int), &n);\n",
    "    clSetKernelArg(kernel, 3, sizeof(cl_mem), &d_coefficients);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, NULL);\n",
    "\n",
    "    // Read result back\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_coefficients);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseMemObject(d_coefficients);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "7a63cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      166   \n",
      "86      172   \n",
      "77      154   \n",
      "15      30    \n",
      "93      186   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf09f0d",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- We see that the values in the `output` array are twice as large compared to the `input` array,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a505b9a",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Sample Problems\n",
    "---\n",
    "\n",
    "## 3.1 1D Vector Addition on the Host (CPU)\n",
    "\n",
    "<img src=\"images/vectoradd_cpu.png\" width=\"500\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Let's start with a simple problem.\n",
    "\n",
    "Problem\n",
    "  - We have three vectors (arrays) `A`, `B`, and `C`, all with `N` elements each.\n",
    "  - We want to compute the elementwise sum of `A` and `B`, and store the sum in `C`.\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576`\n",
    "2. Create a host function `void vectorAdd(float *A, float *B, float *C, int n)`\n",
    "    - Loop through vectors `A` and `B` with `idx=0..N-1`\n",
    "    - Compute `C[idx] = A[idx] + B[idx]`\n",
    "3. Create a host function `main(void)`\n",
    "    - Declare and allocate memory for vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Initialize vectors `h_A` and `h_B` with `N` random floats each.\n",
    "    - Call function `vectorAdd` with `h_A`, `h_B`, `h_C`, `N`, and measure the execution time for `vectorAdd`.\n",
    "    - Verify result is correct.\n",
    "    - Print execution time, verification result, and sample elements in vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Free memory allocated for vectors `h_A`, `h_B`, and `h_C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "2abfae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Number of elements (1048576)\n",
    "#define N (1 << 20) \n",
    "\n",
    "// Host function (elementwise addition of vectors A and B, placing the sum in vector C)\n",
    "void vectorAdd(float *A, float *B, float *C, int n)\n",
    "{   \n",
    "    // Loop through vectors and compute sum C = A + B\n",
    "    for (int idx = 0; idx < n; idx++)\n",
    "    {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host main routine\n",
    "int main(void)\n",
    "{\n",
    "    // Seed pseudorandom number generator\n",
    "    srand(0);\n",
    "\n",
    "    // Compute the size of the vectors (in bytes)\n",
    "    size_t size = N * sizeof(float);\n",
    "\n",
    "    // Declare and allocate host vectors A, B, and C   \n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    // Initialize host input vectors A and B with random values between 0 and 1.0\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        h_A[i] = rand() / (float)RAND_MAX;\n",
    "        h_B[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "\n",
    "    // Call function vectorAdd with timing\n",
    "    clock_t start = clock();\n",
    "\n",
    "    vectorAdd(h_A, h_B, h_C, N); // function call\n",
    "    \n",
    "    clock_t end = clock();\n",
    "    double elapsed_ms = (double)(end - start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    \n",
    "    // Verify results in ouput vector C is correct\n",
    "    float errorsum = 0.0f;\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        float error = fabs(h_A[i] + h_B[i] - h_C[i]);\n",
    "        if (error > 1e-5)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Print measured function execution time, verification result, and sample elements from each vector\n",
    "    printf(\"CPU execution time  : %f ms\\n\", elapsed_ms);\n",
    "    printf(\"Verification result : %s\\n\", (errorsum > 1e-5) ? \"FAILED\" : \"PASSED\");\n",
    "    printf(\"Vector samples      : A[0]=%f, B[0]=%f, C[0]=%f\\n\", h_A[0], h_B[0], h_C[0]);\n",
    "    \n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "5e9bda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU execution time  : 2.569000 ms\n",
      "Verification result : PASSED\n",
      "Vector samples      : A[0]=0.840188, B[0]=0.394383, C[0]=1.234571\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10200a36",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 1D Vector Addition on the Device (GPU)\n",
    "\n",
    "<img src=\"images/vectoradd_gpu1.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Problem\n",
    "  - We have three vectors (arrays) `A`, `B`, and `C`, all with `N` elements each.\n",
    "  - We want to compute the elementwise sum of `A` and `B`, and store the sum in `C`.\n",
    "\n",
    "We Know\n",
    "- In CUDA, we have access to many `threads`, where `threads` are organized into `blocks`, and `blocks` are organized into a `grid`.\n",
    "  - `threadIdx.x` represents a `thread’s index` along the `x` dimension within a `block`. \n",
    "  - `blockIdx.x` represents a `block’s index` along the `x` dimension within the `grid`.\n",
    "  - `blockDim.x` represents the `number of threads` along the `x` dimension with a `block`.\n",
    "- To get a `thread's global index` on the GPU:\n",
    "  - `int index = blockDim.x * blockIdx.x + threadIdx.x`\n",
    "- `Blocks` are assigned to a Streaming Multiprocessor (SM) that has a number of Streaming Processors (SPs).\n",
    "  - Each `thread` executes its own copy of the `kernel function`, in parallel, with the same parameter values.\n",
    "  - Each `thread` should process only one element in the arrays using the `index`.\n",
    "  - If there are more threads than elements (`index >= N`), those threads should `return` immediately from the `kernel function`\n",
    "- There can be a maximum of `1024` threads in a block.\n",
    "  - If we have `N = 1048576` elements,\n",
    "  - and `THREADS_PER_BLOCK = 1024`,\n",
    "  - we get `BLOCKS = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK = (1048576+1024-1) / 1024 = 1024`.\n",
    "  - And if we have `24` SMs, each will be assigned roughly `1024 / 24 = 42` blocks for maximum efficiency.\n",
    "\n",
    "<img src=\"images/vectoradd_gpu2.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576` and `THREADS_PER_BLOCK=1024`\n",
    "2. Create a kernel `__global__ void vectorAdd(float *A, float *B, float *C, int n)`\n",
    "    - Compute global thread ID `idx = blockDim.x * blockIdx.x + threadIdx.x`\n",
    "    - Return if index is out of bounds (`idx >= n`) which means we have more threads than elements `n`.\n",
    "      - In this case we won't since `N` is evenly divisible by `THREADS_PER_BLOCK`.\n",
    "    - Compute `C[idx] = A[idx] + B[idx]`.\n",
    "\n",
    "3. Create a host function `main(void)`\n",
    "    - Declare and allocate memory for host vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Initialize host vectors `h_A` and `h_B` with `N` random floats each.\n",
    "    - Declare and allocate memory for device vectors `d_A`, `d_B`, and `d_C`.\n",
    "    - Copy contents of host vectors `h_A` and `h_B` to device vectors `d_A` and `d_B`.\n",
    "    - Launch kernel `vectorAdd` with `d_A`, `d_B`, `d_C`, `N`, and measure the execution time for `vectorAdd`.\n",
    "    - Copy contents of device vector `d_C` to host vector `h_C`.\n",
    "    - Verify result is correct.\n",
    "    - Print execution time, verification result, and sample elements in host vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Free memory allocated for device vectors `d_A`, `d_B`, and `d_C`.\n",
    "    - Free memory allocated for host vectors `h_A`, `h_B`, and `h_C`.\n",
    "\n",
    "<img src=\"images/coalesced_memory_access.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "No need for shared or constant memory, and the global memory access pattern is **coalesced** in the code, (a) in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "b711eb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "// Device kernel (elementwise addition of vectors A and B, placing the sum in vector C)\n",
    "__kernel void mykernel(\n",
    "    __global const float *A,\n",
    "    __global const float *B,\n",
    "    __global float *C,\n",
    "    const int n)\n",
    "{\n",
    "    // Compute index (idx) from global workitem (thread) ID\n",
    "    int idx = get_global_id(0);\n",
    "\n",
    "    // Return if index is out of bounds (means we have more workitems (threads) than elements)\n",
    "    if (idx >= n)\n",
    "        return;\n",
    "\n",
    "    // Compute the sum C = A + B for the element with index idx\n",
    "    C[idx] = A[idx] + B[idx];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "bb602b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "// Number of elements (1048576)\n",
    "#define N (1 << 20)\n",
    "\n",
    "// Number of threads\n",
    "#define WORKITEMS_PER_WORKGROUP_0 1024\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    // Seed pseudorandom number generator\n",
    "    srand(0);\n",
    "    \n",
    "    // Compute the size of the vectors (in bytes)\n",
    "    size_t size = N * sizeof(float);\n",
    "\n",
    "    // Declare and allocate host vectors A, B, and C   \n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    // Initialize host input vectors A and B with random values between 0 and 1.0\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        h_A[i] = rand() / (float)RAND_MAX;\n",
    "        h_B[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "\n",
    "    // Allocate the device input vectors A, B, and copy data from host vectors A, B\n",
    "    // Allocate the device output vector C\n",
    "    cl_mem d_A, d_B, d_C;\n",
    "    d_A = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, size, h_A, &err);\n",
    "    d_B = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, size, h_B, &err);\n",
    "    d_C = clCreateBuffer(context, CL_MEM_WRITE_ONLY, size, NULL, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int n = N;\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_A);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_B);\n",
    "    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_C);\n",
    "    clSetKernelArg(kernel, 3, sizeof(int), &n);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = WORKITEMS_PER_WORKGROUP_0;\n",
    "    size_t globalSize = ((N + WORKITEMS_PER_WORKGROUP_0 - 1) / WORKITEMS_PER_WORKGROUP_0) * WORKITEMS_PER_WORKGROUP_0;\n",
    "\n",
    "    // Enqueue kernel with timing event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, &kernel_event);\n",
    "    \n",
    "    // Wait for kernel to finish and compute execution time\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "\n",
    "    // Read result back from device output vector C to host output vector C\n",
    "    clEnqueueReadBuffer(queue, d_C, CL_TRUE, 0, size, h_C, 0, NULL, NULL);\n",
    "\n",
    "    // Verify the result vector is correct\n",
    "    float errorsum = 0.0f;\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        float error = fabs(h_A[i] + h_B[i] - h_C[i]);\n",
    "        if (error > 1e-5)\n",
    "        {\n",
    "            //fprintf(stderr, \"Result verification failed at element %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    // Print measured kernel execution time, verification result, and sample elements from each vector\n",
    "    printf(\"GPU execution time  : %f ms\\n\", elapsed_ms);\n",
    "    printf(\"Verification result : %s\\n\", (errorsum > 1e-5) ? \"FAILED\" : \"PASSED\");\n",
    "    printf(\"Vector samples      : A[0]=%f, B[0]=%f, C[0]=%f\\n\", h_A[0], h_B[0], h_C[0]);\n",
    "\n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    // Free device global memory and event\n",
    "    clReleaseMemObject(d_A);\n",
    "    clReleaseMemObject(d_B);\n",
    "    clReleaseMemObject(d_C);\n",
    "    clReleaseEvent(kernel_event);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "7557fc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU execution time  : 0.032768 ms\n",
      "Verification result : PASSED\n",
      "Vector samples      : A[0]=0.840188, B[0]=0.394383, C[0]=1.234571\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308f4c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.3 1D Convolution on the Host (CPU)\n",
    "\n",
    "<img src=\"images/1dconvolution.gif\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Next, let's tackle the problem of a 1-dimensional (1D) convolution.\n",
    "\n",
    "Problem\n",
    "- We have an `input` vector, a `kernel` (filter), and an `output` vector.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` vector.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` vector.\n",
    "- So the `kernel`'s (filter's) width has to be odd, e.g. `1x3`, `1x5`, `1x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` vector with `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` vector with the same `index` as the current `input` vector.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` vector, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576`\n",
    "2. Create a function:\n",
    "   - `void convolve1D(float *input, float *output, float *filter)`\n",
    "   - Loop through `input` vector.\n",
    "   - Compute `output[idx] = input[idx + offset] = filter[FILTER_WIDTH/2 + offset]`\n",
    "     - Only if `if(idx + offset >= 0 && idx + offset < DATA_WIDTH)`\n",
    "     - Where `offset` ranges from `-FILTER_WIDTH/2` to `+FILTER_WIDTH/2`.\n",
    "   - This computation is equivalent to\n",
    "     - Looping through the `input` vector, zero-padded with `FILTER_WIDTH/2` elements on both sides.\n",
    "     - Centering the `filter` over each original element in the zero-padded `input` vector.\n",
    "     - Computing the weighted sum and storing it in the `output` vector.\n",
    "4. Create a function `main(void)`\n",
    "   - Define a `DATA_WIDTH`, `FILTER_WIDTH` and `FILTER_WIDTH_OFFSET` (which is `FILTER_WIDTH/2`).\n",
    "   - Declare and allocate memory for vectors `input`, `ouput`, and `filter`.\n",
    "   - Initialize vector `input` with `DATA_WIDTH` random floats.\n",
    "   - Initialize vector `filter` with `weights` where each weight is `1.0 / FILTER_WIDTH` (averaging filter).\n",
    "   - Call function `convolve1D` with `input`, `ouput`, and `filter`.\n",
    "   - Measure the execution time for `convolve1D`.\n",
    "   - Print execution time and sample elements in vectors `input` and `output`.\n",
    "   - Free memory allocated for vectors `input`, `output`, and `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "0a78bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Number of data elements (1048576)\n",
    "#define DATA_WIDTH (1 << 20)\n",
    "\n",
    "// Number of filter elements \n",
    "#define FILTER_WIDTH 3\n",
    "\n",
    "// Number of elements on each side of a centered filter\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "\n",
    "void convolve1D(float *input, float *output, float *filter)\n",
    "{\n",
    "    // Loop through all elements\n",
    "    for(int d_col = 0; d_col < DATA_WIDTH; d_col++)\n",
    "    {\n",
    "        // Apply filter (slide filter over data and compute weighted sum)\n",
    "        float sum = 0.0f;\n",
    "        for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "        {\n",
    "            int f_col = FILTER_WIDTH_OFFSET + offset_col; // f_col: 0..FILTER_WIDTH-1\n",
    "            int i_col = d_col + offset_col;               // i_col: 0-FILTER_WIDTH_OFFSET..DATA_WIDTH-1+FILTER_WIDTH_OFFSET\n",
    "            \n",
    "            if(i_col >= 0 && i_col < DATA_WIDTH)\n",
    "            {\n",
    "                sum += input[i_col] * filter[f_col];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Store the weighted sum in the output array\n",
    "        output[d_col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Seed the random number generator\n",
    "    srand(0);            // use this for same set of random numbers each time the program is run\n",
    "    //srand(time(NULL)); // use this for different set of random numbers each time the program is run\n",
    "\n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * sizeof(float);     // size of data in bytes\n",
    "    int filter_size = FILTER_WIDTH * sizeof(float); // size of filter in bytes\n",
    "   \n",
    "    // Allocate space for host copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "      \n",
    "    // Setup input values\n",
    "    for (int col = 0; col < DATA_WIDTH; col++)\n",
    "    {\n",
    "        h_input[col] = (float)rand() / RAND_MAX; // Random floats between 0 and 1.0\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for (int col = 0; col < FILTER_WIDTH; col++)\n",
    "    {\n",
    "        h_filter[col] = 1.0f / FILTER_WIDTH; // averaging filter\n",
    "    }\n",
    "   \n",
    "    // Call convolve1D() with timing\n",
    "    clock_t start = clock();                                              // record the start time\n",
    "    convolve1D(h_input, h_output, h_filter);                              // call convolve1D()\n",
    "    clock_t stop = clock();                                               // record the stop time\n",
    "    double elapsed_ms = (double)(stop - start) / CLOCKS_PER_SEC * 1000.0; // calculate the elapsed time in millisecond\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, 1x%d filter) took %.2f ms\\n\", DATA_WIDTH, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"Vector samples:\\n\");\n",
    "    for(int i = 0; i < FILTER_WIDTH; i++)\n",
    "    {\n",
    "        printf(\"h_input[%d]=%.2f, h_output[%d]=%.2f\\n\", i, h_input[i], i, h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "    \n",
    "    return 0;\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "1ba9cf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1048576 elements, 1x3 filter) took 7.21 ms\n",
      "Vector samples:\n",
      "h_input[0]=0.84, h_output[0]=0.41\n",
      "h_input[1]=0.39, h_output[1]=0.67\n",
      "h_input[2]=0.78, h_output[2]=0.66\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5e7ab",
   "metadata": {},
   "source": [
    "- The output shows:\n",
    "  - Given an element with index `idx` in the `output` array.\n",
    "  - It's value is the average of the elements with indices `-FILTER_WIDTH_OFFSET..+FILTER_WIDTH_OFFSET+1` in the `input` array.\n",
    "    - Since an averaging filter was used.\n",
    "  - For example\n",
    "    - If the `FILTER_WIDTH` is `3`, we have `FILTER_WIDTH_OFFSET = FILTER_WIDTH / 2 = 1`.\n",
    "    - The value of an element with index `idx` in the `output` array is the average of the elements with indices `idx-1`, `idx`, and `idx+1` in the `input` array.\n",
    "      - `output[idx] = (input[idx-1] +  nput[idx] + nput[idx+1]) / 3`\n",
    "      - If it's a bounday element, the out-of-bounds indices have zero-padded elements with a value of `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0137e2d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.4 1D Convolution on the Device (GPU)\n",
    "\n",
    "<img src=\"images/tiled_convolution_1d.png\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Problem\n",
    "- We have an `input` vector, a `kernel` (filter), and an `output` vector.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` vector.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` vector.\n",
    "- So the `kernel`'s (filter's) width has to be odd, e.g. `1x3`, `1x5`, `1x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` vector with `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` vector with the same `index` as the current `input` vector.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` vector, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "- We have a 1D `input` vector with `N` elements (vector marked with `N` in the figure).\n",
    "- A `block` of `threads` will process `blockDim` number of elements (top row in figure).\n",
    "- We don't want to load elements multiple times from `global memory` during the calulation.\n",
    "  - So each `thread`in a `block` loads its `input` element into `shared memory` (called `Tile` in the figure).\n",
    "  - The `shared_memory` size needs to be `TILE_BASE_WITH + 2 * FILTER_WIDTH_OFFSET`, where\n",
    "    - `TILE_BASE_WITH` is the number of original `input` elements in a `block` (highlighted elements in figure).\n",
    "    - `FILTER_WIDTH_OFFSET` is `FILTER_WIDTH / 2` (called `halo` elements in the figure).\n",
    "    - `FILTER_WIDTH` is `5` (in the figure).\n",
    "\n",
    "<img src=\"images/block_tile_loading_1d.png\" width=\"400\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "  - This ensures the `filter`, when centered on an element, covers all neighbouring elements, e.g.\n",
    "    - In `Block 0` the threads use `Tile 0`, where the original elements are `0`, `1`, `2`, `3` (see figure).\n",
    "    - The `filter` is centered on `0` covering `FILTER_WIDTH_OFFSET` neighbouring elements on each side.\n",
    "    - For border elements we use zero-padding (called `ghost` elements in the figure for the left-most elements).\n",
    "    - So the elements included in the first convolution are `ghost`, `ghost`, `0`, `1`, `2` (where `ghost = 0`).\n",
    "      - When processing element `3`, the `filter` covers elements `1`, `2`, `3`, `4`, `5`.\n",
    "\n",
    "- For these extra `2 * FILTER_WIDTH_OFFSET` elements to be available in a `block`:\n",
    "    - The `shared memory`, called `Tile`, needs a size of `TILE_BASE_WITH + 2 * FILTER_WIDTH_OFFSET` (see above).\n",
    "      - This is the actual size need for a `block` of `threads`, i.e. `blockDim` which includes:\n",
    "      - Threads for loading the original elements that the `filter` will center on.\n",
    "      - Threads for the extra `2 * FILTER_WIDTH_OFFSET` border elements.\n",
    "      - This is illustrated in the bottom figure.\n",
    "- We also want to load the `filter` elements into `constant` mmeory to avoid hitting global mmeory when accessing them.\n",
    "\n",
    "- So this is what we'll do:\n",
    "1. Define:\n",
    "  - `DATA_WIDTH=1048576` (number of elements in data vectors `input` and `output`)\n",
    "  - `FILTER_WIDTH=3` (number of elements in the `filter` vector)\n",
    "  - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements on each size of a centered `filter`)\n",
    "  - `TILE_WIDTH_BASE=16` (number original elements in a `block` of `threads`)\n",
    "    - Where the final tile size is `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` to cover border elements.\n",
    "    - This is also the size we will use for the `shared memory` and `block` size, i.e. `blockDim.x`.\n",
    "    - So we have these many `threads` in each `block` and we now a `block` is assigned to an `SM`.\n",
    "2. Define `constant` memory of size `FILTER_WIDTH` for the `filter`.\n",
    "3. Create a kernel function `void convolve1D(float *input, float *output)`:\n",
    "   - Define `shared` memory of size `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET`.\n",
    "   - Let the `threads` in a `block` load their `input` elements into `shared` memory.\n",
    "     - For border elements, we load the value `0` into `shared` memory (zero-padding).\n",
    "   - Synchronize `threads`to ensure each `thread`in a `block` has loaded its element into `shared` memory.\n",
    "   - Compute the convolution as in the CPU solution, but now using `shared` memory (input) and `constant` memory (filter).\n",
    "   - Store the result in the `ouput` vector.\n",
    "5. Create a function `main(void)`\n",
    "   - Declare and allocate memory for vectors `input`, `ouput`, and `filter` on the host (CPU).\n",
    "   - Declare and allocate memory for vectors `input` and `ouput` on the device (GPU).\n",
    "   - Initialize vector `input` with `DATA_WIDTH` random floats on the host (CPU).\n",
    "   - Initialize vector `filter` with `weights` on the host (CPU).\n",
    "     - Each weight is `1.0 / FILTER_WIDTH` (averaging filter).\n",
    "   - Copy `input` vector in host (CPU) memory to device (GPU) global memory.\n",
    "   - Copy `filter` vector in host (CPU) memory to `constant` device (GPU) memory.\n",
    "   - Launch kernel `convolve1D` with device (GPU) `input` and `ouput` vectors as arguments.\n",
    "     - Use `gridDim`, `blockDim`, and `shared_memory_size` as launch parameters, where\n",
    "       - `blockDim = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET`\n",
    "       - `gridDim = (DATA_WIDTH + block_width - 1) / block_width`\n",
    "       - `shared_mmeory_size = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` (`* sizeof(float)`)\n",
    "   - Measure the execution time for `convolve1D`.\n",
    "   - Copy `output` vector in device (GPU) memory to host (CPU) memory.\n",
    "   - Print execution time and sample elements in vectors `input` and `output`.\n",
    "   - Free memory allocated for vectors `input`, `output`, and `filter` on the host (CPU).\n",
    "   - Free memory allocated for vectors `input` and `output` on the device (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "c3c685f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(\n",
    "    __global const float *input,\n",
    "    __global float *output,\n",
    "    __constant const float *filter,\n",
    "    __local float *shared,\n",
    "    const int data_width,\n",
    "    const int filter_width_offset)\n",
    "{\n",
    "    int s_col = get_local_id(0);             // Workitem's (thread's) index in shared memory\n",
    "    int d_col = get_global_id(0);            // Workitem's (thread's) index in global memory\n",
    "    int i_col = d_col - filter_width_offset; // Workitem's (thread's) offset index in global memory\n",
    "\n",
    "    // Guard against workitems (threads) with IDs that would index outside the arrays\n",
    "    if (d_col >= data_width) return;\n",
    "\n",
    "    // Fill local (shared) memory with elements in global memory\n",
    "    if (i_col >= 0 && i_col < data_width)\n",
    "    {\n",
    "        shared[s_col] = input[i_col];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        shared[s_col] = 0.0f; // zero-padding\n",
    "    }\n",
    "\n",
    "    // Make sure each workitem (thread) in the workgroup has entered its element\n",
    "    // into local (shared) memory before any workitem (thread) continues\n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "    // Apply filter\n",
    "    float sum = 0.0f;\n",
    "    for (int offset_col = -filter_width_offset; offset_col <= filter_width_offset; offset_col++)\n",
    "    {\n",
    "        int f_col = filter_width_offset + offset_col;\n",
    "        int i_col = s_col + f_col;\n",
    "        \n",
    "        if(i_col >= 0 && i_col < get_local_size(0))\n",
    "        {\n",
    "            sum += shared[i_col] * filter[f_col]; // data elements in local (shared) memory + filter weights in constant memory = super fast computation\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Store the weighted sum in the output array\n",
    "    output[d_col] = sum;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "827bd7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "// Number of data elements (1048576)\n",
    "#define DATA_WIDTH (1 << 20)\n",
    "\n",
    "// Number of filter elements\n",
    "#define FILTER_WIDTH 3\n",
    "\n",
    "// Number of elements on each side of a centered filter\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "\n",
    "// Number of elements in local (shared) memory\n",
    "#define TILE_WIDTH_BASE 16\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    //srand(time(NULL));\n",
    "    \n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * sizeof(float);\n",
    "    int filter_size = FILTER_WIDTH * sizeof(float);\n",
    "\n",
    "    // Allocate space for host (CPU) copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "\n",
    "    // Setup input values\n",
    "    for (int col = 0; col < DATA_WIDTH; col++)\n",
    "    {\n",
    "        h_input[col] = (float)rand() / RAND_MAX; // Random floats between 0 and 1.0\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for (int col = 0; col < FILTER_WIDTH; col++)\n",
    "    {\n",
    "        h_filter[col] = 1.0f / FILTER_WIDTH; // averaging filter\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output, d_filter;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "    d_filter = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, filter_size, h_filter, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int data_width = DATA_WIDTH;\n",
    "    cl_int filter_width_offset = FILTER_WIDTH_OFFSET;\n",
    "    int workgroup_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "    int shared_size = workgroup_width * sizeof(float);\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_filter);\n",
    "    clSetKernelArg(kernel, 3, shared_size, NULL);         // dynamic local (shared) memory size\n",
    "    clSetKernelArg(kernel, 4, sizeof(int), &data_width);\n",
    "    clSetKernelArg(kernel, 5, sizeof(int), &filter_width_offset);  \n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize = workgroup_width;\n",
    "    size_t globalSize = ((DATA_WIDTH + workgroup_width - 1) / workgroup_width) * workgroup_width;\n",
    "\n",
    "    // Enqueue kernel with timing event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalSize, &localSize, 0, NULL, &kernel_event);\n",
    "\n",
    "    // Wait for kernel to finish and compute execution time\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "\n",
    "    // Copy result back to host\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, 1x%d filter) took %.2f ms\\n\", DATA_WIDTH, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"Vector samples:\\n\");\n",
    "    for(int i = 0; i < FILTER_WIDTH; i++)\n",
    "    {\n",
    "        printf(\"h_input[%d]=%.2f, h_output[%d]=%.2f\\n\", i, h_input[i], i, h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseMemObject(d_filter);\n",
    "    clReleaseEvent(kernel_event);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "52c2eef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1048576 elements, 1x3 filter) took 0.14 ms\n",
      "Vector samples:\n",
      "h_input[0]=0.84, h_output[0]=0.41\n",
      "h_input[1]=0.39, h_output[1]=0.67\n",
      "h_input[2]=0.78, h_output[2]=0.66\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21905cb2",
   "metadata": {},
   "source": [
    "In the output we see:\n",
    "- The results are the same for the GPU solution as for the CPU solution.\n",
    "- The execution time for the GPU solution is significantly fast than the CPU solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88e6fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.5 2D Convolution on the Host (CPU)\n",
    "\n",
    "<img src=\"images/2dconvolution.gif\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "**Note**\n",
    "  - This is really just the same problem as a 1D convolution, but with an added second dimension.\n",
    "  - Therefore the problem and solution will be the same, but with the second dimension accounted for.\n",
    "\n",
    "Problem\n",
    "- We have an `input` matrix, a `kernel` (filter), and an `output` matrix.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` matrix.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` matrix.\n",
    "- So the `kernel`'s (filter's) width and height has to be odd, e.g. `3x3`, `5x5`, `7x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` matrix with the `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` matrix with the same `index` as the current `input` matrix.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` matrix, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define:\n",
    "   - `DATA_WIDTH=32` (number of elements in the `col` dimension for the `input` and `output`)\n",
    "   - `DATA_HEIGHT=32` (number of elements in the `row` dimension for the `input` and `output`)\n",
    "   - `FILTER_WIDTH=3` (number of elements int the `col` dimension for the `filter`)\n",
    "   - `FILTER_HEIGHT=3` (number of elements int the `row` dimension for the `filter`)\n",
    "   - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements to the left and right of the centered `filter`)\n",
    "   - `FILTER_HEIGHT_OFFSET=FILTER_HEIGHT/2` (number of elements above and below the centered `filter`)\n",
    "3. Create a function:\n",
    "   - `void convolve2D(float *input, float *output, float *filter)`\n",
    "   - Loop through `input` matrix.\n",
    "   - Compute convolution. Store result in `output` matrix.\n",
    "   - The only difference in the \"D convolution compared to the 1D convolution is the additional dimension.\n",
    "4. Create a function `main(void)`\n",
    "   - Declare and allocate memory for matrices `input`, `ouput`, and `filter`.\n",
    "   - Initialize matrix `input` with `DATA_HEIGHT * DATA_WIDTH` random floats.\n",
    "   - Initialize matrix `filter` with `weights` where each weight is `1.0 / FILTER_HEIGHT * FILTER_WIDTH` (averaging filter).\n",
    "   - Call function `convolve2D` with `input`, `ouput`, and `filter`.\n",
    "   - Measure the execution time for `convolve2D`.\n",
    "   - Print execution time and sample elements in matrices `input` and `output`.\n",
    "   - Free memory allocated for matrices `input`, `output`, and `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "77449720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define DATA_WIDTH 32\n",
    "#define DATA_HEIGHT 32\n",
    "#define FILTER_WIDTH 3\n",
    "#define FILTER_HEIGHT 3\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH/2)\n",
    "#define FILTER_HEIGHT_OFFSET (FILTER_HEIGHT/2)\n",
    "\n",
    "void convolve2D(float *input, float *output, float *filter)\n",
    "{\n",
    "    for(int d_row = 0; d_row < DATA_HEIGHT; d_row++)\n",
    "    {\n",
    "        for(int d_col = 0; d_col < DATA_WIDTH; d_col++)\n",
    "        {\n",
    "            float sum = 0.0f;\n",
    "            for (int offset_row = -FILTER_HEIGHT_OFFSET; offset_row <= FILTER_HEIGHT_OFFSET; offset_row++)\n",
    "            {\n",
    "                for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "                {\n",
    "                    int f_row = FILTER_HEIGHT_OFFSET + offset_row;\n",
    "                    int f_col = FILTER_WIDTH_OFFSET + offset_col;\n",
    "                    int i_row = d_row + offset_row;\n",
    "                    int i_col = d_col + offset_col;\n",
    "\n",
    "                    if(i_row >= 0 && i_row < DATA_HEIGHT && i_col >= 0 && i_col < DATA_WIDTH)\n",
    "                    {\n",
    "                        sum += input[i_row * DATA_WIDTH + i_col] * filter[f_row * FILTER_WIDTH + f_col];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            output[d_row * DATA_WIDTH + d_col] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "\n",
    "    float *h_input = (float *)malloc(DATA_WIDTH * DATA_HEIGHT * sizeof(float));\n",
    "    float *h_output = (float *)malloc(DATA_WIDTH * DATA_HEIGHT * sizeof(float));\n",
    "    float *h_filter = (float *)malloc(FILTER_WIDTH * FILTER_HEIGHT * sizeof(float));\n",
    "\n",
    "    for(int row = 0; row < DATA_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < DATA_WIDTH; col++)\n",
    "        {\n",
    "            h_input[row * DATA_WIDTH + col] = (float)rand() / RAND_MAX;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            h_filter[row * FILTER_WIDTH + col] = 1.0f / (FILTER_WIDTH * FILTER_HEIGHT);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Call convolve2D() with timing\n",
    "    clock_t start = clock();\n",
    "    convolve2D(h_input, h_output, h_filter);\n",
    "    clock_t stop = clock();\n",
    "    double elapsed_ms = (double)(stop - start) / CLOCKS_PER_SEC * 1000.0;\n",
    "\n",
    "    printf(\"Calculation (%d elements, %dx%d filter) took %.2f ms\\n\", DATA_HEIGHT * DATA_WIDTH, FILTER_HEIGHT, FILTER_WIDTH, elapsed_ms);\n",
    "    printf(\"\\nMatrix samples:\\n\");\n",
    "    printf(\"h_input %-12s h_output\\n\", \"\");\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_input[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"%-3s\",\"\");\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_output[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "5b069a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1024 elements, 3x3 filter) took 0.03 ms\n",
      "\n",
      "Matrix samples:\n",
      "h_input              h_output\n",
      "0.840 0.394 0.783    0.238 0.396 0.382 \n",
      "0.613 0.296 0.638    0.328 0.527 0.568 \n",
      "0.267 0.540 0.375    0.325 0.514 0.616 \n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc51aab",
   "metadata": {},
   "source": [
    "- The output shows:\n",
    "  - Given an element with index `[row, col]` in the `output` matrix.\n",
    "  - It's value is the average of the elements with indices:\n",
    "    - `-FILTER_HEIGHT_OFFSET..+FILTER_HEIGHT_OFFSET+1` in the `input` matrix's `row`.\n",
    "    - `-FILTER_WIDTH_OFFSET..+FILTER_WIDTH_OFFSET+1` in the `input` matrix's `col`.\n",
    "    - Since an averaging filter was used.\n",
    "  - For example\n",
    "    - If the `FILTER_HEIGHT` is `3`, we have `FILTER_HEIGHT_OFFSET = FILTER_HEIGHT / 2 = 1`.\n",
    "    - If the `FILTER_WIDTH` is `3`, we have `FILTER_WIDTH_OFFSET = FILTER_WIDTH / 2 = 1`.\n",
    "    - The value of an element with index `[row, col]` in the `output` matrix is the average of the elements in the `input` matrix with indices:\n",
    "\n",
    "      ```c\n",
    "      [row-1, col-1]  [row-1, col]  [row-1, col+1]\n",
    "      [row  , col-1]  [row  , col]  [row  , col+1]\n",
    "      [row+1, col-1]  [row+1, col]  [row+1, col+1]\n",
    "      ```\n",
    "    - If it's a bounday element, the out-of-bounds indices have zero-padded elements with a value of `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333dd74",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.6 2D Convolution on the Device (GPU)\n",
    "\n",
    "**Note**\n",
    "  - This is really just the same problem as a 1D convolution, but with an added second dimension.\n",
    "  - Therefore the problem and solution will be the same, but with the second dimension accounted for.\n",
    "\n",
    "Problem\n",
    "- We have an `input` matrix, a `kernel` (filter), and an `output` matrix.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` matrix.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` matrix.\n",
    "- So the `kernel`'s (filter's) height and width has to be odd, e.g. `3x3`, `5x5`, `7x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` matrix with the `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` matrix with the same `index` as the current `input` matrix.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` matrix, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define:\n",
    "  -  `DATA_WIDTH=32` (number of elements in the `col` dimension for the `input` and `output`)\n",
    "   - `DATA_HEIGHT=32` (number of elements in the `row` dimension for the `input` and `output`)\n",
    "   - `FILTER_WIDTH=3` (number of elements int the `col` dimension for the `filter`)\n",
    "   - `FILTER_HEIGHT=3` (number of elements int the `row` dimension for the `filter`)\n",
    "   - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements to the left and right of the centered `filter`)\n",
    "   - `FILTER_HEIGHT_OFFSET=FILTER_HEIGHT/2` (number of elements above and below the centered `filter`) \n",
    "   - `TILE_WIDTH_BASE=16` (number original elements in a `block` of `threads` in the `col` dimension)\n",
    "   - `TILE_HEIGHT_BASE=16` (number original elements in a `block` of `threads` in the `row` dimension)\n",
    "     - Where the final tile size in the `col` dimension is `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` to cover left and right border elements.\n",
    "       - This is also the size we will use for the `col` dimension in `shared memory` and `block` size, i.e. `blockDim.x`.\n",
    "     - Where the final tile size in the `row` dimension is `TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET` to cover top and bottom border elements.\n",
    "       - This is also the size we will use for the `row` dimension in `shared memory` and `block` size, i.e. `blockDim.y`.\n",
    "     - So we have these many 2D `threads` in each `block` and we know a `block` is assigned to an `SM`.\n",
    "3. Define `constant` memory of size `FILTER_HEIGHT * FILTER_WIDTH` for the `filter`.\n",
    "4. Create a kernel function `void convolve2D(float *input, float *output)`:\n",
    "   - Define `shared` memory of size `(TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET) * (TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET)`.\n",
    "   - Let the `threads` in a `block` load their `input` elements into `shared` memory.\n",
    "     - For border elements, we load the value `0` into `shared` memory (zero-padding).\n",
    "   - Synchronize `threads`to ensure each `thread`in a `block` has loaded its element into `shared` memory.\n",
    "   - Compute the convolution as in the CPU solution, but now using `shared` memory (input) and `constant` memory (filter).\n",
    "   - Store the result in the `ouput` matrix.\n",
    "5. Create a function `main(void)`\n",
    "   - Declare and allocate memory for matrices `input`, `ouput`, and `filter` on the host (CPU).\n",
    "   - Declare and allocate memory for matrices `input` and `ouput` on the device (GPU).\n",
    "   - Initialize matrix `input` with `DATA_HEIGHT * DATA_WIDTH` random floats on the host (CPU).\n",
    "   - Initialize matrix `filter` with `weights` on the host (CPU).\n",
    "     - Each weight is `1.0 / (FILTER_HEIGHT * FILTER_WIDTH)` (averaging filter).\n",
    "   - Copy `input` matrix in host (CPU) memory to device (GPU) global memory.\n",
    "   - Copy `filter` matrix in host (CPU) memory to `constant` device (GPU) memory.\n",
    "   - Launch kernel `convolve2D` with device (GPU) `input` and `ouput` matrices as arguments.\n",
    "     - Use `gridDim`, `blockDim`, and `shared_memory_size` as launch parameters, where\n",
    "       - `blockDim.x = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET`\n",
    "       - `blockDim.y = TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET`\n",
    "       - `gridDim.x = (DATA_WIDTH + block_width - 1) / block_width`\n",
    "       - `gridDim.y = (DATA_HEIGHT + block_height - 1) / block_height`\n",
    "       - `shared_mmeory_size = (TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET) * (TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET)` (`* sizeof(float)`)\n",
    "   - Measure the execution time for `convolve2D`.\n",
    "   - Copy `output` matrix in device (GPU) memory to host (CPU) memory.\n",
    "   - Print execution time and sample elements in matrices `input` and `output`.\n",
    "   - Free memory allocated for matrices `input`, `output`, and `filter` on the host (CPU).\n",
    "   - Free memory allocated for matrices `input` and `output` on the device (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "1efbead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/kernel.cl\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/kernel.cl\n",
    "__kernel void mykernel(\n",
    "    __global const float *input,\n",
    "    __global float *output,\n",
    "    __constant const float *filter,\n",
    "    __local float *shared,\n",
    "    const int data_height,\n",
    "    const int data_width,\n",
    "    const int filter_height_offset,\n",
    "    const int filter_width_offset,\n",
    "    const int filter_width)\n",
    "{\n",
    "    int s_row = get_local_id(1);\n",
    "    int s_col = get_local_id(0);\n",
    "    \n",
    "    int d_row = get_global_id(1);\n",
    "    int d_col = get_global_id(0);\n",
    "    \n",
    "    int i_row = d_row - filter_height_offset;\n",
    "    int i_col = d_col - filter_width_offset;\n",
    "\n",
    "    if (d_col >= data_width || d_row >= data_height) return;\n",
    "\n",
    "    if (i_row >= 0 && i_row < data_height && i_col >= 0 && i_col < data_width)\n",
    "    {\n",
    "        shared[s_row * get_local_size(0) + s_col] = input[i_row * data_width + i_col];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        shared[s_row * get_local_size(0) + s_col] = 0.0f; // zero-padding\n",
    "    }\n",
    "\n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "    float sum = 0.0f;\n",
    "    for (int offset_row = -filter_height_offset; offset_row <= filter_height_offset; offset_row++)\n",
    "    {\n",
    "        for (int offset_col = -filter_width_offset; offset_col <= filter_width_offset; offset_col++)\n",
    "        {\n",
    "            int f_row = filter_height_offset + offset_row;\n",
    "            int f_col = filter_width_offset + offset_col;\n",
    "            int i_row = s_row + f_row;\n",
    "            int i_col = s_col + f_col;\n",
    "            \n",
    "            if(i_row >= 0 && i_row < get_local_size(1) && i_col >= 0 && i_col < get_local_size(0))\n",
    "            {\n",
    "                sum += shared[i_row * get_local_size(0) + i_col] * filter[f_row * filter_width + f_col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output[d_row * data_width + d_col] = sum;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "17d10183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/main.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.c\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "#include \"utils.h\"\n",
    "\n",
    "// Number of data elements in each dimension\n",
    "#define DATA_WIDTH 32\n",
    "#define DATA_HEIGHT 32\n",
    "\n",
    "// Number of filter elements in each dimension\n",
    "#define FILTER_WIDTH 3\n",
    "#define FILTER_HEIGHT 3\n",
    "\n",
    "// Number of elements on each side of a centered filter in each dimension\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "#define FILTER_HEIGHT_OFFSET (FILTER_HEIGHT / 2)\n",
    "\n",
    "// Number of elements in local (shared) memory in each dimension\n",
    "#define TILE_WIDTH_BASE 16\n",
    "#define TILE_HEIGHT_BASE 16\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Setup OpenCL\n",
    "    cl_int err; cl_context context; cl_command_queue queue; cl_program program; cl_kernel kernel;\n",
    "    setupOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    srand(0);\n",
    "    //srand(time(NULL));\n",
    "    \n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * DATA_HEIGHT * sizeof(float);\n",
    "    int filter_size = FILTER_WIDTH * FILTER_HEIGHT * sizeof(float);\n",
    "\n",
    "    // Allocate space for host (CPU) copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "\n",
    "    // Setup input values\n",
    "    for(int row = 0; row < DATA_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < DATA_WIDTH; col++)\n",
    "        {\n",
    "            h_input[row * DATA_WIDTH + col] = (float)rand() / RAND_MAX;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            h_filter[row * FILTER_WIDTH + col] = 1.0f / (FILTER_WIDTH * FILTER_HEIGHT);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Device buffers\n",
    "    cl_mem d_input, d_output, d_filter;\n",
    "    d_input = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, data_size, h_input, &err);\n",
    "    d_output = clCreateBuffer(context, CL_MEM_WRITE_ONLY, data_size, NULL, &err);    \n",
    "    d_filter = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, filter_size, h_filter, &err);\n",
    "\n",
    "    // Set kernel arguments\n",
    "    cl_int data_height = DATA_HEIGHT;\n",
    "    cl_int data_width = DATA_WIDTH;\n",
    "    cl_int filter_height_offset = FILTER_HEIGHT_OFFSET;\n",
    "    cl_int filter_width_offset = FILTER_WIDTH_OFFSET;\n",
    "    cl_int filter_width = FILTER_WIDTH;\n",
    "    int workgroup_height = TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET;\n",
    "    int workgroup_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "    int shared_size = workgroup_height * workgroup_width * sizeof(float);\n",
    "    clSetKernelArg(kernel, 0, sizeof(cl_mem), &d_input);\n",
    "    clSetKernelArg(kernel, 1, sizeof(cl_mem), &d_output);\n",
    "    clSetKernelArg(kernel, 2, sizeof(cl_mem), &d_filter);\n",
    "    clSetKernelArg(kernel, 3, shared_size, NULL);\n",
    "    clSetKernelArg(kernel, 4, sizeof(int), &data_height);\n",
    "    clSetKernelArg(kernel, 5, sizeof(int), &data_width);\n",
    "    clSetKernelArg(kernel, 6, sizeof(int), &filter_height_offset);\n",
    "    clSetKernelArg(kernel, 7, sizeof(int), &filter_width_offset);\n",
    "    clSetKernelArg(kernel, 8, sizeof(int), &filter_width);\n",
    "\n",
    "    // Kernel launch configuration\n",
    "    size_t localSize[2] = { workgroup_width, workgroup_height };\n",
    "    size_t globalSize[2] = {\n",
    "        ((data_width + workgroup_width - 1) / workgroup_width) * workgroup_width,\n",
    "        ((data_height + workgroup_height - 1) / workgroup_height) * workgroup_height\n",
    "    };\n",
    "\n",
    "    // Enqueue kernel with timing event\n",
    "    cl_event kernel_event;\n",
    "    clEnqueueNDRangeKernel(queue, kernel, 2, NULL, globalSize, localSize, 0, NULL, &kernel_event);\n",
    "\n",
    "    // Wait for kernel to finish and compute execution time\n",
    "    clWaitForEvents(1, &kernel_event);\n",
    "    cl_ulong time_start, time_end;\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_START, sizeof(time_start), &time_start, NULL);\n",
    "    clGetEventProfilingInfo(kernel_event, CL_PROFILING_COMMAND_END, sizeof(time_end), &time_end, NULL);\n",
    "    double elapsed_ms = (time_end - time_start) * 1e-6;  // Convert nanoseconds to milliseconds\n",
    "\n",
    "    // Copy result back to host\n",
    "    clEnqueueReadBuffer(queue, d_output, CL_TRUE, 0, data_size, h_output, 0, NULL, NULL);\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, %dx%d filter) took %.2f ms\\n\", DATA_HEIGHT * DATA_WIDTH, FILTER_HEIGHT, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"\\nMatrix samples:\\n\");\n",
    "    printf(\"h_input %-12s h_output\\n\", \"\");\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_input[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"%-3s\",\"\");\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_output[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "    clReleaseMemObject(d_input);\n",
    "    clReleaseMemObject(d_output);\n",
    "    clReleaseMemObject(d_filter);\n",
    "    clReleaseEvent(kernel_event);\n",
    "    \n",
    "    // Teardown OpenCL\n",
    "    teardownOpenCL(&context, &queue, &program, &kernel);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "3ad36314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1024 elements, 3x3 filter) took 0.01 ms\n",
      "\n",
      "Matrix samples:\n",
      "h_input              h_output\n",
      "0.840 0.394 0.783    0.238 0.396 0.382 \n",
      "0.613 0.296 0.638    0.328 0.527 0.568 \n",
      "0.267 0.540 0.375    0.325 0.514 0.616 \n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fba808",
   "metadata": {},
   "source": [
    "In the output we see:\n",
    "- The results are the same for the GPU solution as for the CPU solution.\n",
    "- The execution time for the GPU solution is significantly fast than the CPU solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79f513",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Cleanup\n",
    "---\n",
    "\n",
    "- Let's remove all files that have been created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "826d62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "dirs = [\"src\", \"include\", \"bin\", \".vscode\"]\n",
    "files = [\"kernel.cl\", \"main.c\", \"main.exe\"]\n",
    "\n",
    "for d in dirs:\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "\n",
    "for f in files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "fsharp",
    "items": [
     {
      "aliases": [],
      "name": "fsharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
