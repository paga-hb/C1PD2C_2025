{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e27f3f",
   "metadata": {},
   "source": [
    "---\n",
    "# CUDA Programming in C\n",
    "---\n",
    "\n",
    "This notebook contains an introduction to CUDA programming in C. For detailed coverage, NVidia's documentation is a good source:\n",
    "\n",
    "- [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/contents.html)\n",
    "- [Nsight Compute CLI](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html)\n",
    "- [Nsight Developer Tools](https://docs.nvidia.com/nsight-developer-tools/index.html)\n",
    "\n",
    "**Note! If you don't have an Nvidia GPU on your system**\n",
    "\n",
    "- You can run this notebook in Google CoLab.\n",
    "  - Skip ahead to [1.1 Running the Notebook Locally or on Google CoLab](#11-running-the-notebook-locally-or-on-google-coLab)\n",
    "\n",
    "**Note! If you are on Windows**\n",
    "\n",
    "- Make sure you have installed Visual Studio or the Build Tools for Visual Studio.\n",
    "- Make sure you have started VSCode (`code .`) from within a `Visual Studio Developer Command Prompt` to set necessary environment variables.\n",
    "  - This is required when using the MicroSoft Visual C/C++ (MSVC) compiler `cl.exe` in VSCode on Windows.\n",
    "  - The NVidia Cuda Compiler (NVCC) compiles `device` code, but uses your system's default C/C++ compiler to compile `host` code (GCC on Linux, MSVC on Windows).\n",
    "  - If you have multiple C/C++ compilers installed on your system, you can tell NVCC which C/C++ compiler to use with the flag `--compiler-bindir <PATH_TO_DIR>`, where `<PATH_TO_DIR>` is the path to your C/C++ compiler's root directory.\n",
    "\n",
    "This notebook covers:\n",
    "\n",
    "- [1. Prerequisites](#1-prerequisites) \n",
    "  - [1.1 Running the Notebook Locally or on Google CoLab](#11-running-the-notebook-locally-or-on-google-coLab)\n",
    "  - [1.2 C Compiler (`gcc`, `clang`, `cl`) and CUDA Comiler (`nvcc`)](#12-c-compiler-gcc-clang-cl-and-cuda-compiler-nvcc)\n",
    "  - [1.3 Configuring `tasks.json`, `launch.json` and `c_cpp_properties.json`](#13-configuring-tasksjson-launchjson-and-c_cpp_propertiesjson)\n",
    "  - [1.4 Create the File `tasks.json`](#14-create-the-file-tasksjson)\n",
    "  - [1.5 Create the File `launch.json`](#15-create-the-file-launchjson)\n",
    "  - [1.6 Create the File `c_cpp_properties.json`](#16-create-the-file-c_cpp_propertiesjson)\n",
    "  - [1.7 VSCode Extensions](#17-vscode-extensions)\n",
    "  - [1.8 Using Notebook Extension `nvcc4jupyter` and Cell Magic `%%cuda`](#18-using-notebook-extension-nvcc4jupyter-and-cell-magic-cuda)\n",
    "  - [1.9 Using Built-in Cell Magic `%%writefile`](#19-using-built-in-cell-magic-writefile)\n",
    "  - [1.10 Compiling and Executing a CUDA Program from a Notebook Code Cell](#110-compiling-and-executing-a-cuda-program-from-a-notebook-code-cell)\n",
    "  - [1.11 Compiling and Debugging a Single-file CUDA Program](#111-compiling-and-debugging-a-single-file-cuda-program)\n",
    "  - [1.12 Compiling and Debugging a Multi-file CUDA Program](#112-compiling-and-debugging-a-multi-file-cuda-program)\n",
    "- [2. CUDA Basics](#2-cuda-basics)\n",
    "  - [2.1 Listing CUDA-enabled Devices and Properties](#21-listing-cuda-enabled-devices-and-properties)\n",
    "  - [2.2 Hello World in Host Code (CPU)](#22-hello-world-in-host-code-cpu)\n",
    "  - [2.3 Hello World in Device Code (GPU)](#23-hello-world-in-device-code-gpu)\n",
    "  - [2.4 Grids, Blocks, Threads, Devices, SMs, and SPs](#24-grids-blocks-threads-devices-sms-and-sps)\n",
    "  - [2.5 Unified Memory](#25-unified-memory)\n",
    "  - [2.6 Error Checking](#26-error-checking)\n",
    "  - [2.7 Measuring Execution Time on the Host (CPU) and on the Device (GPU)](#27-measuring-execution-time-on-the-host-cpu-and-on-the-device-gpu)\n",
    "  - [2.8 Shared Memory and Thread Synchronization on the Device (GPU)](#28-shared-memory-and-thread-synchronization-on-the-device-gpu)\n",
    "  - [2.9 Constant Memory on the Device (GPU)](#29-constant-memory-on-the-device-gpu)\n",
    "- [3. Sample Problems](#3-sample-problems)\n",
    "  - [3.1 1D Vector Addition on the Host (CPU)](#31-1d-vector-addition-on-the-host-cpu)\n",
    "  - [3.2 1D Vector Addition on the Device (GPU)](#32-1d-vector-addition-on-the-device-gpu)\n",
    "  - [3.3 1D Convolution on the Host (CPU)](#33-1d-convolution-on-the-host-cpu)\n",
    "  - [3.4 1D Convolution on the Device (GPU)](#34-1d-convolution-on-the-device-gpu)\n",
    "  - [3.5 2D Convolution on the Host (CPU)](#35-2d-convolution-on-the-host-cpu)\n",
    "  - [3.6 2D Convolution on the Device (GPU)](#36-2d-convolution-on-the-device-gpu)\n",
    "- [4. Cleanup](#4-cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c99674",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Prerequisites\n",
    "---\n",
    "\n",
    "## 1.1 Running the Notebook Locally or on Google CoLab\n",
    "\n",
    "- Run the cell below to check if you have a CUDA-enabled device and the CUDA tookit on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36736fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 13 17:57:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 2000 Ada Gene...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P3             10W /   35W |       1MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:23:50_PST_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ba36b",
   "metadata": {},
   "source": [
    "### Inspect the output from the cell above:\n",
    "- If you see an NVidia runtime `CUDA Version` (top right row) and a CUDA Toolkit `Build` version (bottom row).\n",
    "  - Make sure the CUDA Toolkit `Build` version is less than or equal to the NVidia runtime `CUDA Version` (if not you can update your CUDA Toolkit or use CoLab). \n",
    "  - Then skip to [1.2 C Compiler (`gcc`, `clang`, `cl`) and CUDA Comiler (`nvcc`)](#12-c-compiler-gcc-clang-cl-and-cuda-compiler-nvcc)\n",
    "- If you don't see an NVidia runtime `CUDA Version` (top right row) and a CUDA Toolkit `Build` version (bottom row), follow the instructions below.\n",
    "\n",
    "  1. Click the icon below to open the notebook in Google CoLab.\n",
    "     \n",
    "     [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/paga-hb/C1PD2C_2025/blob/main/notebooks/cuda.ipynb)\n",
    "\n",
    "  2. When the notebook opens in CoLab, choose `File -> Save a copy in Drive` from the main menu.\n",
    "  3. Choose `Runtime -> Change runtime type` from the main menu, select `TP4 GPU` as the hardware accelerator, and click the `Save` button.\n",
    "  4. In a notebook cell run the following code:\n",
    "\n",
    "      ```c\n",
    "      !sudo apt purge cuda && sudo apt autoremove && sudo apt autoclean\n",
    "      !wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run\n",
    "      !chmod +x cuda_12.4.0_550.54.14_linux.run\n",
    "      !sudo sh cuda_12.4.0_550.54.14_linux.run --silent --toolkit\n",
    "      !pip install nvcc4jupyter\n",
    "      %load_ext nvcc4jupyter\n",
    "      !nvidia-smi\n",
    "      !nvcc --version\n",
    "      ```\n",
    "\n",
    "  5. When the cell stops executing, make sure the CUDA Toolkit `Build` version is less than or equal to the NVidia runtime `CUDA Version`.\n",
    "  6. Then skip to [2. CUDA Basics](#2-cuda-basics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0549db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 C Compiler (`gcc`, `clang`, `cl`) and CUDA Compiler (`nvcc`)\n",
    "\n",
    "We are going to compose JSON configuration files for VSCode, so let's collect some information about your environment.\n",
    "\n",
    "**Mac**\n",
    "\n",
    "- CUDA isn't supported on Mac.\n",
    "- Open the notebook in CoLab instead, and skip to Chapter 2.\n",
    "\n",
    "**Windows**\n",
    "\n",
    "- Your only option is to use `cl` (the C/C++ compiler, part of Microsoft Visual Studio build tools).\n",
    "  - Make sure you have launched VSCode from within a `Developer Command Prompt for VS`.\n",
    "    - Search in your Start Menu for `Developer Command Prompt for VS` (the version depends on your installed Visual Studio version).\n",
    "    - Open it => it launches a command prompt with all environment variables (paths, includes, libs) configured to run `cl.exe` and other build tools.\n",
    "    - Open VSCode from the command prompt: `code .`\n",
    "  - Note that you won't be able to debug CUDA programs in VSCode (but you can in Visual Studio).\n",
    "- Run the cell below to get the path to the C compiler and the CUDA compiler.\n",
    "- If nothing shows up, you need to install a C/CUDA compiler (and/or make sure the C/CUDA compiler is in your `PATH` environment variable).\n",
    "\n",
    "**Linux**\n",
    "\n",
    "- In the cell below, choose the installed C compiler you want to use.\n",
    "  - If you are using `clang` (the C compiler, part of the LLVM project).\n",
    "    - Comment the row `c_compiler = \"gcc\"`\n",
    "    - Uncomment the row `c_compiler = \"clang\"`\n",
    "    - Note that you won't be able to debug CUDA programs in VSCode.\n",
    "  - If you are using `gcc` (GNU Compiler Collection), you're all set.\n",
    "- Run the cell below to get the path to the C compiler and the CUDA compiler.\n",
    "- If nothing shows up, you need to install a C/CUDA compiler (and/or make sure the C/CUDA compiler is in your `PATH` environment variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e700521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating System (OS) : linux\n",
      "C Compiler Name       : gcc\n",
      "C Compiler Path       : /usr/bin/gcc\n",
      "C Debugger Name       : gdb\n",
      "C Debugger Path       : /usr/bin/gdb\n",
      "CUDA Compiler Name    : nvcc\n",
      "CUDA Compiler Path    : /usr/local/cuda/bin/nvcc\n",
      "CUDA Debugger Name    : cuda-gdb\n",
      "CUDA Debugger Path    : /usr/local/cuda/bin/cuda-gdb\n"
     ]
    }
   ],
   "source": [
    "c_compiler = \"gcc\"\n",
    "#c_compiler = \"clang\"\n",
    "\n",
    "import platform, os\n",
    "os_name = platform.system()\n",
    "if os_name == \"Darwin\":\n",
    "    os_name = \"osx\"\n",
    "os_name = os_name.lower()\n",
    "\n",
    "windows_shell_name = \"cmd\"\n",
    "#windows_shell_name = \"powershell\"\n",
    "if os_name == 'windows':\n",
    "    windows_shell_path = !where {windows_shell_name}\n",
    "    windows_shell_path = windows_shell_path[0]\n",
    "    windows_shell_name = os.path.basename(windows_shell_path)\n",
    "    print(f\"{'Windows Shell Name':<21} : {windows_shell_name}\")\n",
    "    print(f\"{'Windows Shell Path':<21} : {windows_shell_path}\")\n",
    "\n",
    "if os_name == 'windows':\n",
    "    c_compiler = \"cl\"\n",
    "    c_compiler_path = !where {c_compiler}\n",
    "else:\n",
    "    c_compiler_path = !which {c_compiler}\n",
    "c_compiler_path = c_compiler_path[0]\n",
    "c_compiler_name = os.path.basename(c_compiler_path)\n",
    "\n",
    "if c_compiler == 'cl':\n",
    "    c_debugger_name = \"cdb.exe\"\n",
    "    c_debugger_path = \"<integrated>\"\n",
    "if c_compiler == \"gcc\":\n",
    "    c_debugger_name = \"gdb\"\n",
    "if c_compiler == \"clang\":\n",
    "    c_debugger_name = \"lldb\"\n",
    "\n",
    "if os_name == 'windows':\n",
    "    if c_compiler != 'cl':\n",
    "        c_debugger_path = !where {c_debugger_name}\n",
    "else:\n",
    "    c_debugger_path = !which {c_debugger_name}\n",
    "\n",
    "if c_compiler != 'cl':\n",
    "    c_debugger_path = c_debugger_path[0]\n",
    "    c_debugger_name = os.path.basename(c_debugger_path)\n",
    "\n",
    "cuda_compiler = \"nvcc\"\n",
    "if os_name == 'windows':\n",
    "    cuda_compiler_path = !where {cuda_compiler}\n",
    "else:\n",
    "    cuda_compiler_path = !which {cuda_compiler}\n",
    "cuda_compiler_path = cuda_compiler_path[0]\n",
    "cuda_compiler_name = os.path.basename(cuda_compiler_path)\n",
    "\n",
    "cuda_debugger = \"cuda-gdb\"\n",
    "if os_name == \"linux\" and c_compiler == \"gcc\":\n",
    "    cuda_debugger_path = !which {cuda_debugger}\n",
    "    cuda_debugger_path = cuda_debugger_path[0]\n",
    "    cuda_debugger_name = os.path.basename(cuda_debugger_path)\n",
    "else:\n",
    "    cuda_debugger_path = \"<N/A>\"\n",
    "    cuda_debugger_name = \"<N/A>\"\n",
    "\n",
    "print(f\"{'Operating System (OS)':<21} : {os_name}\")\n",
    "print(f\"{'C Compiler Name':<21} : {c_compiler_name}\")\n",
    "print(f\"{'C Compiler Path':<21} : {c_compiler_path}\")\n",
    "print(f\"{'C Debugger Name':<21} : {c_debugger_name}\")\n",
    "print(f\"{'C Debugger Path':<21} : {c_debugger_path}\")\n",
    "print(f\"{'CUDA Compiler Name':<21} : {cuda_compiler_name}\")\n",
    "print(f\"{'CUDA Compiler Path':<21} : {cuda_compiler_path}\")\n",
    "print(f\"{'CUDA Debugger Name':<21} : {cuda_debugger_name}\")\n",
    "print(f\"{'CUDA Debugger Path':<21} : {cuda_debugger_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a244e5",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3 Configuring `tasks.json`, `launch.json`, and `c_cpp_properties.json`\n",
    "\n",
    "- To develop CUDA programs in C with VSCode, we need to configure three VSCode workspace configuration files.\n",
    "  - In the file `tasks.json` we can configure various tasks, such as build tasks for compiling and linking CUDA programs with NVCC (and our chosen C compiler).\n",
    "  - In the file `launch.json` we can configure various debug options, such as debugging CUDA with CUDA-GDB (and our chosen C debugger).\n",
    "  - In the file `c_cpp_properties.json` we can configure the C compiler to use for linting purposes (intellisense).\n",
    "    - It isn't strictly necessary to create this configuration file to be able to run and debug CUDA programs in VSCode.\n",
    "- VSCode workspace configuration files (`.json`) are stored in the subfolder `.vscode`.\n",
    "- Run the cell below to create the folder `.vscode`.\n",
    "\n",
    "**Note**\n",
    "\n",
    "- This notebook doesn't dscribe the contents of these three files in detail. To learn more, visit: \n",
    "  - [task.json](https://code.visualstudio.com/docs/debugtest/tasks)\n",
    "  - [launch.json](https://code.visualstudio.com/docs/debugtest/debugging)\n",
    "  - [c_cpp_properties.json](https://code.visualstudio.com/docs/cpp/configure-intellisense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "034371f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\".vscode\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205a57b",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.4 Create the File `tasks.json`\n",
    "\n",
    "- Run the cell below to create the file `tasks.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12375bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "src_path = \"${workspaceFolder}/src/*.cu\"\n",
    "include_path = \"${workspaceFolder}/include\"\n",
    "bin_path = \"${workspaceFolder}/bin/main.exe\"\n",
    "if os_name == \"windows\":\n",
    "    src_path = \"${workspaceFolder}\\\\src\\\\main.cu ${workspaceFolder}\\\\src\\\\helloKernel.cu\"\n",
    "    include_path = \"${workspaceFolder}\\\\include\"\n",
    "    bin_path = \"${workspaceFolder}\\\\bin\\\\main.exe\"\n",
    "\n",
    "windows_shell_name = \"windows\"\n",
    "makedir_command = \"mkdir\"\n",
    "makedir_args = [\"-p\", \"src\", \"include\", \"bin\"]\n",
    "if os_name == \"windows\":\n",
    "    makedir_command = windows_shell_path\n",
    "    if windows_shell_name == \"powershell.exe\":\n",
    "        makedir_args = [\"-NoProfile\", \"-ExecutionPolicy\", \"Bypass\", \"-Command\", \"New-Item -ItemType Directory -Path 'src','include','bin' -Force -ErrorAction SilentlyContinue\"]\n",
    "    else:\n",
    "        makedir_args = [\"/c\", \"if not exist src mkdir src & if not exist include mkdir include & if not exist bin mkdir bin\"]\n",
    "\n",
    "clean_command = \"find\"\n",
    "clean_args = [\"./bin\", \"-type\", \"f\", \"-name\", \"*.exe\", \"-delete\"]\n",
    "if os_name == \"windows\":\n",
    "    clean_command = windows_shell_path\n",
    "    if windows_shell_name == \"powershell.exe\":\n",
    "        clean_args = [\"-NoProfile\", \"-ExecutionPolicy\", \"Bypass\", \"-Command\", \"Get-ChildItem -Path .\\\\bin -Include *.exe, *.ilk, *.pdb, *.obj -Recurse | Remove-Item -Force\"]\n",
    "    else:\n",
    "        clean_args = [\"/c\", \"del /s /q /f .\\\\bin\\\\*.exe 2>nul .\\\\bin\\\\*.ilk 2>nul .\\\\bin\\\\*.pdb 2>nul .\\\\bin\\\\*.obj pdb 2>nul\"]\n",
    "\n",
    "c_build_command = c_compiler_path\n",
    "c_build_multi_args = [\"-std=c++17\", \"-Wall\", \"-g\"] \n",
    "c_build_active_args = [\"-std=c++17\", \"-Wall\", \"-g\"]\n",
    "# c_build_multi_args = [\"-std=++c17\", \"-Wall\", \"-g\", src_path, \"-I\", include_path, \"-o\", bin_path] \n",
    "# c_build_active_args = [\"-std=++c17\", \"-Wall\", \"-g\", \"${file}\", \"-o\", bin_path]\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    c_build_multi_args = [\"/std:c++17\", \"/nologo\", \"/Zi\", \"/EHsc\"]\n",
    "    c_build_active_args = [\"/std:c++17\", \"/nologo\", \"/Zi\", \"/EHsc\"]\n",
    "    # c_build_multi_args = [\"/std:c++17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"src\\\\*.cu\", \"/I\", \"include\"]\n",
    "    # c_build_active_args = [\"/std:c++17\", \"/nologo\", \"/Zi\", \"/EHsc\", \"/Fe:bin\\\\main.exe\", \"/Fo:bin\\\\\", \"/Fd:bin\\\\\", \"${file}\"]\n",
    "\n",
    "c_compiler_dir = os.path.dirname(c_compiler_path)\n",
    "xcompiler_multi_args = \" \".join(c_build_multi_args)\n",
    "xcompiler_active_args = \" \".join(c_build_active_args)\n",
    "\n",
    "cuda_build_command = cuda_compiler_path\n",
    "cuda_build_multi_args = [\"-Wno-deprecated-gpu-targets\", \"-g\", \"-G\", src_path, \"-I\", include_path, \"-o\", bin_path, \"--compiler-bindir\", f'\"{c_compiler_dir}\"', \"-Xcompiler\", f'\"{xcompiler_multi_args}\"']\n",
    "cuda_build_active_args = [\"-Wno-deprecated-gpu-targets\", \"-g\", \"-G\", \"${file}\", \"-o\", bin_path, \"--compiler-bindir\", f'\"{c_compiler_dir}\"', \"-Xcompiler\", f'\"{xcompiler_active_args}\"']\n",
    "if os_name == \"windows\":\n",
    "    cuda_build_multi_args = [\"-Wno-deprecated-gpu-targets\", \"-g\", \"-G\", src_path, \"-I\", include_path, \"-o\", bin_path]\n",
    "    cuda_build_active_args = [\"-Wno-deprecated-gpu-targets\", \"-g\", \"-G\", \"${file}\", \"-o\", bin_path]\n",
    "\n",
    "tasks_json = {\n",
    "    \"version\": \"2.0.0\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"Make directories\",\n",
    "            \"command\": makedir_command,\n",
    "            \"args\": makedir_args,\n",
    "            \"problemMatcher\": []\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": \"Clean .exe files\",\n",
    "            \"dependsOn\": [\"Make directories\"],\n",
    "            \"command\": clean_command,\n",
    "            \"args\": clean_args,\n",
    "            \"problemMatcher\": []\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": f\"{cuda_compiler_name}: build multi file\",\n",
    "            \"dependsOn\": [\"Clean .exe files\"],\n",
    "            \"command\": cuda_build_command,\n",
    "            \"args\": cuda_build_multi_args,\n",
    "            \"options\": {\n",
    "                \"cwd\": \"${workspaceFolder}\"\n",
    "            },\n",
    "            \"problemMatcher\": [\n",
    "                \"$gcc\", \"$nvcc\"\n",
    "            ],\n",
    "            \"group\": {\n",
    "                \"kind\": \"build\",\n",
    "                \"isDefault\": False\n",
    "            },\n",
    "            \"detail\": f\"compiler: {cuda_compiler_path}\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"shell\",\n",
    "            \"label\": f\"{cuda_compiler_name}: build active file\",\n",
    "            \"dependsOn\": [\"Clean .exe files\"],\n",
    "            \"command\": cuda_build_command,\n",
    "            \"args\": cuda_build_active_args,\n",
    "            \"options\": {\n",
    "                \"cwd\": \"${fileDirname}\"\n",
    "            },\n",
    "            \"problemMatcher\": [\n",
    "                \"$gcc\", \"$nvcc\"\n",
    "            ],\n",
    "            \"group\": {\n",
    "                \"kind\": \"build\",\n",
    "                \"isDefault\": True\n",
    "            },\n",
    "            \"detail\": f\"compiler: {cuda_compiler_path}\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "json_string = json.dumps(tasks_json, indent=4)\n",
    "with open(\".vscode/tasks.json\", \"w\") as f:\n",
    "    json.dump(tasks_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88dea1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.5 Create the File `launch.json`\n",
    "\n",
    "- Run the cell below to create the file `launch.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70ff14b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "launch_json = {\n",
    "    \"version\": \"0.2.0\",\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": f\"{cuda_debugger_name}: launch multi file\",\n",
    "            \"preLaunchTask\": f\"{cuda_compiler_name}: build multi file\",\n",
    "            \"type\": \"cuda-gdb\",\n",
    "            \"request\": \"launch\",\n",
    "            \"program\": bin_path,\n",
    "            \"args\": [],\n",
    "            \"stopAtEntry\": False,\n",
    "            \"cwd\": \"${workspaceFolder}\",\n",
    "            \"environment\": []\n",
    "        },\n",
    "        {\n",
    "            \"name\": f\"{cuda_debugger_name}: launch active file\",\n",
    "            \"preLaunchTask\": f\"{cuda_compiler_name}: build active file\",\n",
    "            \"type\": \"cuda-gdb\",\n",
    "            \"request\": \"launch\",\n",
    "            \"program\": bin_path,\n",
    "            \"args\": [],\n",
    "            \"stopAtEntry\": False,\n",
    "            \"cwd\": \"${workspaceFolder}\",\n",
    "            \"environment\": []\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "with open(\".vscode/launch.json\", \"w\") as f:\n",
    "    json.dump(launch_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feec20f",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.6 Create the File `c_cpp_properties.json`\n",
    "\n",
    "- Run the cell below to create the file `c_cpp_properties.json` in subfolder `.vscode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86f2d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "if os_name == \"linux\" and c_compiler_name == \"gcc\":\n",
    "    intelliSenseMode = \"linux-gcc-x64\"\n",
    "    # intelliSenseMode = \"linux-gcc-arm64\"\n",
    "if os_name == \"linux\" and c_compiler_name == \"clang\":\n",
    "    intelliSenseMode = \"linux-clang-x64\"\n",
    "    # intelliSenseMode = \"linux-clang-arm64\"\n",
    "if os_name == \"osx\" and c_compiler_name == \"gcc\":\n",
    "    intelliSenseMode = \"macos-gcc-x64\"\n",
    "    # intelliSenseMode = \"macos-gcc-arm64\"\n",
    "if os_name == \"osx\" and c_compiler_name == \"clang\":\n",
    "    intelliSenseMode = \"macos-clang-x64\"\n",
    "    # intelliSenseMode = \"macos-clang-arm64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"gcc.exe\":\n",
    "    intelliSenseMode = \"windows-gcc-x64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"clang.exe\":\n",
    "    intelliSenseMode = \"windows-clang-x64\"\n",
    "if os_name == \"windows\" and c_compiler_name == \"cl.exe\":\n",
    "    intelliSenseMode = \"windows-msvc-x64\"\n",
    "\n",
    "launch_json = {\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": \"Linter\",\n",
    "            \"includePath\": [\n",
    "                \"${workspaceFolder}/**\"\n",
    "            ],\n",
    "            \"defines\": [],\n",
    "            \"compilerPath\": c_compiler_path,\n",
    "            \"cStandard\": \"c17\",\n",
    "            \"cppStandard\": \"c++17\",\n",
    "            \"intelliSenseMode\": intelliSenseMode\n",
    "        }\n",
    "    ],\n",
    "    \"version\": 4\n",
    "}\n",
    "\n",
    "os.makedirs(\".vscode\", exist_ok=True)\n",
    "with open(\".vscode/c_cpp_properties.json\", \"w\") as f:\n",
    "    json.dump(launch_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6828fd9",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.7 VSCode Extensions\n",
    "\n",
    "- To develop CUDA programs in C with VSCode, we need a few VSCode extensions (the last two are only needed for Jupyter Notebooks).\n",
    "  - C/C++ Extension Pack: https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools-extension-pack\n",
    "  - CodeLLDB: https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb\n",
    "  - Makefile Tools: https://marketplace.visualstudio.com/items?itemName=ms-vscode.makefile-tools\n",
    "  - Nsight Visual Studio Code Edition: https://marketplace.visualstudio.com/items?itemName=NVIDIA.nsight-vscode-edition\n",
    "  - Jupyter: https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter\n",
    "  - Python: https://marketplace.visualstudio.com/items?itemName=ms-python.python\n",
    "\n",
    "- Run the cell below to install any missing VSCode extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d66c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing extensions...\n",
      "Extension 'ms-vscode.cpptools-extension-pack' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'vadimcn.vscode-lldb' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-vscode.makefile-tools' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'nvidia.nsight-vscode-edition' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-toolsai.jupyter' is already installed.\n",
      "Installing extensions...\n",
      "Extension 'ms-python.python' is already installed.\n"
     ]
    }
   ],
   "source": [
    "!code --install-extension ms-vscode.cpptools-extension-pack --force\n",
    "!code --install-extension vadimcn.vscode-lldb --force\n",
    "!code --install-extension ms-vscode.makefile-tools --force\n",
    "!code --install-extension NVIDIA.nsight-vscode-edition --force\n",
    "!code --install-extension ms-toolsai.jupyter --force\n",
    "!code --install-extension ms-python.python --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950de2cb",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.8 Using Notebook Extension `nvcc4jupyter` and Cell Magic `%%cuda`\n",
    "\n",
    "- NVCC for Jupyter `nvcc4jupyter` is a Jupyter Notebook extension that let's us run CUDA code in notebook code cells.\n",
    "  - It's installed into a Python environment with: `pip install nvcc4jupyter`\n",
    "- It defines the cell magic `%%cuda` we can use at the beginning of a code cell, followed by CUDA code (in C or C++).\n",
    "- For each cell with the cell magic `%%cuda`, `nvcc4jupyter` will store the source code and compiled executable in a temporary folder.\n",
    "  - Below, we are remapping the temporary folder to the folder `./nvcc4jupyter` (so we can see what files are created).\n",
    "  - For each cell with the cell magic `%%cuda`, a randomly named subfolder will be created under `./nvcc4jupyter`, containing two files:\n",
    "    - `single_file.cu` which is the source code copied from the notebook cell.\n",
    "    - `cuda_exec.out` which is the compiled executable.\n",
    "\n",
    "### Create Folder for `nvcc4jupyter`\n",
    "\n",
    "- Run the cell below to create the folder `nvcc4jupyter` in the workspace folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d9ed4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "_fixed_temp_dir = os.path.abspath(\"./nvcc4jupyter\")\n",
    "if os.path.exists(_fixed_temp_dir):\n",
    "    shutil.rmtree(_fixed_temp_dir)\n",
    "os.makedirs(_fixed_temp_dir, exist_ok=True)\n",
    "tempfile.gettempdir = lambda: _fixed_temp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38748991",
   "metadata": {},
   "source": [
    "### Load `nvcc4jupyter` Notebook Extension\n",
    "\n",
    "- Jupyter Notebook extensions need to be loaded into the notebook with the cell magic `%load_ext` (or reloaded with `%reload_ext`).\n",
    "- Run the cell below to load the `nvcc4jupyter` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2cf6b135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nvcc4jupyter extension is already loaded. To reload it, use:\n",
      "  %reload_ext nvcc4jupyter\n",
      "Source files will be saved in \"/home/patrick/projects/cuda5/nvcc4jupyter/tmpvg8kmdji\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter\n",
    "%reload_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4669a7",
   "metadata": {},
   "source": [
    "### Use Cell Magic `%%cuda`\n",
    "\n",
    "- Run the cell below, containing a simple CUDA program written in C.\n",
    "  - Notice the code cell starts with the cell magic `%%cuda`, followed by a CUDA program written in C.\n",
    "  - The program uses `printf()` to print out text from both `host` code (runs on the CPU) and `device` code (runs on the GPU).\n",
    "- Then inspect the folder `./nvcc4jupyter`.\n",
    "  - It contains a subfolder with a random name, which contains two files:\n",
    "    - `single_file.cu` which contains the CUDA source code, i.e. code cell's contents (except for the cell magic command `%%cuda`).\n",
    "    - `cuda_exec.out` which is the executable file, compiled from the source code by NVCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "162f2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from host\n",
      "Hello from device -> block: 1, thread: 0\n",
      "Hello from device -> block: 1, thread: 1\n",
      "Hello from device -> block: 0, thread: 0\n",
      "Hello from device -> block: 0, thread: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void hello()\n",
    "{\n",
    "    // CUDA supports the printf() function on the device\n",
    "    printf(\"Hello from device -> block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    printf(\"Hello from host\\n\");\n",
    "    hello<<<2, 2>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca922521",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.9 Using Built-in Cell Magic `%%writefile`\n",
    "\n",
    "- The cell magic `%%writefile filename`, writes the contents of a notebook cell to the specified `filename` (or file path).\n",
    "  - This functionality is built-in to Jupyter Notebooks (it's not an extension).\n",
    "  - We can use it to write any code cell contents to a file in the file system.\n",
    "  - Let's write some CUDA code to the file `main.cu`.\n",
    "- Run the cell below.\n",
    "- Then inspect the resulting file `main.cu`, which contains the code cell's contents (except for the cell magic command `%%writefile filename`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "59f4f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void hello()\n",
    "{\n",
    "    // CUDA supports the printf() function on the device\n",
    "    printf(\"Hello from device -> block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    printf(\"Hello from host\\n\");\n",
    "    hello<<<2, 2>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae19c0",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.10 Compiling and Executing a CUDA Program From a Notebook Code Cell\n",
    "\n",
    "- We can compile and execute a CUDA program from a notebook code cell using the syntax `!<shell command>`, where:\n",
    "  - `!` indicates that the succeeding text on the same row should be sent to the shell (terminal).\n",
    "  - `<shell command>` is the shell (terminal) command we want to execute.\n",
    "  - Standard output is redirected to the cell output.\n",
    "\n",
    "- Run the cell below to see what the build command and execute command is in your shell:\n",
    "  - The *build single file* command compiles and links the file `main.cu` in your workspace folder and places the executable file `main.exe` in the `bin` folder.\n",
    "  - The *build multi file* command compiles and links all `.cu` files in the `src` and `.h` files in the `include` folder and places the executable file `main.exe` in the `bin` folder.\n",
    "  - The *execute* command executes the file `main.exe` in the `bin` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95fadcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build single file command : \"/usr/local/cuda/bin/nvcc\" -Wno-deprecated-gpu-targets -g -G main.cu -o ./bin/main.exe --compiler-bindir \"/usr/bin\" -Xcompiler \"-std=c++17 -Wall -g\"\n",
      "Build multi file command  : \"/usr/local/cuda/bin/nvcc\" -Wno-deprecated-gpu-targets -g -G ./src/*.cu -I ./include -o ./bin/main.exe --compiler-bindir \"/usr/bin\" -Xcompiler \"-std=c++17 -Wall -g\"\n",
      "Execute command           : ./bin/main.exe\n"
     ]
    }
   ],
   "source": [
    "#cuda_build_command = f'\"{cuda_build_command}\"'\n",
    "build_single_file_command = [f'\"{cuda_build_command}\"'] + cuda_build_active_args\n",
    "build_single_file_command = \" \".join(build_single_file_command).replace('${file}', 'main.cu').replace('${workspaceFolder}', '.')\n",
    "\n",
    "build_multi_file_command = [f'\"{cuda_build_command}\"'] + cuda_build_multi_args\n",
    "build_multi_file_command = \" \".join(build_multi_file_command).replace('${file}', 'main.cu').replace('${workspaceFolder}', '.')\n",
    "\n",
    "execute_command = bin_path.replace('${workspaceFolder}', '.')\n",
    "\n",
    "print(f'Build single file command : {build_single_file_command}')\n",
    "print(f'Build multi file command  : {build_multi_file_command}')\n",
    "print(f'Execute command           : {execute_command}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7298415",
   "metadata": {},
   "source": [
    "- Run the cell below to:\n",
    "  - Create the folder `bin` in your workspace folder if it doesn't already exist.\n",
    "  - Build the single source code file `main.cu` in your workspace foilder into the executable file `main.exe` in the `bin` folder.\n",
    "  - Run the executable file `main.exe` in the `bin` folder.\n",
    "- Notice the file `main.exe` has been created in the file system (in the `bin` folder), and the program's output is shown as the cell's output in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6645bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from host\n",
      "Hello from device -> block: 1, thread: 0\n",
      "Hello from device -> block: 1, thread: 1\n",
      "Hello from device -> block: 0, thread: 0\n",
      "Hello from device -> block: 0, thread: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"bin\", exist_ok=True)\n",
    "\n",
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68970095",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.11 Compiling and Debugging a Single-file CUDA Program\n",
    "\n",
    "Let's see `tasks.json` and `launch.json` in action for a single-file (`.cu`) C program.\n",
    "\n",
    "- First, let's create the file `main.cu` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7574216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void hello()\n",
    "{\n",
    "    // CUDA supports the printf() function on the device\n",
    "    printf(\"Hello from device -> block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    printf(\"Hello from host\\n\");\n",
    "    hello<<<2, 2>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f9476",
   "metadata": {},
   "source": [
    "**NOTE! YOU NEED TO BE USING GCC AS YOUR COMPILER ON LINUX TO DEBUG CUDA PROGRAMS IN VSCODE!**\n",
    "\n",
    "- Now, let's debug the file `main.cu`.\n",
    "  - Open the file `main.cu` in VSCode's editor.\n",
    "  - Set a breakpoint on the two `printf()` statements (`F9`).\n",
    "  - Switch to the `Run and Debug` view (Linux/Windows: `Ctrl + Shift + D`, Mac: `Cmd + Shift + D`).\n",
    "  - In the drop-down combobox, select the launch configuration `cuda-dbg: launch active file`.\n",
    "  - Click the green `Play` icon.\n",
    "  - Use the debug toolbar in the top-middle of VSCode to debug the code.\n",
    "    - Notice the debugger stops at the two breakpoints (both in the host code and the device code).\n",
    "      - This is because we are using `cuda-dbg`as the debugger.\n",
    "    - Notice you can view variables (local, registers), watch variables, view the call stack, and toggle breakpoints in the `Run and Debug` view.\n",
    "  - Stop debugging (red `Square` icon in the debug toolbar).\n",
    "- Now look at the status bar (at the bottom of VSCode) where you will see the name of the launch configuration `cuda-gdb: launch active file`.\n",
    "  - Click on it, and select `cuda-gdb: launch active file` again (make sure `main.cu` is the active file in the editor, not the notebook).\n",
    "    - This is an alternative method to start a debug session.\n",
    "  - Stop debugging.\n",
    "- Press `F5` (make sure `main.cu` is active in VSCode's editor), which is a third alternative to launch the debugger.\n",
    "  - This launches the debug configuration with `preLaunchTask` set to the default task (in `tasks.json`).\n",
    "  - Stop debugging.\n",
    "- Press (Linux/Windows: `Ctrl + Shift + B`, Mac: `Ctrl + Shift + B`) to execute the default build task (in `tasks.json`).\n",
    "  - Make sure `main.cu` is active in VSCode's editor (since the default build task is set to the active file task).\n",
    "  - Notice the compiled executable `main.exe` is placed in the subfolder `bin` (configured in the default build task).\n",
    "    - This is also where the debugger finds the executable `main.exe` (configured in `launch.json`).\n",
    "\n",
    "- Remeber, you can always compile a single `main.cu` file in your workspace folder and run it using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d05101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from host\n",
      "Hello from device -> block: 1, thread: 0\n",
      "Hello from device -> block: 1, thread: 1\n",
      "Hello from device -> block: 0, thread: 0\n",
      "Hello from device -> block: 0, thread: 1\n"
     ]
    }
   ],
   "source": [
    "!{build_single_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3a935",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.12 Compiling and Debugging a Multi-file CUDA Program\n",
    "\n",
    "- Let's see `tasks.json` and `launch.json` in action for a multi-file (`*.cu`) CUDA program.\n",
    "  - First We will create two source code files `.cu` in the `src` folder, and one header file `.h` in the `include` folder.\n",
    "    - We will use the same code as before, but will place the CUDA kernel (function) code in its own `.cu` file and its prototype in a header file `.h`.\n",
    "  - Then we will use:\n",
    "    - The other (non-default) build task in `tasks.json` to build the executable.\n",
    "    - The other launch configuration (linked to the non-default build task) in `launch.json` to debug it.\n",
    "\n",
    "- Run the four cells below to create:\n",
    "  - The folder structure `src`, `include`, and `bin` (if it hasn't already been created).\n",
    "  - The main source code file `main.cu` in the folder `src`.\n",
    "  - The kernel source code file `helloKernel.cu` in the folder `src`.\n",
    "  - The kernel prototype in the header file `helloKernel.h` in the folder `include`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84d878c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "os.makedirs(\"include\", exist_ok=True)\n",
    "os.makedirs(\"bin\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf9a2053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/main.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/main.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include \"helloKernel.h\"\n",
    "\n",
    "int main()\n",
    "{\n",
    "    printf(\"Hello from host\\n\");\n",
    "    hello<<<2, 2>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9363d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/helloKernel.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/helloKernel.cu\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include \"helloKernel.h\"\n",
    "\n",
    "__global__ void hello()\n",
    "{\n",
    "    // CUDA supports the printf() function on the device\n",
    "    printf(\"Hello from device -> block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b21adef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing include/helloKernel.h\n"
     ]
    }
   ],
   "source": [
    "%%writefile include/helloKernel.h\n",
    "#pragma once\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void hello();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e163c6",
   "metadata": {},
   "source": [
    "**NOTE! YOU NEED TO BE USING GCC AS YOUR COMPILER ON LINUX TO DEBUG CUDA PROGRAMS IN VSCODE!**\n",
    "\n",
    "- Build the multi-file CUDA program.\n",
    "  - Notice the multi-file build task in `tasks.json` isn't the default build task (`isDefault` is not set to `true` under `group`).\n",
    "  - Therefore, we can't use (Linux/Windows: `Ctrl + Shift + B`, mac: `Cmd + Shift + B`).\n",
    "  - Instead we can:\n",
    "    - Bring up the Command Palette (Linux/Windows: `Ctrl + Shift + P`, mac: `Cmd + Shift + P`).\n",
    "    - Choose `Tasks: Run Task` and select the task `nvcc: build multi file`.\n",
    "  - The executable `main.exe` is placed in the `bin` folder (as configured in `tasks.json`).\n",
    "- Debug the multi-file CUDA program.\n",
    "  - Open `main.cu` and `addKernel.cu` in the `src` folder and set breakpoints on the two `printf()` statements.\n",
    "  - Notice the multi-file launch task in `launch.json` isn't linked to the default build task (in `tasks.json`).\n",
    "    - Therefore, we can't use `F5`.\n",
    "    - Instead we can:\n",
    "      - Switch the the `Run and Debug` view, select `cuda-dbg: launch multi file` from the drop-down list, and click the green `Play` icon.\n",
    "      - Or select `cuda-dbg: launch multi file` from the status bar (at the bottom of VSCode).\n",
    "    - The CUDA program is built and the debugger lauched, attaching to the executable `main.exe` in the `bin` folder.\n",
    "  - Stop debugging.\n",
    "- Remeber, you can always compile a multi-file CUDA program (`src/*.cu`, `include/*.h`) and run it using the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3aae85f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from host\n",
      "Hello from device -> block: 1, thread: 0\n",
      "Hello from device -> block: 1, thread: 1\n",
      "Hello from device -> block: 0, thread: 0\n",
      "Hello from device -> block: 0, thread: 1\n"
     ]
    }
   ],
   "source": [
    "!{build_multi_file_command}\n",
    "!{execute_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e536e",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.13 Using NVIDIA NSight Compute (`ncu`)\n",
    "\n",
    "NVIDIA Nsight Compute (`ncu`) is a CUDA kernel profiler (command-line tool) provided by NVIDIA. It's used to analyze and optimize the performance of CUDA applications by collecting low-level performance metrics from the GPU.\n",
    "\n",
    "- NVidia's documentation is a good source for detailed information about its tools:\n",
    "  - [Nsight Compute CLI](https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html)\n",
    "  - [Nsight Developer Tools](https://docs.nvidia.com/nsight-developer-tools/index.html)\n",
    "- In NVIDIA Nsight Compute, there is:\n",
    "  - A command-line tool : `ncu`\n",
    "  - An application with a graphical user interface: `ncu-ui`\n",
    "    - On Windows, open the start menu and search for `Nsight Compute`.\n",
    "- To profile our executable file `main.exe` in the folder `bin` using `ncu`, we can run the command:\n",
    "  \n",
    "  ```bash\n",
    "  ncu bin/main.exe\n",
    "  ```\n",
    "- To create a profile report and open it in the graphical user interface, we can run the commands below:\n",
    "  - Note that compiling the source code with the flag `-lineinfo` makes line number available in `ncu-ui`.\n",
    "\n",
    "  ```bash\n",
    "  nvcc -lineinfo -o bin/main.exe main.cu\n",
    "  ncu -o profile_report bin/main.exe\n",
    "  ncu-ui profile_report.ncu-rep\n",
    "  ```\n",
    "- Let's try the `ncu` tool.\n",
    "  - Open a terminal and run the command below\n",
    "    \n",
    "    ```bash\n",
    "    ncu bin/main.exe\n",
    "    ```\n",
    "\n",
    "**Note**\n",
    "\n",
    "- If you get the message: `The user does not have permission to access NVIDIA GPU Performance Counters on the target device 0. For instructions on enabling permissions and to get more information see https://developer.nvidia.com/ERR_NVGPUCTRPERM`.\n",
    "- This means you need to run `ncu` as an administrator on your computer\n",
    "- See: https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters\n",
    "  - On Windows, you need to run `ncu` from a terminal with administrative rights.\n",
    "  - On Linux and Mac, you need to use `sudo ncu`.\n",
    "  - Depending on your Linux distribution, you can also enable anyone to use `ncu` without `sudo ncu`, e.g. for Ubuntu (or other Debian-based ditributions):\n",
    "      \n",
    "    ```bash\n",
    "    echo \"options nvidia NVreg_RestrictProfilingToAdminUsers=0\" | sudo tee /etc/modprobe.d/nvidia-profiler.conf\n",
    "    sudo update-initramfs -u\n",
    "    sudo reboot\n",
    "    ```\n",
    "- You can run `ncu` in a notebook cell, only if you have administrator rights (Windows), or don't have to use `sudo ncu` (Linux/Mac).\n",
    "  - Otherwise the notebook cell will \"hang\", waiting on authorization (a password in Linux).\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "**The output from `ncu` will look something like below which contains important information regarding optimizing CUDA code ...**\n",
    "\n",
    "```bash\n",
    "==PROF== Disconnected from process 12856\n",
    "[12856] main.exe@127.0.0.1\n",
    "  hello() (2, 1, 1)x(2, 1, 1), Context 1, Stream 7, Device 0, CC 8.9\n",
    "    Section: GPU Speed Of Light Throughput\n",
    "    ----------------------- ------------- ------------\n",
    "    Metric Name               Metric Unit Metric Value\n",
    "    ----------------------- ------------- ------------\n",
    "    DRAM Frequency          cycle/nsecond         6.51\n",
    "    SM Frequency            cycle/usecond       866.15\n",
    "    Elapsed Cycles                  cycle       93,159\n",
    "    Memory Throughput                   %         0.29\n",
    "    DRAM Throughput                     %         0.00\n",
    "    Duration                      usecond       107.55\n",
    "    L1/TEX Cache Throughput             %         2.37\n",
    "    L2 Cache Throughput                 %         0.29\n",
    "    SM Active Cycles                cycle     7,646.04\n",
    "    Compute (SM) Throughput             %         0.19\n",
    "    ----------------------- ------------- ------------\n",
    "\n",
    "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
    "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
    "\n",
    "    Section: Launch Statistics\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Metric Name                          Metric Unit    Metric Value\n",
    "    -------------------------------- --------------- ---------------\n",
    "    Block Size                                                     2\n",
    "    Function Cache Configuration                     CachePreferNone\n",
    "    Grid Size                                                      2\n",
    "    Registers Per Thread             register/thread              32\n",
    "    Shared Memory Configuration Size           Kbyte           32.77\n",
    "    Driver Shared Memory Per Block       Kbyte/block            1.02\n",
    "    Dynamic Shared Memory Per Block       byte/block               0\n",
    "    Static Shared Memory Per Block        byte/block               0\n",
    "    Threads                                   thread               4\n",
    "    Waves Per SM                                                0.00\n",
    "    -------------------------------- --------------- ---------------\n",
    "\n",
    "    WRN   Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 2      \n",
    "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
    "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
    "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
    "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
    "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
    "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
    "          details on launch configurations.                                                                             \n",
    "    ----- --------------------------------------------------------------------------------------------------------------\n",
    "    WRN   The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 24              \n",
    "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
    "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
    "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
    "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
    "          description for more details on launch configurations.                                                        \n",
    "\n",
    "    Section: Occupancy\n",
    "    ------------------------------- ----------- ------------\n",
    "    Metric Name                     Metric Unit Metric Value\n",
    "    ------------------------------- ----------- ------------\n",
    "    Block Limit SM                        block           24\n",
    "    Block Limit Registers                 block           64\n",
    "    Block Limit Shared Mem                block           32\n",
    "    Block Limit Warps                     block           48\n",
    "    Theoretical Active Warps per SM        warp           24\n",
    "    Theoretical Occupancy                     %           50\n",
    "    Achieved Occupancy                        %         2.08\n",
    "    Achieved Active Warps Per SM           warp            1\n",
    "    ------------------------------- ----------- ------------\n",
    "\n",
    "    WRN   This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM. The    \n",
    "          difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the result    \n",
    "          of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur    \n",
    "          between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide   \n",
    "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
    "          optimizing occupancy.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa3231",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. CUDA Basics\n",
    "---\n",
    "\n",
    "- Now we know how to create CUDA programs in C (single-file, multi-file, and in a Jupyter Notebook cell).\n",
    "- Going forward, we will explore fundamental CUDA programming concepts as single-file programs in notebook cells using the `%%cuda` cell magic command.\n",
    "  - The code in each `%%cuda` cell can be placed in a `main.cu` file by:\n",
    "    - Manually copying the cell contents and removing the `%%cuda` row.\n",
    "    - Replacing the `%%cuda` row with `%%writefile main.cu` and running the cell.\n",
    "  - Then you can manually compile it to `main.exe` with `nvcc main.cu -o main.exe`.\n",
    "- NVidia's documentation is a good source to learn more about CUDA:\n",
    "    - [Cuda C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/contents.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a988e3b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.1 Listing CUDA-enabled Devices and Properties\n",
    "\n",
    "- First, let's find out what CUDA-enabled devices are available on your computer.\n",
    "\n",
    "### Using `nvidia-smi`\n",
    "\n",
    "- The simplest way to list CUDA-enabled devices is using the tool `nvidia-smi`.\n",
    "  - The top row shows you:\n",
    "    - The `nvidia-smi version` (left).\n",
    "    - The NVidia GPU `driver version` (middle).\n",
    "    - The supported `CUDA runtime version` (right).\n",
    "  - It also shows you:\n",
    "    - Information about your CUDA-enabled devices (beneath the top row).\n",
    "    - What processes are running on your CUDA-enabled devices (bottom).\n",
    "- Run the cell below to see the CUDA-enabled devices on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7af0c17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 13 17:58:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 2000 Ada Gene...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   52C    P3             10W /   35W |       1MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326dc763",
   "metadata": {},
   "source": [
    "- Let's also check the NVidia Cuda Compiler (NVCC) version by running the cell below.\n",
    "  - Notice the `nvcc version` in the two last rows (release and build).\n",
    "  - Now compare the `nvcc version` with the supported `CUDA runtime version` above (top right output from `nvidia-smi`).\n",
    "    - The `CUDA runtime version` is the **maximum** supported CUDA runtime version.\n",
    "    - The `nvcc version` is the same are the CUDA Toolkit (SDK) version.\n",
    "    - **The `nvcc version` must be less than or equal to the `CUDA runtime version`**.\n",
    "      - Otherwise your CUDA programs in C won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ad33a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:23:50_PST_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cc78b",
   "metadata": {},
   "source": [
    "### Using C Code\n",
    "\n",
    "- We can also find out what CUDA-enabled devices are available using C code.\n",
    "  - The code below lists important properties that will become familiar the more you learn about CUDA (for optimization purposes).\n",
    "- Run the cell below to list CUDA-enabled devices and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "394e2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nvidia Devices:\n",
      "\n",
      "- Device 0: NVIDIA RTX 2000 Ada Generation Laptop GPU\n",
      "  - Compute capability              : 8.9\n",
      "  - Streaming Multiprocessors (SMs) : 24\n",
      "  - Streaming Processors (SPs)      : 3072\n",
      "  - Max threads per SM              : 1536\n",
      "  - Max threads per block           : 1024\n",
      "  - Warp size                       : 32\n",
      "  - Registers per block             : 65536\n",
      "  - Register file size per SM       : 262144 bytes\n",
      "  - Shared memory per SM            : 102400 bytes\n",
      "  - Shared memory per block         : 49152 bytes\n",
      "  - Constant memory size            : 65536 bytes\n",
      "  - Global memory size              : 8198619136 bytes\n",
      "  - Clock rate                      : 1455.00 MHz\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "int _ConvertSMVer2Cores(int major, int minor)\n",
    "{\n",
    "    typedef struct { int SM; int Cores; } SMtoCores;\n",
    "    SMtoCores map[] = {\n",
    "        {0x30, 192}, {0x32, 192}, {0x35, 192}, {0x37, 192},  // Kepler\n",
    "        {0x50, 128}, {0x52, 128}, {0x53, 128},               // Maxwell\n",
    "        {0x60,  64}, {0x61, 128}, {0x62, 128},               // Pascal\n",
    "        {0x70,  64}, {0x72,  64}, {0x75,  64},               // Volta/Turing\n",
    "        {0x80,  64}, {0x86, 128},                            // Ampere\n",
    "        {0x89, 128}, {0x90, 128},                            // Ada/Hopper\n",
    "        {-1, -1}\n",
    "    };\n",
    "\n",
    "    int sm = ((major << 4) + minor);\n",
    "    for (int i = 0; map[i].SM != -1; ++i)\n",
    "    {\n",
    "        if (map[i].SM == sm)\n",
    "            return map[i].Cores;\n",
    "    }\n",
    "    return -1;  // Unknown\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "\n",
    "    puts(\"Nvidia Devices:\\n\");\n",
    "    for (int i = 0; i < deviceCount; i++)\n",
    "    {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, i);\n",
    "\n",
    "        printf(\"- Device %d: %s\\n\", i, prop.name);\n",
    "        printf(\"  - %-31s : %d.%d\\n\", \"Compute capability\", prop.major, prop.minor);\n",
    "        printf(\"  - %-31s : %d\\n\", \"Streaming Multiprocessors (SMs)\", prop.multiProcessorCount);\n",
    "        printf(\"  - %-31s : %d\\n\", \"Streaming Processors (SPs)\", prop.multiProcessorCount * _ConvertSMVer2Cores(prop.major, prop.minor));\n",
    "        printf(\"  - %-31s : %d\\n\", \"Max threads per SM\", prop.maxThreadsPerMultiProcessor);\n",
    "        printf(\"  - %-31s : %d\\n\", \"Max threads per block\", prop.maxThreadsPerBlock);\n",
    "        printf(\"  - %-31s : %d\\n\", \"Warp size\", prop.warpSize);\n",
    "        printf(\"  - %-31s : %d\\n\", \"Registers per block\", prop.regsPerBlock);\n",
    "        printf(\"  - %-31s : %lu bytes\\n\", \"Register file size per SM\", prop.regsPerBlock * 4);\n",
    "        printf(\"  - %-31s : %lu bytes\\n\", \"Shared memory per SM\", prop.sharedMemPerMultiprocessor);\n",
    "        printf(\"  - %-31s : %lu bytes\\n\", \"Shared memory per block\", prop.sharedMemPerBlock);\n",
    "        printf(\"  - %-31s : %lu bytes\\n\", \"Constant memory size\", prop.totalConstMem);\n",
    "        printf(\"  - %-31s : %lu bytes\\n\", \"Global memory size\", prop.totalGlobalMem);\n",
    "        printf(\"  - %-31s : %.2f MHz\\n\", \"Clock rate\", prop.clockRate / 1000.0);\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    // Sets device 0 as the current device\n",
    "    //cudaSetDevice(0);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cd8e0",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Hello World in Host Code (CPU)\n",
    "\n",
    "- The program in the cell below is a simple C program (no CUDA code) that runs on the host (CPU).\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`.\n",
    "\n",
    "      ```c\n",
    "      #include <stdio.h>\n",
    "      ```\n",
    "  - Then we define the `main()` function:\n",
    "    - We print out the text `Hello World!`.\n",
    "    - Then we return the exit code `0` to the operating system.\n",
    "    \n",
    "      ```c\n",
    "      int main(void)\n",
    "      {\n",
    "        printf(\"Hello World!\\n\");\n",
    "        \n",
    "        return 0;\n",
    "      }\n",
    "      ```\n",
    "- Run the cell below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d0042a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "// Host entry point (a normal C main function)\n",
    "int main(void)\n",
    "{\n",
    "   printf(\"Hello World!\\n\");\n",
    "   \n",
    "   return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763d82e",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Hello World in Device Code (GPU)\n",
    "\n",
    "- The program in the cell below is a simple CUDA program that runs code on the host (CPU) and the device (GPU).\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`.\n",
    "    - `cuda_runtime.h` for `cudaDeviceSynchronize`.\n",
    "\n",
    "      ```c\n",
    "      #include <stdio.h>\n",
    "      #include <cuda_runtime.h> // contains CUDA function prototypes\n",
    "      ```\n",
    "  - Then we define a CUDA kernel function:\n",
    "    - `mykernel()` is the name of the **kernel function** (it can be any name we like).\n",
    "    - A **kernel function** runs on the device (GPU) and is **launched** (called) from the host (CPU).\n",
    "    - `__global__` is a CUDA **qualifier** (qualifying a function with `__global__` makes the function a **kernel function**.\n",
    "    - The kernel function takes no arguments (`void`), prints the text `Hello World!`, and does NOT return a value (`void`).\n",
    "    - All **kernel functions** MUST have a `void` return type.\n",
    "\n",
    "      ```c\n",
    "      __global__ void mykernel(void) // the __global__ qualifier makes this a kernel function\n",
    "      {\n",
    "          printf(\"Hello World!\\n\");\n",
    "      }\n",
    "      ```\n",
    "  - Lastly we define the `main()` function:\n",
    "    - First we **launch** (call) the **kernel function** `mykernel()` using the syntax `kernel<<<n_blocks, n_threads>>>()`.\n",
    "      - `kernel` is the name of the **kernel function** (`mykernel` in our code), which is called from the host (CPU) and runs on the device (GPU).\n",
    "      - `<<<n_blocks, n_threads>>>` is a CUDA **launch configuration** (`<<<1, 1>>>` in our code), which makes this a **kernel launch** (kernel function call).\n",
    "      - `n_blocks` is the number of `blocks` to use when launching the kernel function.\n",
    "        - In Nvidia terminology, a CUDA-enabled GPU contains a `grid` of `blocks`, where each `block` contains a number of `threads`.\n",
    "      - `n_threads` is the number of `threads` to use when launching the kernel function.\n",
    "        - Each `block` contains this number of `threads`.\n",
    "      - The code `mykernel<<<1,1>>>()` launches the kernel (calls the function) `mykernel()` on the GPU with 1 `block` containing 1 `thread` in that `block`.\n",
    "        - It's the *equivalent* of creating 1 new thread and running the function on that thread in a traditional C program running on the host (CPU).\n",
    "        - The kernel launch is an asynchronous function call, so it immediately returns control to the host (CPU).\n",
    "    - Then we call the CUDA function `cudaDeviceSynchronize()`.\n",
    "      - This function call is a synchronous call, which blocks the host's (CPU's) main thread until the kernel function completes (returns) on the device (GPU).\n",
    "      - This is necessary, otherwise the `main()` function would go out of scope (terminate) before we have a chance to retrieve any results from the kernel function.\n",
    "    - Finally we return the exit code `0` to the operating system.\n",
    "    \n",
    "      ```c\n",
    "      int main(void)\n",
    "      {\n",
    "        mykernel<<<1, 1>>>();    // kernel launch (calls the kernel function mykernel, and is asynchronous)\n",
    "        cudaDeviceSynchronize(); // blocks the host's main thread (CPU) until the kernel function completes (returns)\n",
    "        \n",
    "        return 0;\n",
    "      }\n",
    "      ```\n",
    "- Run the cell below to see the output.\n",
    "\n",
    "**TL;DR**\n",
    "\n",
    "- The CUDA function prototypes are declared in header file `cuda_runtime.h`.\n",
    "- A function with the `__global__` qualifier is called a **kernel function** that **runs on the device (GPU)** and is **called from the host (CPU)**.\n",
    "- A **kernel launch** calls a **kernel function** using the syntax `kernelfunction<<<n_blocks, n_threads>>>()` and is an **asynchrounous call**.\n",
    "- The function `cudaDeviceSynchronize()` is a **synchronous** call that blocks the host code until the **kernel function** is complete (returns).\n",
    "- The Nvidia compiler `nvcc` separates source code into **host** and **device** components.\n",
    "  - Device functions (e.g. `mykernel()`) is processed by the NVIDIA compiler `nvcc`.\n",
    "  - Host functions (e.g. `main()`) are processed by a standard host C compiler (e.g. `gcc`, `clang`, or `cl.exe`).\n",
    "    - NVCC instructs the underlying C compiler to compile host code during the compilation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d5e242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void mykernel(void)\n",
    "{\n",
    "    printf(\"Hello World!\\n\");\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "   mykernel<<<1, 1>>>();\n",
    "   cudaDeviceSynchronize();\n",
    "   \n",
    "   return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d198326",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.4 Grids, Blocks, Threads, Devices, SMs, and SPs\n",
    "\n",
    "- In the previous CUDA code, we saw:\n",
    "  - A CUDA **kernel**, where:\n",
    "    - `__global__` is a CUDA-specific qualifier that makes a function a **kernel function**.\n",
    "      - A **kernel function** is executed on the host (GPU), and called from the host (GPU).\n",
    "    - `kernel` is the name of the **kernel function**.\n",
    "    - `parameterlist` is the kernel function's comma-separated list of parameters.\n",
    "\n",
    "      ```c\n",
    "      __global__ kernel(parameterlist)\n",
    "      {\n",
    "\n",
    "      }\n",
    "      ```\n",
    "  - A CUDA **kernel launch**, where:\n",
    "    - `kernel` is the name of the **kernel** function.\n",
    "    - `<<<  >>>` is a special syntax used to **launch** (call) a **kernel function** and contains **launch parameters**.\n",
    "    - `blocks` is a **launch parameter** with the number of **blocks per grid**.\n",
    "    - `threads` is a **launch parameter** with the number of **threads per block**.\n",
    "    - `argumentlist` are the arguments passed to the kernel function (and must match its parameterlist above).\n",
    "  \n",
    "      ```bash\n",
    "      kernel<<<blocks, threads>>>(argumentlist);\n",
    "      ```\n",
    "\n",
    "<img src=\"images/cuda.png\" width=\"500\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "- A CUDA-enabled GPU:\n",
    "  - Is called a `Device`.\n",
    "  - Has a number of `Streaming Multiprocessors (SMs)`.\n",
    "  - Each `SM` has a number of `Streaming Processors (SPs)`.\n",
    "- During a **kernel launch** `kernel<<<blocks, threads>>>(argumentlist)`:\n",
    "  - CUDA lauches a `grid`. \n",
    "  - A `grid` contains a number of `blocks` (specified as `blocks` within `<<<blocks, threads>>>`).\n",
    "  - Each `block` contains a number of `threads` (specified as `threads` within `<<<blocks, threads>>>`).\n",
    "- CUDA maps:\n",
    "  - A `block` to run on an `SM` (multiple `blocks` can be assigned to the same `SM`).\n",
    "  - Each `thread` within a `block` to an `SP`.\n",
    "- So, a `block` runs on an `SM`, and each `thread` within that `block` runs on a `SP` within that `SM`.\n",
    "\n",
    "- For each **kernel launch**:\n",
    "  - CUDA creates a `grid` containing `blocks` containing `threads`.\n",
    "  - Each `thread` runs a copy of the same `kernel` function with the same `argumentlist`.\n",
    "  - 4 CUDA-specific global variables are available within each copy of the `kernel` function:\n",
    "    - `gridDim` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is the number of `blocks` in a `grid`, and can be specified as a 1D (`x`), 2D (`y`), or 3D (`z`) set of `blocks`.\n",
    "    - `blockDim` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is the number of `threads` in a `block`, and can be specified as a 1D (`x`), 2D (`y`), or 3D (`z`) set of `threads`.\n",
    "    - `blockIdx` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is unique ID of a `block` within the `grid`, and has a 1D (`x`), 2D (`y`), and 3D (`z`) ID.\n",
    "    - `threadIdx` of type `dim3`, which is a `struct` with three member variables `int x`, `int y`, and `int z`.\n",
    "      - This is unique ID of a `thread` within a `block`, and has a 1D (`x`), 2D (`y`), and 3D (`z`) ID.\n",
    "\n",
    "      ```c\n",
    "      typedef struct\n",
    "      {\n",
    "        int x;\n",
    "        int y;\n",
    "        int z;\n",
    "      } dim3;\n",
    "      ```\n",
    "    \n",
    "  - In the figure, we see that each `SM` contains:\n",
    "    - A `register file` (the blue rectangle).\n",
    "      - The `register file` is divided into `**chunks**, where each `thread` (running in the `block` assigned to the `SM`) is assigned one **chunk**.\n",
    "      - Each `thread`'s chunk of the `register file` is referred to as the `thread`'s `private memory` (only that `thread` can access that `private memory`).\n",
    "      - In a `kernel` function, we can declare a local variable with the qualifier `__private__` which stores that variable in a `thread`'s `private memory`.\n",
    "        - If we don't specify a qualifier, the variable is by default stored in the `thread`'s `private memory`.\n",
    "    - A `shared memory` buffer (the green rectangle).\n",
    "      - The `shared memory` is shared by all `threads` running in the `block` assigned to that `SM`.\n",
    "      - To store a variable in `shared memory`, we declare the variable with the `__shared__` qualifier.\n",
    "  - There is also `global memory`, which is declared using the qualifier `__global__`.\n",
    "    - If we don't explicitly qualify a parameter in the kernel function's parameterlist with a qualifier, it is by default `__global__`, i.e. referring to `global memory`.\n",
    "  - Finally, there is `constant memory` stored outside an `SM` (just like `global memory`), and is `read-only` memory (it can we written to once before a kernel launch).\n",
    "    - `constant memory` is small, but very effient (the CUDA compiler can optimize access to it since it is `read-only`).\n",
    "\n",
    "**TL;DR**\n",
    "- A CUDA-enabled GPU is referred to as a `device`, and has an array of `SMs`, each with a number of `SPs`.\n",
    "- A kernel launch specifies the number of `blocks` and `threads` to use within `<<<blocks, threads>>>`.\n",
    "- CUDA maps a `block` to an `SM`.\n",
    "- Each `thread` within a `block` is run on an `SP` within that `SM`.\n",
    "- A CUDA-enabled GPU has `global memory` and `constant memory` located outside of any `SM`.\n",
    "- Each `SM` has `shared memory` (shared by all `threads` running in the `block` assigned to an `SM`).\n",
    "- Each `SM` has a `register file`, divided into chunks, where each `thread` is a assigned a chunk referred to as `private memory` (private to that `thread`).\n",
    "- Each `thread` runs a **copy** of the same `kernel` function with the same `argumentlist`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452d38f",
   "metadata": {},
   "source": [
    "### Code Demonstrating Grids, Blocks, and Threads\n",
    "\n",
    "- Let's write a CUDA program that demonstrates running a `grid` of `blocks`, each with a number of `threads`, on the device (GPU).\n",
    "- The program copies the elements from one 1D `int` array `input` to another 1D `int` array `output`.\n",
    "  - We include the necessary header files:\n",
    "    - `stdio.h` for `printf`\n",
    "    - `stdlib.h` for `malloc` and `free`\n",
    "    - `time.h` for `srand` and `rand`\n",
    "    - `cuda_runtime.h` for `cudaMalloc`, `cudaMemcpy`, and `cudaFree`\n",
    "    \n",
    "    ```c\n",
    "    #include <stdio.h>\n",
    "    #include <stdlib.h>\n",
    "    #include <time.h>\n",
    "    #include <cuda_runtime.h>\n",
    "    ```\n",
    "  \n",
    "  - We define symbolic constants:\n",
    "    - `N` with the value `5` for the number of elements in each array\n",
    "    - `THREADS_PER_BLOCK_X` with the value `2` for the number of `threads` in each `block`\n",
    "\n",
    "    ```c\n",
    "    #define N 5\n",
    "    #define THREADS_PER_BLOCK_X 2\n",
    "    ```\n",
    "  \n",
    "  - We define a kernel function called `kernel`.\n",
    "    - It has the `__global__` qualifier, making it a kernel function, returns `void`, and has three parameters:\n",
    "      - An `int *` pointer `input` which points to the `input` array in the device's `global memory`.\n",
    "      - An `int *` pointer `output` which points to the `output` array in the device's `global memory`.\n",
    "      - An `int` variable `n` with the number of elements in each array.\n",
    "    - In the function's body, we:\n",
    "      - Use the CUDA-specific global variables `gridDim`, `blockDim`, `blockIdx`, and `threadIdx`, all of type `dim3` (with `int` member variables `x`, `y`, `z`).\n",
    "        - CUDA sets these as global variables, behind the scenes, during a kernel launch.\n",
    "        - They are available to each thread within each copy of the kernel function.\n",
    "        - `gridDim.x` is the number of `blocks` in a `grid`.\n",
    "          - In our case we have `(5 + 2 - 1) / 2 = 3` since we calculate it as `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X` in the `main` function.\n",
    "        - `blockDim.x` is the number of `threads` in a `block`.\n",
    "          - In our case we have `2`, defined by `THREADS_PER_BLOCK_X`.\n",
    "        - `blockIdx.x` is the unique `ID` of a `block` within the `grid`.\n",
    "          - Since `gridDim.x` is `3`, `blockIdx.x` ranges from `0` to `2` (zero-based, i.e. from `0` to `gridDim.x - 1`).\n",
    "        - `threadIdx.x` is the unique `ID` of a `thread` within a `block`.\n",
    "          - Since `blockDim.x` is `2`, `threadIdx.x` ranges from `0` to `1` (zero-based, i.e. from `0` to `blockDim.x.x - 1`).\n",
    "      - We calculate the global ID for a `thread` as `int idx = threadIdx.x + blockIdx.x * blockDim.x`.\n",
    "        - We have `gridDim.x * blockDim.x = 3 * 2 = 6` threads in total, so `idx` ranges from `0` to `5` (zero-based).\n",
    "      - We print out the values of `gridDim.x`, `blockDim.x`, `blockIdx.x`, `threadIdx.x`, and `idx`.\n",
    "        - This shows us which `thread` is running this specific copy of the kernel function, which `block` it is in, etc.\n",
    "      - We have a `boundary guard`, i.e. `if(idx >= n) return;`\n",
    "        - The ensures we don't index into an array with `idx` if `idx` is out of bounds.\n",
    "        - We have `6` threads in total, and `5` (defined by `N`) elements in each array, so `idx = 5` is out of bounds.\n",
    "      - Finally, we copy one element from the `input` array into the `output` using `idx` as the index.\n",
    "        - Remember, each thread runs its own `copy` of the kernel function (in parallel), each with the same set of kernel function `arguments`.\n",
    "\n",
    "    ```c\n",
    "    __global__ void kernel(int *input, int *output, int n)\n",
    "    {       \n",
    "        int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "        \n",
    "        printf(\"gridDim.x = %d, blockDim.x = %d, blockIdx.x = %d, threadIdx.x = %d, idx = %d\\n\", gridDim.x, blockDim.x, blockIdx.x, threadIdx.x, idx);\n",
    "        \n",
    "        if(idx >= n)\n",
    "        {\n",
    "            printf(\"Boundary checking avoided indexing outside of the arrays [idx = %d]\\n\", idx);\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        output[idx] = input[idx];\n",
    "    }\n",
    "    ```\n",
    "- In the `main()` function:\n",
    "  - We seed the pseudorandom number generator with the value `0` so the random numers we create will be the same every time we run the program.\n",
    "\n",
    "    ```c\n",
    "    srand(0);\n",
    "    ```\n",
    "\n",
    "  - We declare:\n",
    "    - `int` pointer variables `h_input` and `h_output` for the two arrays, which will point to heap memory (RAM) on the host (CPU).\n",
    "    - `int` pointer variables `d_input` and `d_output` for the two arrays, which will point to global memory on the device (GPU).\n",
    "    - `int` variable `data_size` and initialize it to `N * sizeof(int)`, i.e. the total size of each array in bytes (with `N` elements of type `int` in each).\n",
    "    \n",
    "    ```c\n",
    "    int *h_input, *h_output;\n",
    "    int *d_input, *d_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "    ```\n",
    "  - We allocate memory on the host (GPU) with `malloc`, storing the pointers to the memory in variables `h_input` and `h_output`.\n",
    "\n",
    "    ```c\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    ```\n",
    "  - We allocate memory on the device (GPU) with `cudaMalloc` storing the pointers to the memory in variables `d_input` and `d_output`.\n",
    "\n",
    "    ```c\n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "    ```\n",
    "  - We initialize the `h_input` array on the host (CPU) with random values using the `rand()` function.\n",
    "\n",
    "    ```c\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100; // random integers between 0 and 99\n",
    "    }\n",
    "    ```\n",
    "  - We copy the elements of both arrays stored in host (GPU) memory (RAM) to device (GPU) global memory with `cudaMemcpy`.\n",
    "    - Its first argument is a pointer to the memory to `copy to`.\n",
    "    - Its second argument is a pointer to the memory to `copy from`.\n",
    "    - Its third argument is the `size (in bytes)` of memory to copy (all `N` elements in our case).\n",
    "    - Its fourth argument is a symbolic constant which determines the direction of the copy operation.\n",
    "      - `cudaMemcpyHostToDevice` copies memory from the host (CPU) to the device (GPU).\n",
    "      - `cudaMemcpyDeviceToHost` copies memory from the device (GPU) to the host (CPU).\n",
    "    - Here we are copying the `input` and `output` arrays from the host (`h_input`, `h_output`) to the device (`d_input`, `d_output`).\n",
    "\n",
    "    ```c\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice);\n",
    "    ```\n",
    "  - We are now using CUDA's `dim3` struct to define the `gridDim` (number of blocks) and `blockDim` (number of threads in each block).\n",
    "    - Before we just used `int` litterals in the launch configuration `<<<1, 1>>>`, but we can also use `dim3` variables `<<<gridDim, blockDim>>>`.\n",
    "    - The launch configuration supports launching `1D`, `2D`, and `3D` blocks and threads, depending on the problem we want to solve, e.g:\n",
    "      - For a `1D` problem such as copying elements between two 1D arrays, we only need to use 1 dimension, which is the `x` member variable in `dim3` structs.\n",
    "      - For a `2D` problem such as filtering a 2D image, we might need to use 2 dimensions, which are the `x` and `y` member variables in `dim3` structs.\n",
    "      - For a `3D` problem such as filtering a 3D MRI-scan volume, we might need to use 3 dimensions, which are the `x`, `y` and `z` member variables in `dim3` structs.\n",
    "      - For our 1D problem, we are only using the `x` member variable, which means the other dimensions `y` and `z` will be set to the value `1`.\n",
    "    - We create a `dim3` variable `blockDim` and initialize its `x` member variable to `THREADS_PER_BLOCK_X` (member variables `y` and `z` will be set to `1`).\n",
    "      - This means we have `2` threads per block since `THREADS_PER_BLOCK_X` is defined with the value `2`.\n",
    "    - We create a `dim3` variable `gridDim` and initialize its `x` member variable to `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X`.\n",
    "      - This means we have `3` blocks per grid since `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X = (5 + 2 -1) / 2 = 3` (and there is only ever 1 grid).\n",
    "      - This (standard) construct is commonly used to ensure enough `threads` are launched to solve a problem (but can launch more `threads` than data elements).\n",
    "\n",
    "    ```c\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "    ```\n",
    "  - Next, we launch the kernel with:\n",
    "    - Launch configuration `<<<gridDim, blockDim>>>`, where `gridDim` and `blockDim` are our two `dim3` variables.\n",
    "    - Argument list `d_input, d_output, N`  `d_input`, where `d_input` and `d_output` are the `int` pointers to the arrays on the device (GPU), and `N` the number of elements.\n",
    "\n",
    "    ```c\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "    ```\n",
    "  - We copy the elements in the `d_output` array on the device (GPU) back to the array `h_output` on the host (CPU) using `cudaMemcpy`.\n",
    "    - Notice the final argument is now `cudaMemcpyDeviceToHost`, i.e. the direction of the copy operation is from the device (GPU) to the host (CPU).\n",
    "\n",
    "    ```c\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);   \n",
    "    ```\n",
    "  - Then we print out the elements in the two arrays `h_input` and `h_output` on the host (CPU).\n",
    "   \n",
    "    ```c\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    ```\n",
    "  - Finally, we free the mmeory allocated for the arrays:\n",
    "    - We free the `int` pointers (`d_input` and `d_output`) pointing to memory on the device (GPU) with `cudaFree`.\n",
    "    - We free the `int` pointers (`h_input` and `h_output`) pointing to memory on the host (CPU) with `free`.\n",
    "    - Both functions take a pointer to the memory \n",
    "    - Notice the naming convention used in this program for pointers to memory on the host (`h_` prefix) and the device (`d_` prefix).\n",
    "\n",
    "    ```c\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    ```\n",
    "- Run the cell below to see the output from the program.\n",
    "\n",
    "**TL;DR**\n",
    "- A kernel launch `kernel<<<blocks, threads>>>(argumentlist)` has:\n",
    "  - A launch configuration `<<<gridDim, blockDim>>>` that specifies how man `blocks` (`gridDim`) and `threads` per `block` (`blockDim`) to launch.\n",
    "    - It accepts `int` parameters, e.g. `<<<3, 2>>>` or `dim3` parameters, e.g. `<<<gridDim, blockDim>>>`.\n",
    "    - `dim3` is a struct containing `int` member variables `x`, `y`, and `z`, used to structure `blocks` and `threads` for `1D`, `2D`, or `3D` problems.\n",
    "  - An argument list `argumentlist` which must match the kernel function's parameter list.\n",
    "    - Each `thread` runs a `copy` of the same kernel function, with the exact same `argumentlist`, in parallel (at the same time).\n",
    "- A kernel function is run for each `thread`, where each `thread` has access to 4 global `dim3` variables `gridDim`, `blockDim`, `blockIdx`, and `threadIdx`\n",
    "  - `gridDim` and `blockDim` are from the launch configuration and contain the number of `blocks` (`gridDim`) and number of `threads` per `block` (`blockDim`).\n",
    "  - `blockIdx` and `threadIdx` contain unique `block` IDs within a grid (`blockIdx`) and unique `thread` IDs within a `block` (`threadIdx`).\n",
    "- The construct `(N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X`:\n",
    "  - Is commonly used ensure enough (at least as many) `threads` are launched needed to solve a problem (cover all data elements).\n",
    "  - But can launch more `threads` than the total number of data elements.\n",
    "- Since we can have more `threads` than data elements, we **always use bounday guards in CUDA kernels** to avoid out-of-bounds indexing.\n",
    "- We use `malloc` and `free` for managing memory on the host (CPU).\n",
    "- We use `cudaMalloc` and `cudaFree` for managing memory on the device (GPU).\n",
    "- We use `cudaMemcpy` to copy memory between the host (CPU) and device (GPU), where the fourth argument determines the direction of the copy operation.\n",
    "  - `cudaMemcpyHostToDevice` copies memory from the `host` (CPU) to the `device` (GPU).\n",
    "  - `cudaMemcpyDeviceToHost` copies memory from the `device` (GPU) to the `host` (CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f579afdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridDim.x = 3, blockDim.x = 2, blockIdx.x = 2, threadIdx.x = 0, idx = 4\n",
      "gridDim.x = 3, blockDim.x = 2, blockIdx.x = 2, threadIdx.x = 1, idx = 5\n",
      "gridDim.x = 3, blockDim.x = 2, blockIdx.x = 1, threadIdx.x = 0, idx = 2\n",
      "gridDim.x = 3, blockDim.x = 2, blockIdx.x = 1, threadIdx.x = 1, idx = 3\n",
      "gridDim.x = 3, blockDim.x = 2, blockIdx.x = 0, threadIdx.x = 0, idx = 0\n",
      "gridDim.x = 3, blockDim.x = 2, blockIdx.x = 0, threadIdx.x = 1, idx = 1\n",
      "Boundary checking avoided indexing outside of the arrays [idx = 5]\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N 5\n",
    "#define THREADS_PER_BLOCK_X 2\n",
    "\n",
    "__global__ void kernel(int *input, int *output, int n)\n",
    "{       \n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    \n",
    "    printf(\"gridDim.x = %d, blockDim.x = %d, blockIdx.x = %d, threadIdx.x = %d, idx = %d\\n\", gridDim.x, blockDim.x, blockIdx.x, threadIdx.x, idx);\n",
    "    \n",
    "    if(idx >= n)\n",
    "    {\n",
    "        printf(\"Boundary checking avoided indexing outside of the arrays [idx = %d]\\n\", idx);\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    output[idx] = input[idx];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int *d_input, *d_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);   \n",
    "    cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice);   \n",
    "\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "\n",
    "    // Launch kernel() function\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);   \n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32a341",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see:\n",
    "  - `gridDim.x` is `3`, i.e. there a `3` `blocks` in the grid.\n",
    "  - `blockDim.x` is `2`, i.e. there a `2` `threads` in each `block`.\n",
    "  - `blockIdx.x` varies from `0` to `2`, i.e. from `0` to `gridDim.x - 1`, and is a `block`'s unique ID (i.e. unique within a kernel launch).\n",
    "  - `threadIdx.x` varies from `0` to `1`, i.e. from `0` to `blockDim.x - 1`, and is a `thread`'s unique block ID (i.e. unique within a block).\n",
    "  - `idx` varies from `0` to `5`, i.e. from `0` to `gridDim.x * blockDim.x`, and is a `thread`'s unique global ID (i.e. unique within a kernel launch).\n",
    "  - The boundary guard was triggered for one thread, i.e. the thread with `idx = 5`, because we only have `N = 5` elements in each array.\n",
    "    - So, we can have more threads running than elements in our data/arrays, why **we should always make use of boundary guards in our CUDA kernels**.\n",
    "  - The `input` and `output` arrays have the same element values, so our CUDA kernel's logic is functionally correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a9a69",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.5 Unified Memory\n",
    "\n",
    "- CUDA supports using `Unified Memory` on most modern NVidia GPUs.\n",
    "  - `Unified Memory` is mapped to the same address space on the CPU and GPU, which means we don't need to explicitly copy memory between the CPU and GPU with `cudaMemcpy`.\n",
    "  - If this example doesn't work, your GPU doesn't support `Unified Memory`.\n",
    "- The `#include`s and `#define`s are the same as in the previous example.\n",
    "  - `cuda_runtime` also includes the function prototype for `cudaMallocManaged`, which we use for allocating `Unified Memory`.\n",
    "- The kernel function is the same as before, but without the code for printing blocks, threads, etc.\n",
    "  - We are just copying the elements between the two arrays now.\n",
    "\n",
    "    ```c\n",
    "    __global__ void kernel(int *input, int *output, int n)\n",
    "    {       \n",
    "        int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "        output[idx] = input[idx];\n",
    "    }\n",
    "    ```\n",
    "\n",
    "- The only modifications in the `main()` function are:\n",
    "  - We declare two `int *` pointer variables `input` and `output`.\n",
    "    - Notice we don't have `h_input`, `h_output`, `d_input`, and `d_output`, since `unified memory` is shared by the host (CPU) and the device (GPU).\n",
    "    - We also declare and initialize the `data_size` in bytes of the elements in the arrays, just as before (same code).\n",
    "      \n",
    "    ```c\n",
    "    int *input, *output;\n",
    "    int data_size = N * sizeof(int);\n",
    "    ```\n",
    "  - Then memory is allocated in `unified memory` using the function `cudaMallocManaged()`, that takes a `void **` pointer and an `int` size (in bytes) as parameters.\n",
    "    - The address of each `int` pointer variable (`&input` and `&output`) is typecast using `(void **)` for the first argument to the function.\n",
    "    - The `data_size` (in bytes) is passed as the second argument to the function.\n",
    "    - The function assigns an address to the memory allocated in `unified memory` to each of the `int` pointer varaibles `input` and `output`.\n",
    "      - Now these pointer variables are pointing to memory in `unified memory`.\n",
    "        \n",
    "    ```c       \n",
    "    cudaMallocManaged((void **)&input, data_size);\n",
    "    cudaMallocManaged((void **)&output, data_size);\n",
    "    ```\n",
    "  - Next, we initialize the `input` array with random values, just as before (the only difference in the name of the pointer, which is `input` instead of `h_input`).\n",
    "\n",
    "    ```c\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        input[i] = rand() % 100;\n",
    "    }\n",
    "    ```\n",
    "  - In the previous code, we used `cudaMemcpy` to copy memory from the host (CPU) to the device (GPU), but we don't need to do that here when using `unified memory`.\n",
    "  - The code for the `dim3` launch parameter variables is the same as in the prevous code, so no need to reiterate that code here.\n",
    "  - We then launch the kernel just as before, the only difference being the name of the pointers, which are `input` and `output` instead of `d_input` and `d_output`.\n",
    "  \n",
    "    ```c\n",
    "    kernel<<<gridDim, blockDim>>>(input, output, N);\n",
    "    ```\n",
    "  - Now, since we aren't using `cudaMemcpy()`, we need to call `cudaDeviceSynchronize()` to block the `main()` thread on the host until the kernel function on the device completes.\n",
    "  \n",
    "    ```c\n",
    "    cudaDeviceSynchronize();\n",
    "    ```\n",
    "  - Then we print out the values of the `input` and `output` arrays as before.\n",
    "    - The only difference is the name of the variables, which are `input` and `output` instead of `h_input` and `h_output`.\n",
    "  \n",
    "    ```c\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", input[i], output[i]);\n",
    "    }\n",
    "    ```\n",
    "  - We free the allocated `unified memory` using the function `cudaFree()`, just as before.\n",
    "    - The only difference is the name of the variables, which are `input` and `output` instead of `h_input`, `h_output`, `d_input` and `d_output`.\n",
    "      \n",
    "    ```c\n",
    "    cudaFree(input);\n",
    "    cudaFree(output);\n",
    "    ```\n",
    "- Run the cell below to see the output (it will be the same as before, except for the print-outs of the blocks, threads, etc.).\n",
    "\n",
    "**TL:DR**\n",
    "\n",
    "- Memory is allocated in `unified memory` using the function `cudaMallocManaged()` and is analogous to the normal C function `malloc()`.\n",
    "- The function `cudaDeviceSynchronize()` is used to block the host's main thread until the kernel completes.\n",
    "- Memory is deallocated (freed) in `unified memory` using the function `cudaFree()` and is analogous to the normal C function `free()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd66ec18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N 5\n",
    "#define THREADS_PER_BLOCK_X 2\n",
    "\n",
    "__global__ void kernel(int *input, int *output, int n)\n",
    "{       \n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    output[idx] = input[idx];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "    \n",
    "    int *input, *output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    cudaMallocManaged((void **)&input, data_size);\n",
    "    cudaMallocManaged((void **)&output, data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "\n",
    "    // Launch kernel() function\n",
    "    kernel<<<gridDim, blockDim>>>(input, output, N);\n",
    "\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", input[i], output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    cudaFree(input);\n",
    "    cudaFree(output);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f31f3",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see that the result is the same as before (the only difference is the abscense of the print-outs of blocks, threads, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea72c64",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.6 Error Checking\n",
    "\n",
    "- CUDA supports checking for errors in device (GPU) code and from calling any CUDA function.\n",
    "- The `#include`s, `#define`s, and the kernel function are the same as in the previous example.\n",
    "  - `stdlib.h` also includes the function prototype for `exit` and the symbolic constant `EXIT_FAILURE` used in this example.\n",
    "  - `cuda_runtime` also includes:\n",
    "    - The the function prototypes for `cudaMalloc`, `cudaFree`, and `cudaMemcpy`, since we are back to NOT using `unified memory`.\n",
    "    - The function prototypes for `cudaGetLastError` and `cudaGetErrorString`, which we use for checking errors from CUDA function calls.\n",
    "    - The typedef `cudaError_t` and symbolic constant `cudaSuccess`, also used for checking CUDA errors.\n",
    "- The only modifications in the `main()` function are:\n",
    "  - We are using the code from the example we used before the `unified memory` example, where we don't use `unified memory`.\n",
    "  - We have wrapped all CUDA function calls as arguments in a function called `checkCuda()`, explained below, e.g.\n",
    "\n",
    "    ```c\n",
    "    checkCuda(cudaMalloc((void **)&d_input, data_size), \"cudaMalloc\");\n",
    "    ```\n",
    "  - After the kernel launch, we use the code below to check for CUDA errors in the kernel function.\n",
    "\n",
    "    ```c\n",
    "    checkCuda(cudaGetLastError(), \"kernel\");\n",
    "    ```\n",
    "  - Then, after freeing all allocated memory, we deliberately produce an error:\n",
    "    - We call `cudaFree()` on the `d_output` pointer twice, which produces an error the second time since that memory has already been freed.\n",
    "      - All CUDA functions return a value of type `cudaError_t` which can be checked to see if an error occured in CUDA code on the device (GPU).\n",
    "      - The only CUDA operation that doesn't have a return value of type `cudaError_t` is the kernel launch.\n",
    "      - For that reason, CUDA provides the function `cudaGetLastError()` which will return the latest `cudaError_t` (we can use it after any CUDA function call). \n",
    "    ```c\n",
    "    checkCuda(cudaFree(d_output), \"cudaFree\");\n",
    "    ```\n",
    "- We have wrapped all CUDA function calls, returning a value of type `cudaError_t`, in the function `checkCuda()`.\n",
    "  - We pass the `cudaError_t` value as the first argument, and a string message as the second (the wrapped function name has been used).\n",
    "- The function `checkCuda()` is defined by ourselves (it's been placed between the kernel function and the `main()` function the sample code):\n",
    "  - It takes a CUDA error (`cudaError_t`) as its first argument and a message (string) as its second argument, returning `void`.\n",
    "  - It checks if the value of the `cudaError_t` error is different from the symbolic constant `cudaSuccess` (`cudaSuccess` means there is no error).\n",
    "    - If so, it retrieves a string-representation of the error by calling the function `cudaGetErrorString()`, which takes the error as an argument.\n",
    "    - Then the error string is printed out together with an optional message (second argument to `checkCuda()`).\n",
    "    - Finally, it terminates the program by calling the `exit()` function, passing in the symbolic constant `EXIT_FAILURE` as the return value to the operating system.\n",
    "  - We can use `checkCuda()` by wrapping it around any CUDA function call, e.g. `checkCuda(cudaFree(d_output), \"cudaFree\")`.\n",
    "    - Error checking will not be used going forward in this notebook to make the examples clearer, but good practice is to check for errors after each CUDA function call.\n",
    "\n",
    "    ```c\n",
    "    void checkCuda(cudaError_t err, const char *msg)\n",
    "    {\n",
    "        if (err != cudaSuccess)\n",
    "        {\n",
    "            printf(\"Error: %s (%s)\\n\", msg, cudaGetErrorString(err));\n",
    "            exit(EXIT_FAILURE);\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "- Run the cell below to see the output (it will be the same as before, except for the error message that we deliberately produced).\n",
    "\n",
    "**TL:DR**\n",
    "\n",
    "- Each CUDA function returns a value of type `cudaError_t`.\n",
    "  - If it's value is different from the symbolic constant `cudaSuccess`, and error occurred.\n",
    "  - We can retrieve a string representation of a CUDA error by passing a `cudaError_t` instance as an argument the the function `cudaGetErrorString`.\n",
    "  - We can retrieve the last CUDA error after a CUDA function call, including the kernel launch, using the function `cudaGetLastError`.\n",
    "- We can exit from a C program prematurely, by calling the C function `exit`, passing in an exit code to the operating system.\n",
    "  - The symbolic constant `EXIT_FAILURE` can be used as an exit code, representing a general error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2ed14d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "Error: cudaFree (invalid argument)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N 5\n",
    "#define THREADS_PER_BLOCK_X 2\n",
    "\n",
    "__global__ void kernel(int *input, int *output, int n)\n",
    "{       \n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    output[idx] = input[idx];\n",
    "}\n",
    "\n",
    "// Error checking\n",
    "void checkCuda(cudaError_t err, const char *msg)\n",
    "{\n",
    "    if (err != cudaSuccess)\n",
    "    {\n",
    "        printf(\"Error: %s (%s)\\n\", msg, cudaGetErrorString(err));\n",
    "        exit(EXIT_FAILURE);\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int *d_input, *d_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    checkCuda(cudaMalloc((void **)&d_input, data_size), \"cudaMalloc\");\n",
    "    checkCuda(cudaMalloc((void **)&d_output, data_size), \"cudaMalloc\");\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    checkCuda(cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice), \"cudaMemcpy\");\n",
    "    checkCuda(cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice), \"cudaMemcpy\");\n",
    "\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "\n",
    "    // Launch kernel() function\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "\n",
    "    checkCuda(cudaGetLastError(), \"kernel\");\n",
    "\n",
    "    checkCuda(cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost), \"cudaMemcpy\");\n",
    "\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    checkCuda(cudaFree(d_input), \"cudaFree\");\n",
    "    checkCuda(cudaFree(d_output), \"cudaFree\");\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "\n",
    "    checkCuda(cudaFree(d_output), \"cudaFree\");\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eba3cb",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see that the result is the same as before (the only difference is that we are NOT using `unified memory`).\n",
    "- We also see the error message `invalid argument` returned from `cudaFree()` when we try to deallocate device (GPU) memory that has already been freed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276810cd",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.7 Measuring Execution Time on the Host (CPU) and on the Device (GPU)\n",
    "\n",
    "- A common workflow is to first implement an algorithm in a function on the host (CPU), and then in a kernel on the device (GPU).\n",
    "  - The CPU version can act as a baseline benchmark for GPU kernel performance.\n",
    "  - The CPU version can be used to verify the results of a GPU kernel.\n",
    "  - For inexperienced manycore programmers, it's often easier to start with a CPU version, and then convert it into a GPU version.\n",
    "- Let's use the same code as before, but instrument it with timing code, wrapped around the CPU function call and around the GPU kernel launch.\n",
    "- The imported header files are the same as before:\n",
    "  - `stdlib.h` also contains the function prototype for `clock`, the `clock_t` typedef, and the symbolic constant `CLOCKS_PER_SEC`.\n",
    "    - `clock()` is a parameterless function returning a value of type `clock_t`.\n",
    "    - `clock_t` contains the number of `ticks` elapsed since the program started.\n",
    "    - `CLOCKS_PER_SEC` is defined as the number of `ticks` in a second (`ticks / CLOCKS_PER_SEC * 1000.0` converts `ticks` to milliseconds).\n",
    "  - `cuda_runtime.h` contains a typedef `cudaEvent_t` and prototypes `cudaEventCreate`, `cudaEventRecord`, `cudaEventElapsedTime`, `cudaEventSynchronize`, and `cudaEventDestroy`.\n",
    "    - `cudaEvent_t` represent a CUDA event, e.g. `cudaEvent_t start` (we won't explore CUDA events (or CUDA streams) in detail in this notebook).\n",
    "    - `cudaEventCreate` is used to initialize a CUDA event, e.g. `cudaEventCreate(&start)`\n",
    "    - `cudaEventRecord` is used to start recording (monitoring) a CUDA event, e.g. `cudaEventRecord(start)`\n",
    "    - `cudaEventElapsedTime` is used to compute and return the elapsed time in milliseconds between to CUDA events, e.g. `cudaEventElapsedTime(&elapsed_ms, start, stop)`\n",
    "    - `cudaEventSynchronize` blocks the CPU's main thread until an event has completed (in our code, when the kernel is done), e.g. `cudaEventSynchronize(stop)`\n",
    "    - `cudaEventDestroy` frees (destroys) a CUDA event, e.g. `cudaEventDestroy(start)`\n",
    "    \n",
    "- We define a host (CPU) function `copy()`, equivalent to the device (GPU) kernel function `kernel()`\n",
    "  - The GPU kernel function is the same as before.\n",
    "    \n",
    "    ```c\n",
    "    void copy(int *input, int *output, int n)\n",
    "    {\n",
    "        for(int idx = 0; idx < n; idx++)\n",
    "        {\n",
    "            output[idx] = input[idx];\n",
    "        }\n",
    "    }\n",
    "    ```\n",
    "- In the `main()` function:\n",
    "  - We wrap the code below around the device (GPU) kernel launch `kernel()`.\n",
    "    - First we declare two CUDA event variables `start` and `stop`, and initialize them with `cudaEventCreate(&start)` and `cudaEventCreate(&stop)`.\n",
    "    - Then we record the `start` event with `cudaEventRecord(start)` (this records the current time in the kernel and stores it in the `start` event).\n",
    "    - Next, the device (GPU) kernel is launched as usual.\n",
    "    - Then we record the `stop` event with `cudaEventRecord(stop)` (this records the current time in the kernel and stores it in the `stop` event).\n",
    "    - We call `cudaEventSynchronize(stop)` to block the host (CPU) main thread until the `stop`event is done (i.e. until the kernel is done).\n",
    "    - Finally, we declare a variable `float gpu_elapsed_ms` and pass it to the function `cudaEventElapsedTime(&gpu_elapsed_ms, start, stop);`\n",
    "      - We also pass in `start`and `stop`, where the function will store the elapsed time in milliseonds in the variable `gpu_elapsed_ms`.\n",
    "\n",
    "    ```c\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the device (GPU) kernel execution time\n",
    "    // --------------------------------------------------------------\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start);\n",
    "  \n",
    "    // Device kernel() launch\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "  \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float gpu_elapsed_ms;\n",
    "    cudaEventElapsedTime(&gpu_elapsed_ms, start, stop);\n",
    "    // --------------------------------------------------------------\n",
    "    ```\n",
    "  - We wrap the code below around the host (CPU) function call `copy()`.\n",
    "    - We call the function `clock()` to record the current number of `ticks` since the program started, and store the result in a variable `cpu_start` of type `clock_t`.\n",
    "    - Then we call the host (CPU) function `copy()`.\n",
    "    - Next, we call the function `clock()` again to record the current number of `ticks` again and store the result in a variable `cpu_stop` of type `clock_t`.\n",
    "    - Finally, we calculate the elapsed number of milliseconds as `float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0`.\n",
    "  \n",
    "    ```c\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the host (CPU) function execution time\n",
    "    // --------------------------------------------------------------\n",
    "    clock_t cpu_start = clock();\n",
    "\n",
    "    // Host function call\n",
    "    copy(h_input, h_output_cpu, N);\n",
    "    \n",
    "    clock_t cpu_stop = clock();\n",
    "    float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    // --------------------------------------------------------------\n",
    "    ```\n",
    "  - We print out the execution time for the device (GPU) kernel and host (GPU) function.\n",
    "\n",
    "    ```c\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "    ```\n",
    "  - We print out the execution time for the device (GPU) kernel and host (GPU) function.\n",
    "\n",
    "    ```c\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "    ```\n",
    "  - We use a separate `int` pointer variable `h_output_cpu` for storing the output from the host (CPU) function call.\n",
    "    - We verify the output results from the device (GPU) kernel and host (CPU) function are the same.\n",
    "    - This is a common best practice when verifying the correct functionality of an algorithm implemented in a device (GPU) kernel.\n",
    "      - We use the `abs()` function to compute the absolute difference between each eleement pair in the two arrays.\n",
    "      - If we were using `float`s instead of `int`s, we can use the `fabs()` function and compare the difference to e.g. `1e-5`.\n",
    "\n",
    "    ```c\n",
    "    int errorsum = 0;\n",
    "    for (int i = 0; i < N; i++)\n",
    "    {\n",
    "        int error = abs(h_output[i] - h_output_cpu[i]);\n",
    "        if (error > 0)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "    // Print verification result\n",
    "    printf(\"\\nVerification : %s\\n\", (errorsum > 0) ? \"FAILED\" : \"PASSED\");\n",
    "    ```\n",
    "  - We also print out the two arrays as before (same code) after launching the device (GPU) kernel and after calling the host (CPU) function.\n",
    "  - Lastly, we also free all memory, including the two CUDA events.\n",
    "\n",
    "    ```c\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    ```\n",
    "- Run the cell below to see the output.\n",
    "  - We won't record time in this notebook going forward to make the example code clearer, but now you know how to do it yourself-\n",
    "\n",
    "**TL:DR**\n",
    "\n",
    "- We can measure the execution time for a CUDA kernel with types and function prototypes declared in `cuda_runtime.h`.\n",
    "- We can measure the execution time for a C function (or any C code) with types and function prototypes declared in `stdlib.h`.\n",
    "- We can cmopute the absolute difference between two results to determine if they are correct (given at least one of the results is correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee6f9a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU execution time  : 0.087040 ms\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "CPU execution time  : 0.002000 ms\n",
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n",
      "Verification : PASSED\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N 5\n",
    "#define THREADS_PER_BLOCK_X 2\n",
    "\n",
    "// Device kernel\n",
    "__global__ void kernel(int *input, int *output, int n)\n",
    "{       \n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    output[idx] = input[idx];\n",
    "}\n",
    "\n",
    "// Host function\n",
    "void copy(int *input, int *output, int n)\n",
    "{\n",
    "    for(int idx = 0; idx < n; idx++)\n",
    "    {\n",
    "        output[idx] = input[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output, *h_output_cpu;\n",
    "    int *d_input, *d_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    h_output_cpu = (int *)malloc(data_size);\n",
    "\n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the device (GPU) kernel execution time\n",
    "    // --------------------------------------------------------------\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start);\n",
    "  \n",
    "    // Device kernel() launch\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "  \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float gpu_elapsed_ms;\n",
    "    cudaEventElapsedTime(&gpu_elapsed_ms, start, stop);\n",
    "    // --------------------------------------------------------------\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print measured device kernel execution time\n",
    "    printf(\"GPU execution time  : %f ms\\n\", gpu_elapsed_ms);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "\n",
    "    // --------------------------------------------------------------\n",
    "    // Timing the host (CPU) function execution time\n",
    "    // --------------------------------------------------------------\n",
    "    clock_t cpu_start = clock();\n",
    "\n",
    "    // Host function call\n",
    "    copy(h_input, h_output_cpu, N);\n",
    "    \n",
    "    clock_t cpu_stop = clock();\n",
    "    float cpu_elapsed_ms = (double)(cpu_stop - cpu_start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    // --------------------------------------------------------------\n",
    "\n",
    "    // Print measured host function execution time\n",
    "    printf(\"CPU execution time  : %f ms\\n\", cpu_elapsed_ms);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output_cpu[i]);\n",
    "    }\n",
    "\n",
    "    // Verify the results in the GPU output with the CPU output\n",
    "    int errorsum = 0;\n",
    "    for (int i = 0; i < N; i++)\n",
    "    {\n",
    "        int error = abs(h_output[i] - h_output_cpu[i]);\n",
    "        if (error > 0)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Print verification result\n",
    "    printf(\"\\nVerification : %s\\n\", (errorsum > 0) ? \"FAILED\" : \"PASSED\");\n",
    "\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_output_cpu);\n",
    "\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad70c7",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- In the output we see that the execution time on the GPU is slower than on the CPU.\n",
    "- This is expected since copying 5 elements from one array to another is just a waste of time on a GPU.\n",
    "  - **Not all problems are suitable for a GPU, in which case we should use the CPU instead**.\n",
    "- We also see the results verification `PASSED` so we can rest assured that the kernel function is correct (if the CPU function correct, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae5f59",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.8 Shared Memory and Thread Synchronization on the Device (GPU)\n",
    "\n",
    "- `Shared memory` is a fast, low-latency memory located on-chip, accessible by all `threads` in a `block`.\n",
    "  - **Location**: On-chip, accessible by all `threads` in a `block`.\n",
    "  - **Access**: Readable and writeable by all `threads` in a `block` (also `writable` from the `host` before kernel launch).\n",
    "  - **Size limit**: Typically `48 KB` per `SM` (Streaming Multiprocessor).\n",
    "  - **Speed**: Very fast, much faster than `global memory`.\n",
    "  - **Scope**: Shared only among `threads` in the same `block`.\n",
    "  - **Lifetime**: Exists for the duration of the `block`.\n",
    "- Use `shared memory` when:\n",
    "  - Threads need to cooperate, such as tiling, caching, or communication between `threads`.\n",
    "    - `Shared memory` is specified within a kernel function with the qualifier `__shared__`.\n",
    "    - It can be initialized `statically` or `dynamically`.\n",
    "- `Thread synchronization` is used to synchronize `threads`, especially `threads`in a `block` when using `shared memory`:\n",
    "  - Purpose: Barrier synchronization — all `threads` in the `block` must reach it before any can proceed.\n",
    "    - A barrier (thread synchronization) is done with the command `__syncthreads()` in kernel function.\n",
    "  - Ensures all `shared memory` reads/writes are complete before continuing.\n",
    "  - Used to prevent `race conditions`.\n",
    "- Issues to be aware of when using `shared memory`:\n",
    "  - `Non-coalesced memory access` also applies when accessing `global memory` (actually more important in that case).\n",
    "  - `Warp (thread) divergence` applies to `threads` within the same `warp` (a `warp` is a group of `32` threads that are scheduled to run on `SPs` within a `SM`/`block`).\n",
    "  - `Low occupancy` is related to `shared memory` but applies more generally to a `kernel launch`.\n",
    "\n",
    "  | Issue                       | Consequence                        | Fix                                |\n",
    "  | --------------------------- | ---------------------------------- | ---------------------------------- |\n",
    "  | Race conditions             | Wrong results                      | Use `__syncthreads()` or atomics   |\n",
    "  | No synchronization          | Inconsistent reads/writes          | Use `__syncthreads()`              |\n",
    "  | [Bank conflicts](https://www.youtube.com/watch?v=CZgM3DEBplE)              | Performance slowdown               | Pad arrays, restructure access     |\n",
    "  | Exceeding memory limit      | Kernel launch fails or runs slower | Reduce usage, use fewer threads    |\n",
    "  | Wrong indexing              | Wrong data or crash                | Use `threadIdx` properly           |\n",
    "  | Uninitialized/out-of-bounds | Undefined behavior                 | Always initialize and guard bounds |\n",
    "  | [Non-coalesced memory access](https://www.youtube.com/watch?v=mLxZyWOI340&list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2&index=97)| Slower execution speed | Coalesce memory access |\n",
    "  | [Warp (thread) divergence](https://www.youtube.com/watch?v=bHkFV-YMxxY&list=PLAwxTw4SYaPnFKojVQrmyOGFCqHTxfdv2&index=106) | Slower execution speed | Avoid branches and loops |\n",
    "  | [Low occupancy](https://www.youtube.com/watch?v=2NGQvnT_3gU) | Slower execution speed | Increase occupancy |  \n",
    "\n",
    "<br />  \n",
    "\n",
    "- Now, let's look at a simple example of using `shared memory`.\n",
    "- The code is the same as before, but with the following modifications:\n",
    "  - In the `kernel` function, we declare a buffer (array) with the `__shared__` qualifier.\n",
    "  - The `shared memory` can be declared with a `static` size or with a `dynamic size`.\n",
    "  - In the sample code, we are using a `dynamic size`, where\n",
    "    - the size isn't provided within the square brackets `[]`\n",
    "    - the keyword `extern` is used infront of the `__static__` qualifier\n",
    "      - this means the size id declared elsewhere (as an additional launch configuration parameter)\n",
    "  - If we wanted a `static` size, we could use the commented-out row below instead, where\n",
    "    - the sizs is provided within the square brackets `[THREADS_PER_BLOCK_X]` (`THREADS_PER_BLOCK_X` in this case).\n",
    "\n",
    "  ```c\n",
    "  extern __shared__ int shared[];               // dynamic size\n",
    "  //__shared__ int shared[THREADS_PER_BLOCK_X]; // static size\n",
    "  ```\n",
    "- Let's look at the complete kernel function:\n",
    "  - At the top, we decalare `shared memory` with a dynamic size.\n",
    "  - Then we calculate a `thread`'s global index/ID (`g_idx`) and a `thread`'s local/shared index/ID (`s_idx`).\n",
    "    - We have to be careful in how we use the threads for indexing (`g_idx` is unique within a kernel launch, `s_id` is unique within a `block` on the same `SM`).\n",
    "    - Remember, if we have `blockDim.x` `threads` per `block` (with a `s_id` ranging from `0` to `blockDim.x` - 1).\n",
    "  - Our usual `boundary guard` comes next `if(g_idx >0 n)`.\n",
    "  - Then we copy elements from the `input` array into `shared` memory.\n",
    "    - The index into the `input` array is `g_idx`.\n",
    "    - The index into the `shared` array is `s_idx`.\n",
    "    - Different indexing schemes might ne necessary depending on the problem/algorithm.\n",
    "  - Next, we have a thread barrier `__syncthreads()`.\n",
    "    - This ensures no `thread` within the `block` can continue past this row until all `threads` in the `block` have completed the code above this row.\n",
    "      - This is important, since some `threads`might not have copied their element from the `input` array into the `shared` array yet.\n",
    "      - In this example, it isn't an issue, because no other `thread` will read another `thread`'s element in the `shared` array in the code below the barrier `__syncthreads`.\n",
    "      - For other problems, this might not be the case, so if `threads` aren't synchronized, they might continue and read stale data from the `shared` array.\n",
    "  - Lastly, when all `threads` are synchronized, a `thread` copies an element from the `shared` array into the `output` array.\n",
    "    - The index into the `output` array is `g_idx`.\n",
    "    - The index into the `shared` array is `s_idx`.\n",
    "    - Different indexing schemes might ne necessary depending on the problem/algorithm.\n",
    "\n",
    "  ```C\n",
    "  __global__ void kernel(int *input, int *output, int n)\n",
    "  {\n",
    "      // Shared memory\n",
    "      extern __shared__ int shared[];               // dynamic size\n",
    "      //__shared__ int shared[THREADS_PER_BLOCK_X]; // static size\n",
    "      \n",
    "      int g_idx = threadIdx.x + blockIdx.x * blockDim.x; // index in global memory (globally unique)\n",
    "      int s_idx = threadIdx.x;                           // index in shared memory (unique within a block)\n",
    "\n",
    "      if(g_idx >= n) return; // boundary guard\n",
    "\n",
    "      // Copy elements in global memory (input) to shared memory (shared)\n",
    "      shared[s_idx] = input[g_idx];\n",
    "\n",
    "      // Synchronize threads\n",
    "      __syncthreads();       // all threads in the same block must be done with the operations above before any thread can continue\n",
    "\n",
    "      // Copy elements in shared memory (shared) to global memory (output)\n",
    "      output[g_idx] = shared[s_idx];\n",
    "  }\n",
    "  ```\n",
    "- Now, let's look at modifications in the `main()` function (most of the code is the same as before, but with the timing removed for clarity).\n",
    "  - In fact, there is only one modification:\n",
    "    - Since we are using a dynamic size for our `shared memory`, we first define the size of the memory with `int shared_size = THREADS_PER_BLOCK_X * sizeof(int)`.\n",
    "    - Then we supply the size `shared_size` (in bytes) as a third parameter in the launch configuration `<<<gridDim, blockDim, shared_size>>>`.\n",
    "    - If we were using a static size, we would comment these two rows, uncomment the last row, and use the same launch configuration as before (i.e. no change).\n",
    "  - Best practice is to use a dynamic size, since we can determine a variable size in the code (without relying on e.g. a `#define` preprocessing directive).\n",
    "\n",
    "    ```c\n",
    "    // Device kernel() launch\n",
    "    int shared_size = THREADS_PER_BLOCK_X * sizeof(int);\n",
    "    kernel<<<gridDim, blockDim, shared_size>>>(d_input, d_output, N);\n",
    "    //kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "\n",
    "    ```\n",
    "- Run the cell below to see the output (which is exactly the same as before).\n",
    "\n",
    "**TL;DR**\n",
    "\n",
    "- `Shared memory` can be declared using either:\n",
    "  - A dynamic size\n",
    "    - We use the keyword `extern`and the qualifier `__shared__` infront of the local variable in the kernel function.\n",
    "    - We don't specify the size when declaring the variable in the kernel function, e.g. `extern __shared__ int shared[]`\n",
    "    - We pass the size (in bytes) of the `shared memory` as a third parameter in the launch configuration `<<<blocks, threads, shared_size>>>`.\n",
    "  - A static size\n",
    "    - We use the qualifier `__shared__` infront of the local variable in the kernel function.\n",
    "    - We include the size when declaring the variable in the kernel function, e.g. `__shared__ int shared[THREADS_PER_BLOCK_X]`\n",
    "    - We call the kernel function without passing a third parameter to the launch configuration (or the value `0`) `<<<blocks, threads, shared_size>>>`.\n",
    "- `Thread synchronization` is important when using `shared memory`.\n",
    "  - We can synchronize `threads` with the statement `__syncthreads();`\n",
    "    - No `thread` in a `block` can continue past that row until all `threads` have completed their tasks in the code above that row.\n",
    "- Remember this regarding indexing:\n",
    "  - A `thread`'s unique ID within a `block` is `threadIdx.x` (a specific `block` only runs on one `SM`, the `SM` with the `shared memory`).\n",
    "  - A `thread`'s globally unique ID within a grid is calculated as `blockIdx.x * blockDim.x + threadIdx.x`.\n",
    "- Multiple issues are related to `shared memory` (is one isn't aware of them).\n",
    "  - We won't explore these issues (e.g. memory coalescence, warp divergence, bank conflicts, occupancy, etc.) in detail in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "322da13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      83    \n",
      "86      86    \n",
      "77      77    \n",
      "15      15    \n",
      "93      93    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N 5\n",
    "#define THREADS_PER_BLOCK_X 2\n",
    "\n",
    "// Device kernel\n",
    "__global__ void kernel(int *input, int *output, int n)\n",
    "{\n",
    "    // Shared memory\n",
    "    extern __shared__ int shared[];               // dynamic size\n",
    "    //__shared__ int shared[THREADS_PER_BLOCK_X]; // static size\n",
    "    \n",
    "    int g_idx = threadIdx.x + blockIdx.x * blockDim.x; // index in global memory (globally unique)\n",
    "    int s_idx = threadIdx.x;                           // index in shared memory (unique within a block)\n",
    "\n",
    "    if(g_idx >= n) return; // boundary guard\n",
    "\n",
    "    // Copy elements in global memory (input) to shared memory (shared)\n",
    "    shared[s_idx] = input[g_idx];\n",
    "\n",
    "    // Synchronize threads\n",
    "    __syncthreads();       // all threads in the same block must be done with the operations above before any thread can continue\n",
    "\n",
    "    // Copy elements in shared memory (shared) to global memory (output)\n",
    "    output[g_idx] = shared[s_idx];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output;\n",
    "    int *d_input, *d_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "\n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "    \n",
    "    // Device kernel() launch\n",
    "    int shared_size = THREADS_PER_BLOCK_X * sizeof(int);\n",
    "    kernel<<<gridDim, blockDim, shared_size>>>(d_input, d_output, N);\n",
    "    //kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b44be",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- The output is exactly the same as before (same algorithm, just using different type of memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855d224",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.9 Constant Memory on the Device (GPU)\n",
    "\n",
    "- `Constant memory` is a special type of GPU memory optimized for cases where many `threads` read the same values.\n",
    "  - **Location**: On-device, separate from `global memory`.\n",
    "  - **Access**: Readable by all `threads` and is `read-only` from the `device`, but `writable` from `host`.\n",
    "  - **Size limit**: `64 KB` (per `device`).\n",
    "  - **Speed**: Very fast if all `threads` access the same address.\n",
    "  - **Scope**: Globally accessible (like global variables).\n",
    "  - **Lifetime**: Exists for the duration of the `kernel launch`.\n",
    "- Use `constant memory` when:\n",
    "  - All or most `threads` access the same data (e.g., coefficients, transformation matrices, filters).\n",
    "  - The data is known before kernel launch and doesn't change during execution.\n",
    "  - The data is small (<= `64 KB`).\n",
    "- Let's look at a simple example using `constant memory`.\n",
    "- It's the same code as before, but with the `shared memory` removed, and with the following modifications:\n",
    "  - Above the kernel function (not inside it), we declare a constant memory buffer (array) using the `__constant__` qualifier.\n",
    "  - In the kernel function:\n",
    "    - We multiply an element in the `input` array with an elements in the `constant` array, both with the same index.\n",
    "    - Then we assigning the product to the `output` array using the same index.\n",
    "    - Note that we have declared the size of the `constant memory` to be the same as the number of elements `N`.\n",
    "      - This is perfectly fine for this example where `N = 5`, but `constant memory` is extremely limited (small).\n",
    "      - We wouldn't be able to use `N` as the `constant memory`'s size if, say, `N` was `1000000` (a million elements).\n",
    "\n",
    "      ```c\n",
    "      // constant memory\n",
    "      __constant__ int constant[N];\n",
    "      \n",
    "      // Device kernel\n",
    "      __global__ void kernel(int *input, int *output, int n)\n",
    "      {   \n",
    "          int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "          \n",
    "          if(idx >= n) return; // boundary guard\n",
    "          \n",
    "          // Multiply input elements with coefficients in constant memory and store the product in output\n",
    "          output[idx] = input[idx] * constant[idx];\n",
    "      }\n",
    "      ```\n",
    "  - In the main() function, the code is the same as before (but with `shared memory` removed), but with the following modifications:\n",
    "    - We declare an `int` pointer variable on the host (CPU) to define the contents to be copied to the `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      int *h_coefficients;\n",
    "      ```\n",
    "    - We create a variable with the same size (but in bytes) as the statically defined `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      int constant_size = N * sizeof(int);\n",
    "      ```\n",
    "    - We allocate space in host (CPU) memory (RAM) the data with will be copying to the `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      h_coefficients = (int *)malloc(constant_size);\n",
    "      ```\n",
    "    - We initialize the data we will be copying tp `constant memory`.\n",
    "      - Notice, all the elements in `h_coefficients` are two (so the elements in the `output` array from the kernel function will be twice as large as in the `input` array).\n",
    "\n",
    "      ```c\n",
    "      for(int i = 0; i<N; i++)\n",
    "      {\n",
    "         h_coefficients[i] = 2;\n",
    "      } \n",
    "      ```\n",
    "    - Then we copy the host (CPU) memory to the device (GPU) `constant mmeory` using the CUDA function `cudaMemcpyToSymbol`.\n",
    "      - Notice that we aren't using the `cudaMemcpy` function when copying host (CPU) memory to device (GPU) `constant memory`.\n",
    "      - We pass in a pointer to the `constant` memory as the first argument.\n",
    "      - We pass in a pointer to the host (CPU) memory `h_coefficients` as the second argument.\n",
    "      - We pass in the size (in bytes) of the `constant memory` as the third argument.\n",
    "    \n",
    "      ```c\n",
    "      cudaMemcpyToSymbol(constant, h_coefficients, constant_size); // copy host (CPU) memory to device (GPU) constant memory\n",
    "      ```\n",
    "    - At the very end of the `main()` function, we free the memory on the host (CPU), allocated to store the values copied to `constant memory`.\n",
    "\n",
    "      ```c\n",
    "      free(h_coefficients);\n",
    "      ```\n",
    "- Run the cell below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4906f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input   output\n",
      "83      166   \n",
      "86      172   \n",
      "77      154   \n",
      "15      30    \n",
      "93      186   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define N 5\n",
    "#define THREADS_PER_BLOCK_X 2\n",
    "\n",
    "// constant memory\n",
    "__constant__ int constant[N];\n",
    "\n",
    "// Device kernel\n",
    "__global__ void kernel(int *input, int *output, int n)\n",
    "{   \n",
    "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "\n",
    "    if(idx >= n) return; // boundary guard\n",
    "\n",
    "    // Multiply input elements with coefficients in constant memory and store the product in output\n",
    "    output[idx] = input[idx] * constant[idx];\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "    \n",
    "    int *h_input, *h_output, *h_coefficients;\n",
    "    int *d_input, *d_output;\n",
    "    int data_size = N * sizeof(int);\n",
    "    int constant_size = N * sizeof(int);\n",
    "\n",
    "    h_input = (int *)malloc(data_size);\n",
    "    h_output = (int *)malloc(data_size);\n",
    "    h_coefficients = (int *)malloc(constant_size);\n",
    "\n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_input[i] = rand() % 100;\n",
    "    }\n",
    "\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        h_coefficients[i] = 2;\n",
    "    }\n",
    "\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpyToSymbol(constant, h_coefficients, constant_size);       // copy host (CPU) memory to device (GPU) constant memory\n",
    "\n",
    "    dim3 blockDim(THREADS_PER_BLOCK_X);\n",
    "    dim3 gridDim((N + THREADS_PER_BLOCK_X - 1) / THREADS_PER_BLOCK_X);\n",
    "    \n",
    "    // Device kernel() launch\n",
    "    kernel<<<gridDim, blockDim>>>(d_input, d_output, N);\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print elements in both arrays\n",
    "    printf(\"\\n%-5s   %-6s\\n\", \"input\", \"output\");\n",
    "    for(int i = 0; i<N; i++)\n",
    "    {\n",
    "        printf(\"%-5d   %-6d\\n\", h_input[i], h_output[i]);\n",
    "    }\n",
    "\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_coefficients);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf09f0d",
   "metadata": {},
   "source": [
    "### Inspecting the Output\n",
    "\n",
    "- We see that the values in the `output` array are twice as large compared to the `input` array,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a505b9a",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Sample Problems\n",
    "---\n",
    "\n",
    "## 3.1 1D Vector Addition on the Host (CPU)\n",
    "\n",
    "<img src=\"images/vectoradd_cpu.png\" width=\"500\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Let's start with a simple problem.\n",
    "\n",
    "Problem\n",
    "  - We have three vectors (arrays) `A`, `B`, and `C`, all with `N` elements each.\n",
    "  - We want to compute the elementwise sum of `A` and `B`, and store the sum in `C`.\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576`\n",
    "2. Create a host function `void vectorAdd(float *A, float *B, float *C, int n)`\n",
    "    - Loop through vectors `A` and `B` with `idx=0..N-1`\n",
    "    - Compute `C[idx] = A[idx] + B[idx]`\n",
    "3. Create a host function `main(void)`\n",
    "    - Declare and allocate memory for vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Initialize vectors `h_A` and `h_B` with `N` random floats each.\n",
    "    - Call function `vectorAdd` with `h_A`, `h_B`, `h_C`, `N`, and measure the execution time for `vectorAdd`.\n",
    "    - Verify result is correct.\n",
    "    - Print execution time, verification result, and sample elements in vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Free memory allocated for vectors `h_A`, `h_B`, and `h_C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU execution time  : 2.295000 ms\n",
      "Verification result : PASSED\n",
      "Vector samples      : A[0]=0.840188, B[0]=0.394383, C[0]=1.234571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Number of elements (1048576)\n",
    "#define N (1 << 20) \n",
    "\n",
    "// Host function (elementwise addition of vectors A and B, placing the sum in vector C)\n",
    "void vectorAdd(float *A, float *B, float *C, int n)\n",
    "{   \n",
    "    // Loop through vectors and compute sum C = A + B\n",
    "    for (int idx = 0; idx < n; idx++)\n",
    "    {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "// Host main routine\n",
    "int main(void)\n",
    "{\n",
    "    // Seed pseudorandom number generator\n",
    "    srand(0);\n",
    "\n",
    "    // Compute the size of the vectors (in bytes)\n",
    "    size_t size = N * sizeof(float);\n",
    "\n",
    "    // Declare and allocate host vectors A, B, and C   \n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    // Initialize host input vectors A and B with random values between 0 and 1.0\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        h_A[i] = rand() / (float)RAND_MAX;\n",
    "        h_B[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "\n",
    "    // Call function vectorAdd with timing\n",
    "    clock_t start = clock();\n",
    "\n",
    "    vectorAdd(h_A, h_B, h_C, N); // function call\n",
    "    \n",
    "    clock_t end = clock();\n",
    "    double elapsed_ms = (double)(end - start) / CLOCKS_PER_SEC * 1000.0;\n",
    "    \n",
    "    // Verify results in ouput vector C is correct\n",
    "    float errorsum = 0.0f;\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        float error = fabs(h_A[i] + h_B[i] - h_C[i]);\n",
    "        if (error > 1e-5)\n",
    "        {\n",
    "            //printf(\"Result verification failed for element with index %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Print measured function execution time, verification result, and sample elements from each vector\n",
    "    printf(\"CPU execution time  : %f ms\\n\", elapsed_ms);\n",
    "    printf(\"Verification result : %s\\n\", (errorsum > 1e-5) ? \"FAILED\" : \"PASSED\");\n",
    "    printf(\"Vector samples      : A[0]=%f, B[0]=%f, C[0]=%f\\n\", h_A[0], h_B[0], h_C[0]);\n",
    "    \n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10200a36",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 1D Vector Addition on the Device (GPU)\n",
    "\n",
    "<img src=\"images/vectoradd_gpu1.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Problem\n",
    "  - We have three vectors (arrays) `A`, `B`, and `C`, all with `N` elements each.\n",
    "  - We want to compute the elementwise sum of `A` and `B`, and store the sum in `C`.\n",
    "\n",
    "We Know\n",
    "- In CUDA, we have access to many `threads`, where `threads` are organized into `blocks`, and `blocks` are organized into a `grid`.\n",
    "  - `threadIdx.x` represents a `thread’s index` along the `x` dimension within a `block`. \n",
    "  - `blockIdx.x` represents a `block’s index` along the `x` dimension within the `grid`.\n",
    "  - `blockDim.x` represents the `number of threads` along the `x` dimension with a `block`.\n",
    "- To get a `thread's global index` on the GPU:\n",
    "  - `int index = blockDim.x * blockIdx.x + threadIdx.x`\n",
    "- `Blocks` are assigned to a Streaming Multiprocessor (SM) that has a number of Streaming Processors (SPs).\n",
    "  - Each `thread` executes its own copy of the `kernel function`, in parallel, with the same parameter values.\n",
    "  - Each `thread` should process only one element in the arrays using the `index`.\n",
    "  - If there are more threads than elements (`index >= N`), those threads should `return` immediately from the `kernel function`\n",
    "- There can be a maximum of `1024` threads in a block.\n",
    "  - If we have `N = 1048576` elements,\n",
    "  - and `THREADS_PER_BLOCK = 1024`,\n",
    "  - we get `BLOCKS = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK = (1048576+1024-1) / 1024 = 1024`.\n",
    "  - And if we have `24` SMs, each will be assigned roughly `1024 / 24 = 42` blocks for maximum efficiency.\n",
    "\n",
    "<img src=\"images/vectoradd_gpu2.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576` and `THREADS_PER_BLOCK=1024`\n",
    "2. Create a kernel `__global__ void vectorAdd(float *A, float *B, float *C, int n)`\n",
    "    - Compute global thread ID `idx = blockDim.x * blockIdx.x + threadIdx.x`\n",
    "    - Return if index is out of bounds (`idx >= n`) which means we have more threads than elements `n`.\n",
    "      - In this case we won't since `N` is evenly divisible by `THREADS_PER_BLOCK`.\n",
    "    - Compute `C[idx] = A[idx] + B[idx]`.\n",
    "\n",
    "3. Create a host function `main(void)`\n",
    "    - Declare and allocate memory for host vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Initialize host vectors `h_A` and `h_B` with `N` random floats each.\n",
    "    - Declare and allocate memory for device vectors `d_A`, `d_B`, and `d_C`.\n",
    "    - Copy contents of host vectors `h_A` and `h_B` to device vectors `d_A` and `d_B`.\n",
    "    - Launch kernel `vectorAdd` with `d_A`, `d_B`, `d_C`, `N`, and measure the execution time for `vectorAdd`.\n",
    "    - Copy contents of device vector `d_C` to host vector `h_C`.\n",
    "    - Verify result is correct.\n",
    "    - Print execution time, verification result, and sample elements in host vectors `h_A`, `h_B`, and `h_C`.\n",
    "    - Free memory allocated for device vectors `d_A`, `d_B`, and `d_C`.\n",
    "    - Free memory allocated for host vectors `h_A`, `h_B`, and `h_C`.\n",
    "\n",
    "<img src=\"images/coalesced_memory_access.png\" width=\"450\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "No need for shared or constant memory, and the global memory access pattern is **coalesced** in the code, (a) in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU execution time  : 0.880288 ms\n",
      "Verification result : PASSED\n",
      "Vector samples      : A[0]=0.840188, B[0]=0.394383, C[0]=1.234571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Number of elements (1048576)\n",
    "#define N (1 << 20)\n",
    "\n",
    "// Number of threads\n",
    "#define THREADS_PER_BLOCK 1024\n",
    "\n",
    "// Device kernel (elementwise addition of vectors A and B, placing the sum in vector C)\n",
    "__global__ void vectorAdd(float *A, float *B, float *C, int n)\n",
    "{\n",
    "    // Compute index (idx) from global thread ID\n",
    "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "\n",
    "    // Return if index is out of bounds (means we have more threads than elements)\n",
    "    if (idx >= n)\n",
    "      return;\n",
    "\n",
    "    // Compute the sum C = A + B for the element with index idx\n",
    "    C[idx] = A[idx] + B[idx];\n",
    "}\n",
    "\n",
    "// Host main routine\n",
    "int main(void)\n",
    "{\n",
    "    // Seed pseudorandom number generator\n",
    "    srand(0);\n",
    "\n",
    "    // Compute the size of the vector to use\n",
    "    size_t size = N * sizeof(float);\n",
    "\n",
    "    // Allocate the host input vectors A, B, and output vector C\n",
    "    float *h_A = (float *)malloc(size);\n",
    "    float *h_B = (float *)malloc(size);\n",
    "    float *h_C = (float *)malloc(size);\n",
    "\n",
    "    // Initialize the host input vectors (with random values)\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        h_A[i] = rand() / (float)RAND_MAX;\n",
    "        h_B[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "\n",
    "    // Allocate the device input vectors A, B, and output vector  C\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc((void **)&d_A, size);\n",
    "    cudaMalloc((void **)&d_B, size);\n",
    "    cudaMalloc((void **)&d_C, size); \n",
    "\n",
    "    // Copy the host input vectors A and B in host memory to the device input vectors in device memory\n",
    "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
    "  \n",
    "    // Launch kernel with timing\n",
    "    int blocksPerGrid = (N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start);\n",
    "  \n",
    "    vectorAdd<<<blocksPerGrid, THREADS_PER_BLOCK>>>(d_A, d_B, d_C, N);\n",
    "  \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float elapsed;\n",
    "    cudaEventElapsedTime(&elapsed, start, stop);\n",
    " \n",
    "    // Copy the device result vector in device memory to the host result vector in host memory.\n",
    "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
    "  \n",
    "    // Verify the result vector is correct\n",
    "    float errorsum = 0.0f;\n",
    "    for (int i = 0; i < N; ++i)\n",
    "    {\n",
    "        float error = fabs(h_A[i] + h_B[i] - h_C[i]);\n",
    "        if (error > 1e-5)\n",
    "        {\n",
    "            //fprintf(stderr, \"Result verification failed at element %d!\\n\", i);\n",
    "            errorsum += error;\n",
    "        }\n",
    "    }\n",
    "  \n",
    "    // Print measured kernel execution time, verification result, and sample elements from each vector\n",
    "    printf(\"GPU execution time  : %f ms\\n\", elapsed);\n",
    "    printf(\"Verification result : %s\\n\", (errorsum > 1e-5) ? \"FAILED\" : \"PASSED\");\n",
    "    printf(\"Vector samples      : A[0]=%f, B[0]=%f, C[0]=%f\\n\", h_A[0], h_B[0], h_C[0]);\n",
    "\n",
    "    // Free device global memory\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "  \n",
    "    // Free host memory\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308f4c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.3 1D Convolution on the Host (CPU)\n",
    "\n",
    "<img src=\"images/1dconvolution.gif\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Next, let's tackle the problem of a 1-dimensional (1D) convolution.\n",
    "\n",
    "Problem\n",
    "- We have an `input` vector, a `kernel` (filter), and an `output` vector.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` vector.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` vector.\n",
    "- So the `kernel`'s (filter's) width has to be odd, e.g. `1x3`, `1x5`, `1x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` vector with `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` vector with the same `index` as the current `input` vector.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` vector, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define number of elements `N=1048576`\n",
    "2. Create a function:\n",
    "   - `void convolve1D(float *input, float *output, float *filter)`\n",
    "   - Loop through `input` vector.\n",
    "   - Compute `output[idx] = input[idx + offset] = filter[FILTER_WIDTH/2 + offset]`\n",
    "     - Only if `if(idx + offset >= 0 && idx + offset < DATA_WIDTH)`\n",
    "     - Where `offset` ranges from `-FILTER_WIDTH/2` to `+FILTER_WIDTH/2`.\n",
    "   - This computation is equivalent to\n",
    "     - Looping through the `input` vector, zero-padded with `FILTER_WIDTH/2` elements on both sides.\n",
    "     - Centering the `filter` over each original element in the zero-padded `input` vector.\n",
    "     - Computing the weighted sum and storing it in the `output` vector.\n",
    "4. Create a function `main(void)`\n",
    "   - Define a `DATA_WIDTH`, `FILTER_WIDTH` and `FILTER_WIDTH_OFFSET` (which is `FILTER_WIDTH/2`).\n",
    "   - Declare and allocate memory for vectors `input`, `ouput`, and `filter`.\n",
    "   - Initialize vector `input` with `DATA_WIDTH` random floats.\n",
    "   - Initialize vector `filter` with `weights` where each weight is `1.0 / FILTER_WIDTH` (averaging filter).\n",
    "   - Call function `convolve1D` with `input`, `ouput`, and `filter`.\n",
    "   - Measure the execution time for `convolve1D`.\n",
    "   - Print execution time and sample elements in vectors `input` and `output`.\n",
    "   - Free memory allocated for vectors `input`, `output`, and `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1048576 elements, 1x3 filter) took 7.65 ms\n",
      "Vector samples:\n",
      "h_input[0]=0.84, h_output[0]=0.41\n",
      "h_input[1]=0.39, h_output[1]=0.67\n",
      "h_input[2]=0.78, h_output[2]=0.66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "// Number of data elements (1048576)\n",
    "#define DATA_WIDTH (1 << 20)\n",
    "\n",
    "// Number of filter elements \n",
    "#define FILTER_WIDTH 3\n",
    "\n",
    "// Number of elements on each side of a centered filter\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "\n",
    "void convolve1D(float *input, float *output, float *filter)\n",
    "{\n",
    "    // Loop through all elements\n",
    "    for(int d_col = 0; d_col < DATA_WIDTH; d_col++)\n",
    "    {\n",
    "        // Apply filter (slide filter over data and compute weighted sum)\n",
    "        float sum = 0.0f;\n",
    "        for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "        {\n",
    "            int f_col = FILTER_WIDTH_OFFSET + offset_col; // f_col: 0..FILTER_WIDTH-1\n",
    "            int i_col = d_col + offset_col;               // i_col: 0-FILTER_WIDTH_OFFSET..DATA_WIDTH-1+FILTER_WIDTH_OFFSET\n",
    "            \n",
    "            if(i_col >= 0 && i_col < DATA_WIDTH)\n",
    "            {\n",
    "                sum += input[i_col] * filter[f_col];\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        // Store the weighted sum in the output array\n",
    "        output[d_col] = sum;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Seed the random number generator\n",
    "    srand(0);            // use this for same set of random numbers each time the program is run\n",
    "    //srand(time(NULL)); // use this for different set of random numbers each time the program is run\n",
    "\n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * sizeof(float);     // size of data in bytes\n",
    "    int filter_size = FILTER_WIDTH * sizeof(float); // size of filter in bytes\n",
    "   \n",
    "    // Allocate space for host copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "      \n",
    "    // Setup input values\n",
    "    for (int col = 0; col < DATA_WIDTH; col++)\n",
    "    {\n",
    "        h_input[col] = (float)rand() / RAND_MAX; // Random floats between 0 and 1.0\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for (int col = 0; col < FILTER_WIDTH; col++)\n",
    "    {\n",
    "        h_filter[col] = 1.0f / FILTER_WIDTH; // averaging filter\n",
    "    }\n",
    "   \n",
    "    // Call convolve1D() with timing\n",
    "    clock_t start = clock();                                              // record the start time\n",
    "    convolve1D(h_input, h_output, h_filter);                              // call convolve1D()\n",
    "    clock_t stop = clock();                                               // record the stop time\n",
    "    double elapsed_ms = (double)(stop - start) / CLOCKS_PER_SEC * 1000.0; // calculate the elapsed time in millisecond\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, 1x%d filter) took %.2f ms\\n\", DATA_WIDTH, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"Vector samples:\\n\");\n",
    "    for(int i = 0; i < FILTER_WIDTH; i++)\n",
    "    {\n",
    "        printf(\"h_input[%d]=%.2f, h_output[%d]=%.2f\\n\", i, h_input[i], i, h_output[i]);\n",
    "    }\n",
    "\n",
    "    // Cleanup\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "    \n",
    "    return 0;\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5e7ab",
   "metadata": {},
   "source": [
    "- The output shows:\n",
    "  - Given an element with index `idx` in the `output` array.\n",
    "  - It's value is the average of the elements with indices `-FILTER_WIDTH_OFFSET..+FILTER_WIDTH_OFFSET+1` in the `input` array.\n",
    "    - Since an averaging filter was used.\n",
    "  - For example\n",
    "    - If the `FILTER_WIDTH` is `3`, we have `FILTER_WIDTH_OFFSET = FILTER_WIDTH / 2 = 1`.\n",
    "    - The value of an element with index `idx` in the `output` array is the average of the elements with indices `idx-1`, `idx`, and `idx+1` in the `input` array.\n",
    "      - `output[idx] = (input[idx-1] +  nput[idx] + nput[idx+1]) / 3`\n",
    "      - If it's a bounday element, the out-of-bounds indices have zero-padded elements with a value of `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0137e2d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.4 1D Convolution on the Device (GPU)\n",
    "\n",
    "<img src=\"images/tiled_convolution_1d.png\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "Problem\n",
    "- We have an `input` vector, a `kernel` (filter), and an `output` vector.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` vector.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` vector.\n",
    "- So the `kernel`'s (filter's) width has to be odd, e.g. `1x3`, `1x5`, `1x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` vector with `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` vector with the same `index` as the current `input` vector.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` vector, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "- We have a 1D `input` vector with `N` elements (vector marked with `N` in the figure).\n",
    "- A `block` of `threads` will process `blockDim` number of elements (top row in figure).\n",
    "- We don't want to load elements multiple times from `global memory` during the calulation.\n",
    "  - So each `thread`in a `block` loads its `input` element into `shared memory` (called `Tile` in the figure).\n",
    "  - The `shared_memory` size needs to be `TILE_BASE_WITH + 2 * FILTER_WIDTH_OFFSET`, where\n",
    "    - `TILE_BASE_WITH` is the number of original `input` elements in a `block` (highlighted elements in figure).\n",
    "    - `FILTER_WIDTH_OFFSET` is `FILTER_WIDTH / 2` (called `halo` elements in the figure).\n",
    "    - `FILTER_WIDTH` is `5` (in the figure).\n",
    "\n",
    "<img src=\"images/block_tile_loading_1d.png\" width=\"400\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "  - This ensures the `filter`, when centered on an element, covers all neighbouring elements, e.g.\n",
    "    - In `Block 0` the threads use `Tile 0`, where the original elements are `0`, `1`, `2`, `3` (see figure).\n",
    "    - The `filter` is centered on `0` covering `FILTER_WIDTH_OFFSET` neighbouring elements on each side.\n",
    "    - For border elements we use zero-padding (called `ghost` elements in the figure for the left-most elements).\n",
    "    - So the elements included in the first convolution are `ghost`, `ghost`, `0`, `1`, `2` (where `ghost = 0`).\n",
    "      - When processing element `3`, the `filter` covers elements `1`, `2`, `3`, `4`, `5`.\n",
    "\n",
    "- For these extra `2 * FILTER_WIDTH_OFFSET` elements to be available in a `block`:\n",
    "    - The `shared memory`, called `Tile`, needs a size of `TILE_BASE_WITH + 2 * FILTER_WIDTH_OFFSET` (see above).\n",
    "      - This is the actual size need for a `block` of `threads`, i.e. `blockDim` which includes:\n",
    "      - Threads for loading the original elements that the `filter` will center on.\n",
    "      - Threads for the extra `2 * FILTER_WIDTH_OFFSET` border elements.\n",
    "      - This is illustrated in the bottom figure.\n",
    "- We also want to load the `filter` elements into `constant` mmeory to avoid hitting global mmeory when accessing them.\n",
    "\n",
    "- So this is what we'll do:\n",
    "1. Define:\n",
    "  - `DATA_WIDTH=1048576` (number of elements in data vectors `input` and `output`)\n",
    "  - `FILTER_WIDTH=3` (number of elements in the `filter` vector)\n",
    "  - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements on each size of a centered `filter`)\n",
    "  - `TILE_WIDTH_BASE=16` (number original elements in a `block` of `threads`)\n",
    "    - Where the final tile size is `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` to cover border elements.\n",
    "    - This is also the size we will use for the `shared memory` and `block` size, i.e. `blockDim.x`.\n",
    "    - So we have these many `threads` in each `block` and we now a `block` is assigned to an `SM`.\n",
    "2. Define `constant` memory of size `FILTER_WIDTH` for the `filter`.\n",
    "3. Create a kernel function `void convolve1D(float *input, float *output)`:\n",
    "   - Define `shared` memory of size `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET`.\n",
    "   - Let the `threads` in a `block` load their `input` elements into `shared` memory.\n",
    "     - For border elements, we load the value `0` into `shared` memory (zero-padding).\n",
    "   - Synchronize `threads`to ensure each `thread`in a `block` has loaded its element into `shared` memory.\n",
    "   - Compute the convolution as in the CPU solution, but now using `shared` memory (input) and `constant` memory (filter).\n",
    "   - Store the result in the `ouput` vector.\n",
    "5. Create a function `main(void)`\n",
    "   - Declare and allocate memory for vectors `input`, `ouput`, and `filter` on the host (CPU).\n",
    "   - Declare and allocate memory for vectors `input` and `ouput` on the device (GPU).\n",
    "   - Initialize vector `input` with `DATA_WIDTH` random floats on the host (CPU).\n",
    "   - Initialize vector `filter` with `weights` on the host (CPU).\n",
    "     - Each weight is `1.0 / FILTER_WIDTH` (averaging filter).\n",
    "   - Copy `input` vector in host (CPU) memory to device (GPU) global memory.\n",
    "   - Copy `filter` vector in host (CPU) memory to `constant` device (GPU) memory.\n",
    "   - Launch kernel `convolve1D` with device (GPU) `input` and `ouput` vectors as arguments.\n",
    "     - Use `gridDim`, `blockDim`, and `shared_memory_size` as launch parameters, where\n",
    "       - `blockDim = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET`\n",
    "       - `gridDim = (DATA_WIDTH + block_width - 1) / block_width`\n",
    "       - `shared_mmeory_size = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` (`* sizeof(float)`)\n",
    "   - Measure the execution time for `convolve1D`.\n",
    "   - Copy `output` vector in device (GPU) memory to host (CPU) memory.\n",
    "   - Print execution time and sample elements in vectors `input` and `output`.\n",
    "   - Free memory allocated for vectors `input`, `output`, and `filter` on the host (CPU).\n",
    "   - Free memory allocated for vectors `input` and `output` on the device (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03156384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1048576 elements, 1x3 filter) took 0.15 ms\n",
      "Vector samples:\n",
      "h_input[0]=0.84, h_output[0]=0.41\n",
      "h_input[1]=0.39, h_output[1]=0.67\n",
      "h_input[2]=0.78, h_output[2]=0.66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// Number of data elements (1048576)\n",
    "#define DATA_WIDTH (1 << 20)\n",
    "\n",
    "// Number of filter elements\n",
    "#define FILTER_WIDTH 3\n",
    "\n",
    "// Number of elements on each side of a centered filter\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH / 2)\n",
    "\n",
    "// Number of elements in shared memory\n",
    "#define TILE_WIDTH_BASE 16\n",
    "\n",
    "// Constant memory with filter weights\n",
    "__constant__ float filter[FILTER_WIDTH];\n",
    "\n",
    "// Kernel function\n",
    "__global__ void convolve1D(float *input, float *output)\n",
    "{\n",
    "    // Shared mmeory of size (TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET) elements\n",
    "    extern __shared__ float shared[];\n",
    "\n",
    "    int s_col = threadIdx.x;                           // Thread's index in shared memory\n",
    "    int d_col = blockIdx.x * blockDim.x + threadIdx.x; // Thread's index in global memory\n",
    "    int i_col = d_col - FILTER_WIDTH_OFFSET;           // Thread's offset index in global memory\n",
    "\n",
    "    // Guard against threads with IDs that would index outside the arrays\n",
    "    if(d_col >= DATA_WIDTH) return;\n",
    "\n",
    "    // Fill shared memory with elements in global memory\n",
    "    if (i_col >= 0 && i_col < DATA_WIDTH)\n",
    "    {\n",
    "        shared[s_col] = input[i_col];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        shared[s_col] = 0.0f; // zero-padding\n",
    "    }\n",
    "\n",
    "    // Make sure each thread in the block has entered its element into shared memory before any thread continues\n",
    "    __syncthreads();\n",
    "\n",
    "    // Apply filter\n",
    "    float sum = 0.0f;\n",
    "    for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "    {\n",
    "        int f_col = FILTER_WIDTH_OFFSET + offset_col;\n",
    "        int i_col = s_col + f_col;\n",
    "        \n",
    "        if(i_col >= 0 && i_col < blockDim.x)\n",
    "        {\n",
    "            sum += shared[i_col] * filter[f_col]; // data elements in shared memory + filter weights in constant memory = super fast computation\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Store the weighted sum in the output array\n",
    "    output[d_col] = sum;\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    // Seed the random number generator\n",
    "    srand(0);\n",
    "    //srand(time(NULL));\n",
    "\n",
    "    // Declare variables\n",
    "    float *h_input, *h_output, *h_filter; // host copies of input, output, filter\n",
    "    float *d_input, *d_output, *d_filter; // device copies of input, output, filter\n",
    "    int data_size = DATA_WIDTH * sizeof(float);\n",
    "    int filter_size = FILTER_WIDTH * sizeof(float);\n",
    "   \n",
    "    // Allocate space for host (CPU) copies of input, output, filter\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "\n",
    "    // Allocate space for device (GPU) copies of input, output\n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "      \n",
    "    // Setup input values\n",
    "    for (int col = 0; col < DATA_WIDTH; col++)\n",
    "    {\n",
    "        h_input[col] = (float)rand() / RAND_MAX; // Random floats between 0 and 1.0\n",
    "    }\n",
    "\n",
    "    // Setup filter\n",
    "    for (int col = 0; col < FILTER_WIDTH; col++)\n",
    "    {\n",
    "        h_filter[col] = 1.0f / FILTER_WIDTH; // averaging filter\n",
    "    }\n",
    "   \n",
    "    // Copy inputs to device\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Copy filter to constant memory\n",
    "    cudaMemcpyToSymbol(filter, h_filter, filter_size);\n",
    "\n",
    "    // Calculate launch parameters\n",
    "    int block_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "    dim3 blockDim(block_width);\n",
    "    dim3 gridDim((DATA_WIDTH + block_width - 1) / block_width);\n",
    "    int shared_size = block_width * sizeof(float);\n",
    "\n",
    "    // Launch kernel convolve1D() with timing\n",
    "    cudaEvent_t start, stop;         // declare start and stop CUDA events\n",
    "    cudaEventCreate(&start);         // create start event\n",
    "    cudaEventCreate(&stop);          // create stop event\n",
    "    cudaEventRecord(start);          // record the start time\n",
    "\n",
    "    // Launch convolve1D() kernel on GPU\n",
    "    convolve1D<<<gridDim, blockDim, shared_size>>>(d_input, d_output); // dynamically sized shared memory\n",
    "    \n",
    "    cudaEventRecord(stop);           // record the stop time\n",
    "    cudaEventSynchronize(stop);      // wait for the stop time to become available\n",
    "    float elapsed_ms;\n",
    "    cudaEventElapsedTime(&elapsed_ms, start, stop); // calculate the elapsed time in millisecond\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Print measured calculation execution time\n",
    "    printf(\"Calculation (%d elements, 1x%d filter) took %.2f ms\\n\", DATA_WIDTH, FILTER_WIDTH, elapsed_ms);\n",
    "   \n",
    "    // Print out the FILTER_WIDTH number of elements in the two arrays\n",
    "    printf(\"Vector samples:\\n\");\n",
    "    for(int i = 0; i < FILTER_WIDTH; i++)\n",
    "    {\n",
    "        printf(\"h_input[%d]=%.2f, h_output[%d]=%.2f\\n\", i, h_input[i], i, h_output[i]);\n",
    "    }\n",
    "   \n",
    "    // Cleanup\n",
    "    free(h_input); free(h_output); free(h_filter); // free host (CPU) memory\n",
    "    cudaFree(d_input); cudaFree(d_output);         // free device (GPU) memory\n",
    "    cudaEventDestroy(start);                       // free the CUDA start event\n",
    "    cudaEventDestroy(stop);                        // free the CUDA stop event\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21905cb2",
   "metadata": {},
   "source": [
    "In the output we see:\n",
    "- The results are the same for the GPU solution as for the CPU solution.\n",
    "- The execution time for the GPU solution is significantly fast than the CPU solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88e6fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.5 2D Convolution on the Host (CPU)\n",
    "\n",
    "<img src=\"images/2dconvolution.gif\" width=\"600\" style=\"float: right; margin-right: 50px;\" />\n",
    "\n",
    "**Note**\n",
    "  - This is really just the same problem as a 1D convolution, but with an added second dimension.\n",
    "  - Therefore the problem and solution will be the same, but with the second dimension accounted for.\n",
    "\n",
    "Problem\n",
    "- We have an `input` matrix, a `kernel` (filter), and an `output` matrix.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` matrix.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` matrix.\n",
    "- So the `kernel`'s (filter's) width and height has to be odd, e.g. `3x3`, `5x5`, `7x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` matrix with the `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` matrix with the same `index` as the current `input` matrix.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` matrix, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define:\n",
    "   - `DATA_WIDTH=32` (number of elements in the `col` dimension for the `input` and `output`)\n",
    "   - `DATA_HEIGHT=32` (number of elements in the `row` dimension for the `input` and `output`)\n",
    "   - `FILTER_WIDTH=3` (number of elements int the `col` dimension for the `filter`)\n",
    "   - `FILTER_HEIGHT=3` (number of elements int the `row` dimension for the `filter`)\n",
    "   - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements to the left and right of the centered `filter`)\n",
    "   - `FILTER_HEIGHT_OFFSET=FILTER_HEIGHT/2` (number of elements above and below the centered `filter`)\n",
    "3. Create a function:\n",
    "   - `void convolve2D(float *input, float *output, float *filter)`\n",
    "   - Loop through `input` matrix.\n",
    "   - Compute convolution. Store result in `output` matrix.\n",
    "   - The only difference in the \"D convolution compared to the 1D convolution is the additional dimension.\n",
    "4. Create a function `main(void)`\n",
    "   - Declare and allocate memory for matrices `input`, `ouput`, and `filter`.\n",
    "   - Initialize matrix `input` with `DATA_HEIGHT * DATA_WIDTH` random floats.\n",
    "   - Initialize matrix `filter` with `weights` where each weight is `1.0 / FILTER_HEIGHT * FILTER_WIDTH` (averaging filter).\n",
    "   - Call function `convolve2D` with `input`, `ouput`, and `filter`.\n",
    "   - Measure the execution time for `convolve2D`.\n",
    "   - Print execution time and sample elements in matrices `input` and `output`.\n",
    "   - Free memory allocated for matrices `input`, `output`, and `filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77449720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1024 elements, 3x3 filter) took 0.03 ms\n",
      "\n",
      "Matrix samples:\n",
      "h_input              h_output\n",
      "0.840 0.394 0.783    0.238 0.396 0.382 \n",
      "0.613 0.296 0.638    0.328 0.527 0.568 \n",
      "0.267 0.540 0.375    0.325 0.514 0.616 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <math.h>\n",
    "\n",
    "#define DATA_WIDTH 32\n",
    "#define DATA_HEIGHT 32\n",
    "#define FILTER_WIDTH 3\n",
    "#define FILTER_HEIGHT 3\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH/2)\n",
    "#define FILTER_HEIGHT_OFFSET (FILTER_HEIGHT/2)\n",
    "\n",
    "void convolve2D(float *input, float *output, float *filter)\n",
    "{\n",
    "    for(int d_row = 0; d_row < DATA_HEIGHT; d_row++)\n",
    "    {\n",
    "        for(int d_col = 0; d_col < DATA_WIDTH; d_col++)\n",
    "        {\n",
    "            float sum = 0.0f;\n",
    "            for (int offset_row = -FILTER_HEIGHT_OFFSET; offset_row <= FILTER_HEIGHT_OFFSET; offset_row++)\n",
    "            {\n",
    "                for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "                {\n",
    "                    int f_row = FILTER_HEIGHT_OFFSET + offset_row;\n",
    "                    int f_col = FILTER_WIDTH_OFFSET + offset_col;\n",
    "                    int i_row = d_row + offset_row;\n",
    "                    int i_col = d_col + offset_col;\n",
    "\n",
    "                    if(i_row >= 0 && i_row < DATA_HEIGHT && i_col >= 0 && i_col < DATA_WIDTH)\n",
    "                    {\n",
    "                        sum += input[i_row * DATA_WIDTH + i_col] * filter[f_row * FILTER_WIDTH + f_col];\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            output[d_row * DATA_WIDTH + d_col] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "\n",
    "    float *h_input = (float *)malloc(DATA_WIDTH * DATA_HEIGHT * sizeof(float));\n",
    "    float *h_output = (float *)malloc(DATA_WIDTH * DATA_HEIGHT * sizeof(float));\n",
    "    float *h_filter = (float *)malloc(FILTER_WIDTH * FILTER_HEIGHT * sizeof(float));\n",
    "\n",
    "    for(int row = 0; row < DATA_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < DATA_WIDTH; col++)\n",
    "        {\n",
    "            h_input[row * DATA_WIDTH + col] = (float)rand() / RAND_MAX;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            h_filter[row * FILTER_WIDTH + col] = 1.0f / (FILTER_WIDTH * FILTER_HEIGHT);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Call convolve2D() with timing\n",
    "    clock_t start = clock();\n",
    "    convolve2D(h_input, h_output, h_filter);\n",
    "    clock_t stop = clock();\n",
    "    double elapsed_ms = (double)(stop - start) / CLOCKS_PER_SEC * 1000.0;\n",
    "\n",
    "    printf(\"Calculation (%d elements, %dx%d filter) took %.2f ms\\n\", DATA_HEIGHT * DATA_WIDTH, FILTER_HEIGHT, FILTER_WIDTH, elapsed_ms);\n",
    "    printf(\"\\nMatrix samples:\\n\");\n",
    "    printf(\"h_input %-12s h_output\\n\", \"\");\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_input[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"%-3s\",\"\");\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_output[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc51aab",
   "metadata": {},
   "source": [
    "- The output shows:\n",
    "  - Given an element with index `[row, col]` in the `output` matrix.\n",
    "  - It's value is the average of the elements with indices:\n",
    "    - `-FILTER_HEIGHT_OFFSET..+FILTER_HEIGHT_OFFSET+1` in the `input` matrix's `row`.\n",
    "    - `-FILTER_WIDTH_OFFSET..+FILTER_WIDTH_OFFSET+1` in the `input` matrix's `col`.\n",
    "    - Since an averaging filter was used.\n",
    "  - For example\n",
    "    - If the `FILTER_HEIGHT` is `3`, we have `FILTER_HEIGHT_OFFSET = FILTER_HEIGHT / 2 = 1`.\n",
    "    - If the `FILTER_WIDTH` is `3`, we have `FILTER_WIDTH_OFFSET = FILTER_WIDTH / 2 = 1`.\n",
    "    - The value of an element with index `[row, col]` in the `output` matrix is the average of the elements in the `input` matrix with indices:\n",
    "\n",
    "      ```c\n",
    "      [row-1, col-1]  [row-1, col]  [row-1, col+1]\n",
    "      [row  , col-1]  [row  , col]  [row  , col+1]\n",
    "      [row+1, col-1]  [row+1, col]  [row+1, col+1]\n",
    "      ```\n",
    "    - If it's a bounday element, the out-of-bounds indices have zero-padded elements with a value of `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333dd74",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.6 2D Convolution on the Device (GPU)\n",
    "\n",
    "**Note**\n",
    "  - This is really just the same problem as a 1D convolution, but with an added second dimension.\n",
    "  - Therefore the problem and solution will be the same, but with the second dimension accounted for.\n",
    "\n",
    "Problem\n",
    "- We have an `input` matrix, a `kernel` (filter), and an `output` matrix.\n",
    "- We want to slide the `kernel` (filter) over each element in the `input` matrix.\n",
    "- The `kernel` (filter) will be centered over each element in the `input` matrix.\n",
    "- So the `kernel`'s (filter's) height and width has to be odd, e.g. `3x3`, `5x5`, `7x7`.\n",
    "- We multiply each element under the `kernel` (filter) in the `input` matrix with the `kernel`'s (filter's) elements.\n",
    "- We sum the products, and assign the sum to the `output` matrix with the same `index` as the current `input` matrix.\n",
    "- Since the `kernel` (filter) can't be centered over the boundary elements in the `input` matrix, we use `zero-padding`.\n",
    "\n",
    "Solution\n",
    "1. Define:\n",
    "  -  `DATA_WIDTH=32` (number of elements in the `col` dimension for the `input` and `output`)\n",
    "   - `DATA_HEIGHT=32` (number of elements in the `row` dimension for the `input` and `output`)\n",
    "   - `FILTER_WIDTH=3` (number of elements int the `col` dimension for the `filter`)\n",
    "   - `FILTER_HEIGHT=3` (number of elements int the `row` dimension for the `filter`)\n",
    "   - `FILTER_WIDTH_OFFSET=FILTER_WIDTH/2` (number of elements to the left and right of the centered `filter`)\n",
    "   - `FILTER_HEIGHT_OFFSET=FILTER_HEIGHT/2` (number of elements above and below the centered `filter`) \n",
    "   - `TILE_WIDTH_BASE=16` (number original elements in a `block` of `threads` in the `col` dimension)\n",
    "   - `TILE_HEIGHT_BASE=16` (number original elements in a `block` of `threads` in the `row` dimension)\n",
    "     - Where the final tile size in the `col` dimension is `TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET` to cover left and right border elements.\n",
    "       - This is also the size we will use for the `col` dimension in `shared memory` and `block` size, i.e. `blockDim.x`.\n",
    "     - Where the final tile size in the `row` dimension is `TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET` to cover top and bottom border elements.\n",
    "       - This is also the size we will use for the `row` dimension in `shared memory` and `block` size, i.e. `blockDim.y`.\n",
    "     - So we have these many 2D `threads` in each `block` and we know a `block` is assigned to an `SM`.\n",
    "3. Define `constant` memory of size `FILTER_HEIGHT * FILTER_WIDTH` for the `filter`.\n",
    "4. Create a kernel function `void convolve2D(float *input, float *output)`:\n",
    "   - Define `shared` memory of size `(TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET) * (TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET)`.\n",
    "   - Let the `threads` in a `block` load their `input` elements into `shared` memory.\n",
    "     - For border elements, we load the value `0` into `shared` memory (zero-padding).\n",
    "   - Synchronize `threads`to ensure each `thread`in a `block` has loaded its element into `shared` memory.\n",
    "   - Compute the convolution as in the CPU solution, but now using `shared` memory (input) and `constant` memory (filter).\n",
    "   - Store the result in the `ouput` matrix.\n",
    "5. Create a function `main(void)`\n",
    "   - Declare and allocate memory for matrices `input`, `ouput`, and `filter` on the host (CPU).\n",
    "   - Declare and allocate memory for matrices `input` and `ouput` on the device (GPU).\n",
    "   - Initialize matrix `input` with `DATA_HEIGHT * DATA_WIDTH` random floats on the host (CPU).\n",
    "   - Initialize matrix `filter` with `weights` on the host (CPU).\n",
    "     - Each weight is `1.0 / (FILTER_HEIGHT * FILTER_WIDTH)` (averaging filter).\n",
    "   - Copy `input` matrix in host (CPU) memory to device (GPU) global memory.\n",
    "   - Copy `filter` matrix in host (CPU) memory to `constant` device (GPU) memory.\n",
    "   - Launch kernel `convolve2D` with device (GPU) `input` and `ouput` matrices as arguments.\n",
    "     - Use `gridDim`, `blockDim`, and `shared_memory_size` as launch parameters, where\n",
    "       - `blockDim.x = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET`\n",
    "       - `blockDim.y = TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET`\n",
    "       - `gridDim.x = (DATA_WIDTH + block_width - 1) / block_width`\n",
    "       - `gridDim.y = (DATA_HEIGHT + block_height - 1) / block_height`\n",
    "       - `shared_mmeory_size = (TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET) * (TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET)` (`* sizeof(float)`)\n",
    "   - Measure the execution time for `convolve2D`.\n",
    "   - Copy `output` matrix in device (GPU) memory to host (CPU) memory.\n",
    "   - Print execution time and sample elements in matrices `input` and `output`.\n",
    "   - Free memory allocated for matrices `input`, `output`, and `filter` on the host (CPU).\n",
    "   - Free memory allocated for matrices `input` and `output` on the device (GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59663462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation (1024 elements, 3x3 filter) took 0.02 ms\n",
      "\n",
      "Matrix samples:\n",
      "h_input              h_output\n",
      "0.840 0.394 0.783    0.238 0.396 0.382 \n",
      "0.613 0.296 0.638    0.328 0.527 0.568 \n",
      "0.267 0.540 0.375    0.325 0.514 0.616 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "#define DATA_WIDTH 32\n",
    "#define DATA_HEIGHT 32\n",
    "#define FILTER_WIDTH 3\n",
    "#define FILTER_HEIGHT 3\n",
    "#define FILTER_WIDTH_OFFSET (FILTER_WIDTH/2)\n",
    "#define FILTER_HEIGHT_OFFSET (FILTER_HEIGHT/2)\n",
    "#define TILE_WIDTH_BASE 16\n",
    "#define TILE_HEIGHT_BASE 16\n",
    "\n",
    "__constant__ float filter[FILTER_WIDTH * FILTER_HEIGHT];\n",
    "\n",
    "__global__ void convolve2D(float *input, float *output)\n",
    "{\n",
    "    extern __shared__ float shared[];\n",
    "\n",
    "    int s_row = threadIdx.y;\n",
    "    int s_col = threadIdx.x;\n",
    "    \n",
    "    int d_col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int d_row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "    int i_row = d_row - FILTER_HEIGHT_OFFSET;\n",
    "    int i_col = d_col - FILTER_WIDTH_OFFSET;\n",
    "\n",
    "    if(d_col >= DATA_WIDTH || d_row >= DATA_HEIGHT) return;   \n",
    "    \n",
    "    if (i_row >= 0 && i_row < DATA_HEIGHT && i_col >= 0 && i_col < DATA_WIDTH)\n",
    "    {\n",
    "        shared[s_row * blockDim.x + s_col] = input[i_row * DATA_WIDTH + i_col];\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        shared[s_row * blockDim.x + s_col] = 0.0f; // zero-padding\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    float sum = 0.0f;\n",
    "    for (int offset_row = -FILTER_HEIGHT_OFFSET; offset_row <= FILTER_HEIGHT_OFFSET; offset_row++)\n",
    "    {\n",
    "        for (int offset_col = -FILTER_WIDTH_OFFSET; offset_col <= FILTER_WIDTH_OFFSET; offset_col++)\n",
    "        {\n",
    "            int f_row = FILTER_HEIGHT_OFFSET + offset_row;\n",
    "            int f_col = FILTER_WIDTH_OFFSET + offset_col;\n",
    "            int i_row = s_row + f_row;\n",
    "            int i_col = s_col + f_col;\n",
    "\n",
    "            if(i_row >= 0 && i_row < blockDim.y && i_col >= 0 && i_col < blockDim.x)\n",
    "            {\n",
    "                sum += shared[i_row * blockDim.x + i_col] * filter[f_row * FILTER_WIDTH + f_col];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    output[d_row * DATA_WIDTH + d_col] = sum;\n",
    "}\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    srand(0);\n",
    "\n",
    "    float *h_input, *h_output, *h_filter;\n",
    "    float *d_input, *d_output, *d_filter;\n",
    "    int data_size = DATA_WIDTH * DATA_HEIGHT * sizeof(float);\n",
    "    int filter_size = FILTER_WIDTH * FILTER_HEIGHT * sizeof(float);\n",
    "\n",
    "    h_input = (float *)malloc(data_size);\n",
    "    h_output = (float *)malloc(data_size);\n",
    "    h_filter = (float *)malloc(filter_size);\n",
    "    \n",
    "    cudaMalloc((void **)&d_input, data_size);\n",
    "    cudaMalloc((void **)&d_output, data_size);\n",
    "    cudaMalloc((void **)&d_filter, filter_size);\n",
    "\n",
    "    for(int row = 0; row < DATA_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < DATA_WIDTH; col++)\n",
    "        {\n",
    "            h_input[row * DATA_WIDTH + col] = (float)rand() / RAND_MAX;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            h_filter[row * FILTER_WIDTH + col] = 1.0f / (FILTER_WIDTH * FILTER_HEIGHT);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cudaMemcpy(d_input, h_input, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_output, h_output, data_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_filter, h_filter, filter_size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpyToSymbol(filter, h_filter, filter_size);\n",
    "    \n",
    "    int block_size_width = TILE_WIDTH_BASE + 2 * FILTER_WIDTH_OFFSET;\n",
    "    int block_size_height = TILE_HEIGHT_BASE + 2 * FILTER_HEIGHT_OFFSET;\n",
    "\n",
    "    dim3 blockDim(block_size_width, block_size_height);\n",
    "    dim3 gridDim((DATA_WIDTH + block_size_width - 1) / block_size_width, (DATA_HEIGHT + block_size_height - 1) / block_size_height);\n",
    "    int shared_size = block_size_width * block_size_height * sizeof(float);\n",
    "\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    cudaEventRecord(start);\n",
    "\n",
    "    // Launch convolve2D() kernel on GPU\n",
    "    convolve2D<<<gridDim, blockDim, shared_size>>>(d_input, d_output);\n",
    "    \n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float elapsed_ms;\n",
    "    cudaEventElapsedTime(&elapsed_ms, start, stop);\n",
    "\n",
    "    cudaMemcpy(h_output, d_output, data_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    printf(\"Calculation (%d elements, %dx%d filter) took %.2f ms\\n\", DATA_HEIGHT * DATA_WIDTH, FILTER_HEIGHT, FILTER_WIDTH, elapsed_ms);\n",
    "    printf(\"\\nMatrix samples:\\n\");\n",
    "    printf(\"h_input %-12s h_output\\n\", \"\");\n",
    "    for(int row = 0; row < FILTER_HEIGHT; row++)\n",
    "    {\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_input[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"%-3s\",\"\");\n",
    "        for(int col = 0; col < FILTER_WIDTH; col++)\n",
    "        {\n",
    "            printf(\"%.3f \", h_output[row * DATA_WIDTH + col]);\n",
    "        }\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_output);\n",
    "    cudaFree(d_filter);\n",
    "\n",
    "    free(h_input);\n",
    "    free(h_output);\n",
    "    free(h_filter);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fba808",
   "metadata": {},
   "source": [
    "In the output we see:\n",
    "- The results are the same for the GPU solution as for the CPU solution.\n",
    "- The execution time for the GPU solution is significantly fast than the CPU solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79f513",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Cleanup\n",
    "---\n",
    "\n",
    "- Let's remove all files that have been created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "826d62ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "dirs = [\"nvcc4jupyter\", \"src\", \"include\", \"bin\", \".vscode\"]\n",
    "files = [\"main.cu\", \"main.exe\", \"profile_report.ncu-rep\"]\n",
    "\n",
    "for d in dirs:\n",
    "    if os.path.exists(d):\n",
    "        shutil.rmtree(d)\n",
    "\n",
    "for f in files:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a81ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "fsharp",
    "items": [
     {
      "aliases": [],
      "name": "fsharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
